
@inproceedings{sanchez_high-dimensional_2011,
	address = {Colorado Springs, CO, USA},
	title = {High-dimensional signature compression for large-scale image classification},
	isbn = {978-1-4577-0394-2},
	url = {http://ieeexplore.ieee.org/document/5995504/},
	doi = {10.1109/CVPR.2011.5995504},
	abstract = {We address image classiﬁcation on a large-scale, i.e. when a large number of images and classes are involved. First, we study classiﬁcation accuracy as a function of the image signature dimensionality and the training set size. We show experimentally that the larger the training set, the higher the impact of the dimensionality on the accuracy. In other words, high-dimensional signatures are important to obtain state-of-the-art results on large datasets. Second, we tackle the problem of data compression on very large signatures (on the order of 105 dimensions) using two lossy compression strategies: a dimensionality reduction technique known as the hash kernel and an encoding technique based on product quantizers. We explain how the gain in storage can be traded against a loss in accuracy and / or an increase in CPU cost. We report results on two large databases – ImageNet and a dataset of 1M Flickr images – showing that we can reduce the storage of our signatures by a factor 64 to 128 with little loss in accuracy. Integrating the decompression in the classiﬁer learning yields an efﬁcient and scalable training algorithm. On ILSVRC2010 we report a 74.3\% accuracy at top-5, which corresponds to a 2.5\% absolute improvement with respect to the state-of-the-art. On a subset of 10K classes of ImageNet we report a top-1 accuracy of 16.7\%, a relative improvement of 160\% with respect to the state-of-the-art.},
	language = {en},
	urldate = {2021-05-21},
	booktitle = {{CVPR} 2011},
	publisher = {IEEE},
	author = {Sanchez, Jorge and Perronnin, Florent},
	month = jun,
	year = {2011},
	pages = {1665--1672},
}

@article{krizhevsky_imagenet_2017,
	title = {{ImageNet} classification with deep convolutional neural networks},
	volume = {60},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/3065386},
	doi = {10.1145/3065386},
	abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of ﬁve convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a ﬁnal 1000-way softmax. To make training faster, we used non-saturating neurons and a very efﬁcient GPU implementation of the convolution operation. To reduce overﬁtting in the fully-connected layers we employed a recently-developed regularization method called “dropout” that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
	language = {en},
	number = {6},
	urldate = {2021-05-21},
	journal = {Communications of the ACM},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
	month = may,
	year = {2017},
	pages = {84--90},
}

@article{everingham_pascal_2010,
	title = {The {Pascal} {Visual} {Object} {Classes} ({VOC}) {Challenge}},
	volume = {88},
	issn = {0920-5691, 1573-1405},
	url = {http://link.springer.com/10.1007/s11263-009-0275-4},
	doi = {10.1007/s11263-009-0275-4},
	abstract = {The PASCAL Visual Object Classes (VOC) challenge is a benchmark in visual object category recognition and detection, providing the vision and machine learning communities with a standard dataset of images and annotation, and standard evaluation procedures. Organised annually from 2005 to present, the challenge and its associated dataset has become accepted as the benchmark for object detection.},
	language = {en},
	number = {2},
	urldate = {2021-05-21},
	journal = {International Journal of Computer Vision},
	author = {Everingham, Mark and Van Gool, Luc and Williams, Christopher K. I. and Winn, John and Zisserman, Andrew},
	month = jun,
	year = {2010},
	pages = {303--338},
}

@article{madry_towards_2019,
	title = {Towards {Deep} {Learning} {Models} {Resistant} to {Adversarial} {Attacks}},
	url = {http://arxiv.org/abs/1706.06083},
	abstract = {Recent work has demonstrated that deep neural networks are vulnerable to adversarial examples---inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. Code and pre-trained models are available at https://github.com/MadryLab/mnist\_challenge and https://github.com/MadryLab/cifar10\_challenge.},
	language = {en},
	urldate = {2021-05-21},
	journal = {arXiv:1706.06083 [cs, stat]},
	author = {Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
	month = sep,
	year = {2019},
	note = {arXiv: 1706.06083},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{akhtar_threat_2018,
	title = {Threat of {Adversarial} {Attacks} on {Deep} {Learning} in {Computer} {Vision}: {A} {Survey}},
	shorttitle = {Threat of {Adversarial} {Attacks} on {Deep} {Learning} in {Computer} {Vision}},
	url = {http://arxiv.org/abs/1801.00553},
	abstract = {Deep learning is at the heart of the current rise of artiﬁcial intelligence. In the ﬁeld of Computer Vision, it has become the workhorse for applications ranging from self-driving cars to surveillance and security. Whereas deep neural networks have demonstrated phenomenal success (often beyond human capabilities) in solving complex problems, recent studies show that they are vulnerable to adversarial attacks in the form of subtle perturbations to inputs that lead a model to predict incorrect outputs. For images, such perturbations are often too small to be perceptible, yet they completely fool the deep learning models. Adversarial attacks pose a serious threat to the success of deep learning in practice. This fact has recently lead to a large inﬂux of contributions in this direction. This article presents the ﬁrst comprehensive survey on adversarial attacks on deep learning in Computer Vision. We review the works that design adversarial attacks, analyze the existence of such attacks and propose defenses against them. To emphasize that adversarial attacks are possible in practical conditions, we separately review the contributions that evaluate adversarial attacks in the real-world scenarios. Finally, drawing on the reviewed literature, we provide a broader outlook of this research direction.},
	language = {en},
	urldate = {2021-05-21},
	journal = {arXiv:1801.00553 [cs]},
	author = {Akhtar, Naveed and Mian, Ajmal},
	month = feb,
	year = {2018},
	note = {arXiv: 1801.00553},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{pitropov_canadian_2021,
	title = {Canadian {Adverse} {Driving} {Conditions} dataset},
	volume = {40},
	issn = {0278-3649, 1741-3176},
	url = {http://journals.sagepub.com/doi/10.1177/0278364920979368},
	doi = {10.1177/0278364920979368},
	abstract = {The Canadian Adverse Driving Conditions (CADC) dataset was collected with the Autonomoose autonomous vehicle platform, based on a modified Lincoln MKZ. The dataset, collected during winter within the Region of Waterloo, Canada, is the first autonomous driving dataset that focuses on adverse driving conditions specifically. It contains 7,000 frames of annotated data from 8 cameras (Ximea MQ013CG-E2), lidar (VLP-32C), and a GNSS + INS system (Novatel OEM638), collected through a variety of winter weather conditions. The sensors are time synchronized and calibrated with the intrinsic and extrinsic calibrations included in the dataset. Lidar frame annotations that represent ground truth for 3D object detection and tracking have been provided by Scale AI.},
	language = {en},
	number = {4-5},
	urldate = {2021-05-12},
	journal = {The International Journal of Robotics Research},
	author = {Pitropov, Matthew and Garcia, Danson Evan and Rebello, Jason and Smart, Michael and Wang, Carlos and Czarnecki, Krzysztof and Waslander, Steven},
	month = apr,
	year = {2021},
	pages = {681--690},
}

@article{zhu_cylindrical_2020,
	title = {Cylindrical and {Asymmetrical} {3D} {Convolution} {Networks} for {LiDAR} {Segmentation}},
	url = {http://arxiv.org/abs/2011.10033},
	abstract = {State-of-the-art methods for large-scale driving-scene LiDAR segmentation often project the point clouds to 2D space and then process them via 2D convolution. Although this corporation shows the competitiveness in the point cloud, it inevitably alters and abandons the 3D topology and geometric relations. A natural remedy is to utilize the 3D voxelization and 3D convolution network. However, we found that in the outdoor point cloud, the improvement obtained in this way is quite limited. An important reason is the property of the outdoor point cloud, namely sparsity and varying density. Motivated by this investigation, we propose a new framework for the outdoor LiDAR segmentation, where cylindrical partition and asymmetrical 3D convolution networks are designed to explore the 3D geometric pattern while maintaining these inherent properties. Moreover, a point-wise reﬁnement module is introduced to alleviate the interference of lossy voxel-based label encoding. We evaluate the proposed model on two large-scale datasets, i.e., SemanticKITTI and nuScenes. Our method achieves the 1st place in the leaderboard of SemanticKITTI 1 and outperforms existing methods on nuScenes with a noticeable margin, about 4\%. Furthermore, the proposed 3D framework also generalizes well to LiDAR panoptic segmentation and LiDAR 3D detection.},
	language = {en},
	urldate = {2021-05-03},
	journal = {arXiv:2011.10033 [cs]},
	author = {Zhu, Xinge and Zhou, Hui and Wang, Tai and Hong, Fangzhou and Ma, Yuexin and Li, Wei and Li, Hongsheng and Lin, Dahua},
	month = nov,
	year = {2020},
	note = {arXiv: 2011.10033},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{dong_boosting_2018,
	title = {Boosting {Adversarial} {Attacks} with {Momentum}},
	url = {http://arxiv.org/abs/1710.06081},
	abstract = {Deep neural networks are vulnerable to adversarial examples, which poses security concerns on these algorithms due to the potentially severe consequences. Adversarial attacks serve as an important surrogate to evaluate the robustness of deep learning models before they are deployed. However, most of existing adversarial attacks can only fool a black-box model with a low success rate. To address this issue, we propose a broad class of momentum-based iterative algorithms to boost adversarial attacks. By integrating the momentum term into the iterative process for attacks, our methods can stabilize update directions and escape from poor local maxima during the iterations, resulting in more transferable adversarial examples. To further improve the success rates for black-box attacks, we apply momentum iterative algorithms to an ensemble of models, and show that the adversarially trained models with a strong defense ability are also vulnerable to our black-box attacks. We hope that the proposed methods will serve as a benchmark for evaluating the robustness of various deep models and defense methods. With this method, we won the ﬁrst places in NIPS 2017 Non-targeted Adversarial Attack and Targeted Adversarial Attack competitions.},
	language = {en},
	urldate = {2021-05-03},
	journal = {arXiv:1710.06081 [cs, stat]},
	author = {Dong, Yinpeng and Liao, Fangzhou and Pang, Tianyu and Su, Hang and Zhu, Jun and Hu, Xiaolin and Li, Jianguo},
	month = mar,
	year = {2018},
	note = {arXiv: 1710.06081},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{goodfellow_explaining_2015,
	title = {Explaining and {Harnessing} {Adversarial} {Examples}},
	url = {http://arxiv.org/abs/1412.6572},
	abstract = {Several machine learning models, including neural networks, consistently misclassify adversarial examples—inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high conﬁdence. Early attempts at explaining this phenomenon focused on nonlinearity and overﬁtting. We argue instead that the primary cause of neural networks’ vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the ﬁrst explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.},
	language = {en},
	urldate = {2021-05-03},
	journal = {arXiv:1412.6572 [cs, stat]},
	author = {Goodfellow, Ian J. and Shlens, Jonathon and Szegedy, Christian},
	month = mar,
	year = {2015},
	note = {arXiv: 1412.6572},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{moosavi-dezfooli_deepfool_2016,
	title = {{DeepFool}: a simple and accurate method to fool deep neural networks},
	shorttitle = {{DeepFool}},
	url = {http://arxiv.org/abs/1511.04599},
	abstract = {State-of-the-art deep neural networks have achieved impressive results on many image classification tasks. However, these same architectures have been shown to be unstable to small, well sought, perturbations of the images. Despite the importance of this phenomenon, no effective methods have been proposed to accurately compute the robustness of state-of-the-art deep classifiers to such perturbations on large-scale datasets. In this paper, we fill this gap and propose the DeepFool algorithm to efficiently compute perturbations that fool deep networks, and thus reliably quantify the robustness of these classifiers. Extensive experimental results show that our approach outperforms recent methods in the task of computing adversarial perturbations and making classifiers more robust.},
	language = {en},
	urldate = {2021-05-03},
	journal = {arXiv:1511.04599 [cs]},
	author = {Moosavi-Dezfooli, Seyed-Mohsen and Fawzi, Alhussein and Frossard, Pascal},
	month = jul,
	year = {2016},
	note = {arXiv: 1511.04599},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{kurakin_adversarial_2017,
	title = {Adversarial examples in the physical world},
	url = {http://arxiv.org/abs/1607.02533},
	abstract = {Most existing machine learning classiﬁers are highly vulnerable to adversarial examples. An adversarial example is a sample of input data which has been modiﬁed very slightly in a way that is intended to cause a machine learning classiﬁer to misclassify it. In many cases, these modiﬁcations can be so subtle that a human observer does not even notice the modiﬁcation at all, yet the classiﬁer still makes a mistake. Adversarial examples pose security concerns because they could be used to perform an attack on machine learning systems, even if the adversary has no access to the underlying model. Up to now, all previous work has assumed a threat model in which the adversary can feed data directly into the machine learning classiﬁer. This is not always the case for systems operating in the physical world, for example those which are using signals from cameras and other sensors as input. This paper shows that even in such physical world scenarios, machine learning systems are vulnerable to adversarial examples. We demonstrate this by feeding adversarial images obtained from a cell-phone camera to an ImageNet Inception classiﬁer and measuring the classiﬁcation accuracy of the system. We ﬁnd that a large fraction of adversarial examples are classiﬁed incorrectly even when perceived through the camera.},
	language = {en},
	urldate = {2021-05-03},
	journal = {arXiv:1607.02533 [cs, stat]},
	author = {Kurakin, Alexey and Goodfellow, Ian and Bengio, Samy},
	month = feb,
	year = {2017},
	note = {arXiv: 1607.02533},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{carballo_libre_2020,
	title = {{LIBRE}: {The} {Multiple} {3D} {LiDAR} {Dataset}},
	shorttitle = {{LIBRE}},
	url = {http://arxiv.org/abs/2003.06129},
	abstract = {In this work, we present LIBRE: LiDAR Benchmarking and Reference, a ﬁrst-of-its-kind dataset featuring 10 different LiDAR sensors, covering a range of manufacturers, models, and laser conﬁgurations. Data captured independently from each sensor includes three different environments and conﬁgurations: static targets, where objects were placed at known distances and measured from a ﬁxed position within a controlled environment; adverse weather, where static obstacles were measured from a moving vehicle, captured in a weather chamber where LiDARs were exposed to different conditions (fog, rain, strong light); and ﬁnally, dynamic trafﬁc, where dynamic objects were captured from a vehicle driven on public urban roads, multiple times at different times of the day, and including supporting sensors such as cameras, infrared imaging, and odometry devices. LIBRE will contribute to the research community to (1) provide a means for a fair comparison of currently available LiDARs, and (2) facilitate the improvement of existing self-driving vehicles and robotics-related software, in terms of development and tuning of LiDAR-based perception algorithms.},
	language = {en},
	urldate = {2021-05-03},
	journal = {arXiv:2003.06129 [cs]},
	author = {Carballo, Alexander and Lambert, Jacob and Monrroy-Cano, Abraham and Wong, David Robert and Narksri, Patiphon and Kitsukawa, Yuki and Takeuchi, Eijiro and Kato, Shinpei and Takeda, Kazuya},
	month = jun,
	year = {2020},
	note = {arXiv: 2003.06129},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@article{geiger_vision_2013,
	title = {Vision meets robotics: {The} {KITTI} dataset},
	volume = {32},
	issn = {0278-3649, 1741-3176},
	shorttitle = {Vision meets robotics},
	url = {http://journals.sagepub.com/doi/10.1177/0278364913491297},
	doi = {10.1177/0278364913491297},
	abstract = {We present a novel dataset captured from a VW station wagon for use in mobile robotics and autonomous driving research. In total, we recorded 6 hours of trafﬁc scenarios at 10–100 Hz using a variety of sensor modalities such as high-resolution color and grayscale stereo cameras, a Velodyne 3D laser scanner and a high-precision GPS/IMU inertial navigation system. The scenarios are diverse, capturing real-world trafﬁc situations, and range from freeways over rural areas to inner-city scenes with many static and dynamic objects. Our data is calibrated, synchronized and timestamped, and we provide the rectiﬁed and raw image sequences. Our dataset also contains object labels in the form of 3D tracklets, and we provide online benchmarks for stereo, optical ﬂow, object detection and other tasks. This paper describes our recording platform, the data format and the utilities that we provide.},
	language = {en},
	number = {11},
	urldate = {2021-04-18},
	journal = {The International Journal of Robotics Research},
	author = {Geiger, A and Lenz, P and Stiller, C and Urtasun, R},
	month = sep,
	year = {2013},
	pages = {1231--1237},
}

@inproceedings{zhang_towards_2019,
	address = {Seoul, Korea (South)},
	title = {Towards {Adversarially} {Robust} {Object} {Detection}},
	isbn = {978-1-72814-803-8},
	url = {https://ieeexplore.ieee.org/document/9009990/},
	doi = {10.1109/ICCV.2019.00051},
	abstract = {Object detection is an important vision task and has emerged as an indispensable component in many vision system, rendering its robustness as an increasingly important performance factor for practical applications. While object detection models have been demonstrated to be vulnerable against adversarial attacks by many recent works, very few efforts have been devoted to improving their robustness. In this work, we take an initial attempt towards this direction. We ﬁrst revisit and systematically analyze object detectors and many recently developed attacks from the perspective of model robustness. We then present a multi-task learning perspective of object detection and identify an asymmetric role of task losses. We further develop an adversarial training approach which can leverage the multiple sources of attacks for improving the robustness of detection models. Extensive experiments on PASCAL-VOC and MS-COCO veriﬁed the effectiveness of the proposed approach.},
	language = {en},
	urldate = {2021-03-17},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Zhang, Haichao and Wang, Jianyu},
	month = oct,
	year = {2019},
	pages = {421--430},
}

@article{hendrycks_benchmarking_2019,
	title = {Benchmarking {Neural} {Network} {Robustness} to {Common} {Corruptions} and {Perturbations}},
	url = {http://arxiv.org/abs/1903.12261},
	abstract = {In this paper we establish rigorous benchmarks for image classiﬁer robustness. Our ﬁrst benchmark, IMAGENET-C, standardizes and expands the corruption robustness topic, while showing which classiﬁers are preferable in safety-critical applications. Then we propose a new dataset called IMAGENET-P which enables researchers to benchmark a classiﬁer’s robustness to common perturbations. Unlike recent robustness research, this benchmark evaluates performance on common corruptions and perturbations not worst-case adversarial perturbations. We ﬁnd that there are negligible changes in relative corruption robustness from AlexNet classiﬁers to ResNet classiﬁers. Afterward we discover ways to enhance corruption and perturbation robustness. We even ﬁnd that a bypassed adversarial defense provides substantial common perturbation robustness. Together our benchmarks may aid future work toward networks that robustly generalize.},
	language = {en},
	urldate = {2021-03-09},
	journal = {arXiv:1903.12261 [cs, stat]},
	author = {Hendrycks, Dan and Dietterich, Thomas},
	month = mar,
	year = {2019},
	note = {arXiv: 1903.12261},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{michaelis_benchmarking_2020,
	title = {Benchmarking {Robustness} in {Object} {Detection}: {Autonomous} {Driving} when {Winter} is {Coming}},
	shorttitle = {Benchmarking {Robustness} in {Object} {Detection}},
	url = {http://arxiv.org/abs/1907.07484},
	abstract = {The ability to detect objects regardless of image distortions or weather conditions is crucial for real-world applications of deep learning like autonomous driving. We here provide an easy-to-use benchmark to assess how object detection models perform when image quality degrades. The three resulting benchmark datasets, termed Pascal-C, Coco-C and Cityscapes-C, contain a large variety of image corruptions. We show that a range of standard object detection models suffer a severe performance loss on corrupted images (down to 30–60\% of the original performance). However, a simple data augmentation trick—stylizing the training images—leads to a substantial increase in robustness across corruption type, severity and dataset. We envision our comprehensive benchmark to track future progress towards building robust object detection models. Benchmark, code and data are publicly available.},
	language = {en},
	urldate = {2021-03-09},
	journal = {arXiv:1907.07484 [cs, stat]},
	author = {Michaelis, Claudio and Mitzkus, Benjamin and Geirhos, Robert and Rusak, Evgenia and Bringmann, Oliver and Ecker, Alexander S. and Bethge, Matthias and Brendel, Wieland},
	month = mar,
	year = {2020},
	note = {arXiv: 1907.07484},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{ramasinghe_spectral-gans_2020,
	title = {Spectral-{GANs} for {High}-{Resolution} {3D} {Point}-cloud {Generation}},
	url = {http://arxiv.org/abs/1912.01800},
	abstract = {Point-clouds are a popular choice for vision and graphics tasks due to their accurate shape description and direct acquisition from range-scanners. This demands the ability to synthesize and reconstruct high-quality point-clouds. Current deep generative models for 3D data generally work on simpliﬁed representations (e.g., voxelized objects) and cannot deal with the inherent redundancy and irregularity in point-clouds. A few recent efforts on 3D point-cloud generation offer limited resolution and their complexity grows with the increase in output resolution. In this paper, we develop a principled approach to synthesize 3D point-clouds using a spectral-domain Generative Adversarial Network (GAN). Our spectral representation is highly structured and allows us to disentangle various frequency bands such that the learning task is simpliﬁed for a GAN model. As compared to spatial-domain generative approaches, our formulation allows us to generate high-resolution point-clouds with minimal computational overhead. Furthermore, we propose a fully differentiable block to transform from the spectral to the spatial domain and back, thereby allowing us to integrate knowledge from well-established spatial models. We demonstrate that Spectral-GAN performs well for pointcloud generation task. Additionally, it can learn a highly discriminative representation in an unsupervised fashion and can be used to accurately reconstruct 3D objects.},
	language = {en},
	urldate = {2021-02-26},
	journal = {arXiv:1912.01800 [cs]},
	author = {Ramasinghe, Sameera and Khan, Salman and Barnes, Nick and Gould, Stephen},
	month = jul,
	year = {2020},
	note = {arXiv: 1912.01800},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{arshad_progressive_2020,
	title = {A {Progressive} {Conditional} {Generative} {Adversarial} {Network} for {Generating} {Dense} and {Colored} {3D} {Point} {Clouds}},
	url = {http://arxiv.org/abs/2010.05391},
	abstract = {In this paper, we introduce a novel conditional generative adversarial network that creates dense 3D point clouds, with color, for assorted classes of objects in an unsupervised manner. To overcome the difﬁculty of capturing intricate details at high resolutions, we propose a point transformer that progressively grows the network through the use of graph convolutions. The network is composed of a leaf output layer and an initial set of branches. Every training iteration evolves a point vector into a point cloud of increasing resolution. After a ﬁxed number of iterations, the number of branches is increased by replicating the last branch. Experimental results show that our network is capable of learning and mimicking a 3D data distribution, and produces colored point clouds with ﬁne details at multiple resolutions.},
	language = {en},
	urldate = {2021-02-26},
	journal = {arXiv:2010.05391 [cs]},
	author = {Arshad, Mohammad Samiul and Beksi, William J.},
	month = oct,
	year = {2020},
	note = {arXiv: 2010.05391},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{yang_dsm-net_2020,
	title = {{DSM}-{Net}: {Disentangled} {Structured} {Mesh} {Net} for {Controllable} {Generation} of {Fine} {Geometry}},
	shorttitle = {{DSM}-{Net}},
	url = {http://arxiv.org/abs/2008.05440},
	abstract = {3D shape generation is a fundamental operation in computer graphics. While significant progress has been made, especially with recent deep generative models, it remains a challenge to synthesize high-quality geometric shapes with rich detail and complex structure, in a controllable manner. To tackle this, we introduce DSM-Net, a deep neural network that learns a disentangled structured mesh representation for 3D shapes, where two key aspects of shapes, geometry and structure, are encoded in a synergistic manner to ensure plausibility of the generated shapes, while also being disentangled as much as possible. This supports a range of novel shape generation applications with intuitive control, such as interpolation of structure (geometry) while keeping geometry (structure) unchanged. To achieve this, we simultaneously learn structure and geometry through variational autoencoders (VAEs) in a hierarchical manner for both, with bijective mappings at each level. In this manner we effectively encode geometry and structure in separate latent spaces, while ensuring their compatibility: the structure is used to guide the geometry and vice versa. At the leaf level, the part geometry is represented using a conditional part VAE, to encode high-quality geometric details, guided by the structure context as the condition. Our method not only supports controllable generation applications, but also produces high-quality synthesized shapes, outperforming state-of-the-art methods.},
	language = {en},
	urldate = {2021-02-26},
	journal = {arXiv:2008.05440 [cs]},
	author = {Yang, Jie and Mo, Kaichun and Lai, Yu-Kun and Guibas, Leonidas J. and Gao, Lin},
	month = aug,
	year = {2020},
	note = {arXiv: 2008.05440},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
}

@inproceedings{yang_foldingnet_2018,
	address = {Salt Lake City, UT},
	title = {{FoldingNet}: {Point} {Cloud} {Auto}-{Encoder} via {Deep} {Grid} {Deformation}},
	isbn = {978-1-5386-6420-9},
	shorttitle = {{FoldingNet}},
	url = {https://ieeexplore.ieee.org/document/8578127/},
	doi = {10.1109/CVPR.2018.00029},
	language = {en},
	urldate = {2021-02-26},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Yang, Yaoqing and Feng, Chen and Shen, Yiru and Tian, Dong},
	month = jun,
	year = {2018},
	pages = {206--215},
}

@article{qin_pointdan_2019,
	title = {{PointDAN}: {A} {Multi}-{Scale} {3D} {Domain} {Adaption} {Network} for {Point} {Cloud} {Representation}},
	shorttitle = {{PointDAN}},
	url = {http://arxiv.org/abs/1911.02744},
	abstract = {Domain Adaptation (DA) approaches achieved significant improvements in a wide range of machine learning and computer vision tasks (i.e., classification, detection, and segmentation). However, as far as we are aware, there are few methods yet to achieve domain adaptation directly on 3D point cloud data. The unique challenge of point cloud data lies in its abundant spatial geometric information, and the semantics of the whole object is contributed by including regional geometric structures. Specifically, most general-purpose DA methods that struggle for global feature alignment and ignore local geometric information are not suitable for 3D domain alignment. In this paper, we propose a novel 3D Domain Adaptation Network for point cloud data (PointDAN). PointDAN jointly aligns the global and local features in multi-level. For local alignment, we propose Self-Adaptive (SA) node module with an adjusted receptive field to model the discriminative local structures for aligning domains. To represent hierarchically scaled features, node-attention module is further introduced to weight the relationship of SA nodes across objects and domains. For global alignment, an adversarial-training strategy is employed to learn and align global features across domains. Since there is no common evaluation benchmark for 3D point cloud DA scenario, we build a general benchmark (i.e., PointDA-10) extracted from three popular 3D object/scene datasets (i.e., ModelNet, ShapeNet and ScanNet) for cross-domain 3D objects classification fashion. Extensive experiments on PointDA-10 illustrate the superiority of our model over the state-of-the-art general-purpose DA methods.},
	language = {en},
	urldate = {2021-02-26},
	journal = {arXiv:1911.02744 [cs]},
	author = {Qin, Can and You, Haoxuan and Wang, Lichen and Kuo, C.-C. Jay and Fu, Yun},
	month = nov,
	year = {2019},
	note = {arXiv: 1911.02744},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@incollection{rodrigues_potential_2019,
	address = {Cham},
	title = {On the {Potential} and {Challenges} of {Neural} {Style} {Transfer} for {Three}-{Dimensional} {Shape} {Data}},
	isbn = {978-3-319-97772-0 978-3-319-97773-7},
	url = {http://link.springer.com/10.1007/978-3-319-97773-7_52},
	abstract = {In the ﬁeld of two-dimensional image and video processing, convolutional neural networks have been successfully applied to generate novel images by composing content and style of two different sources, a process called artistic or neural style transfer. However a usage of these methods for threedimensional objects is not straightforward due to the unstructured mesh representations of typical shape data. Hence efﬁcient geometry representations are required to use neural network based style transfer concepts for threedimensional shapes and to enable the fast creation of style options for instance in a product ideation process. In this paper an overview of current stateof-the-art shape representations is presented with respect to their applicability of neural style transfer on three-dimensional shape data. Combinations of threedimensional geometric representations with deep neural network architectures are evaluated towards their capability to store and reproduce content and style information based on previously proposed reconstruction tests.},
	language = {en},
	urldate = {2021-02-26},
	booktitle = {{EngOpt} 2018 {Proceedings} of the 6th {International} {Conference} on {Engineering} {Optimization}},
	publisher = {Springer International Publishing},
	author = {Friedrich, Timo and Aulig, Nikola and Menzel, Stefan},
	editor = {Rodrigues, H.C. and Herskovits, J. and Mota Soares, C.M. and Araújo, A.L. and Guedes, J.M. and Folgado, J.O. and Moleiro, F. and Madeira, J. F. A.},
	year = {2019},
	doi = {10.1007/978-3-319-97773-7_52},
	pages = {581--592},
}

@article{cao_neural_2019,
	title = {Neural {Style} {Transfer} for {Point} {Clouds}},
	url = {http://arxiv.org/abs/1903.05807},
	abstract = {How can we edit or transform the geometric or color property of a point cloud? In this study, we propose a neural style transfer method for point clouds which allows us to transfer the style of geometry or color from one point cloud either independently or simultaneously to another. This transfer is achieved by manipulating the content representations and Gram-based style representations extracted from a pre-trained PointNet-based classiﬁcation network for colored point clouds. As Gram-based style representation is invariant to the number or the order of points, the same method can be extended to transfer the style extracted from an image to the color expression of a point cloud by merely treating the image as a set of pixels. Experimental results demonstrate the capability of the proposed method for transferring style from either an image or a point cloud to another point cloud of a single object or even an indoor scene.},
	language = {en},
	urldate = {2021-02-26},
	journal = {arXiv:1903.05807 [cs]},
	author = {Cao, Xu and Wang, Weimin and Nagao, Katashi},
	month = mar,
	year = {2019},
	note = {arXiv: 1903.05807},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{cao_psnet_2020,
	address = {Snowmass Village, CO, USA},
	title = {{PSNet}: {A} {Style} {Transfer} {Network} for {Point} {Cloud} {Stylization} on {Geometry} and {Color}},
	isbn = {978-1-72816-553-0},
	shorttitle = {{PSNet}},
	url = {https://ieeexplore.ieee.org/document/9093513/},
	doi = {10.1109/WACV45572.2020.9093513},
	abstract = {We propose a neural style transfer method for colored point clouds which allows stylizing the geometry and/or color property of a point cloud from another. The stylization is achieved by manipulating the content representations and Gram-based style representations extracted from a pretrained PointNet-based classiﬁcation network for colored point clouds. As Gram-based style representation is invariant to the number or the order of points, the style can also be an image in the case of stylizing the color property of a point cloud by merely treating the image as a set of pixels. Experimental results and analysis demonstrate the capability of the proposed method for stylizing a point cloud either from another point cloud or an image.},
	language = {en},
	urldate = {2021-02-26},
	booktitle = {2020 {IEEE} {Winter} {Conference} on {Applications} of {Computer} {Vision} ({WACV})},
	publisher = {IEEE},
	author = {Cao, Xu and Wang, Weimin and Nagao, Katashi and Nakamura, Ryosuke},
	month = mar,
	year = {2020},
	pages = {3326--3334},
}

@article{fang_augmented_2019,
	title = {Augmented {LiDAR} {Simulator} for {Autonomous} {Driving}},
	url = {http://arxiv.org/abs/1811.07112},
	abstract = {In Autonomous Driving (AD), detection and tracking of obstacles on the roads is a critical task. Deep-learning based methods using annotated LiDAR data have been the most widely adopted approach for this. Unfortunately, annotating 3D point cloud is a very challenging, time- and money-consuming task. In this paper, we propose a novel LiDAR simulator that augments real point cloud with synthetic obstacles (e.g., cars, pedestrians, and other movable objects). Unlike previous simulators that entirely rely on CG models and game engines, our augmented simulator bypasses the requirement to create high-ﬁdelity background CAD models. Instead, we can simply deploy a vehicle with a LiDAR scanner to sweep the street of interests to obtain the background point cloud, based on which annotated point cloud can be automatically generated. This unique ”scanand-simulate” capability makes our approach scalable and practical, ready for large-scale industrial applications. In this paper, we describe our simulator in detail, in particular the placement of obstacles that is critical for performance enhancement. We show that detectors with our simulated LiDAR point cloud alone can perform comparably (within two percentage points) with these trained with real data. Mixing real and simulated data can achieve over 95\% accuracy.},
	language = {en},
	urldate = {2021-02-26},
	journal = {arXiv:1811.07112 [cs]},
	author = {Fang, Jin and Zhou, Dingfu and Yan, Feilong and Zhao, Tongtong and Zhang, Feihu and Ma, Yu and Wang, Liang and Yang, Ruigang},
	month = apr,
	year = {2019},
	note = {arXiv: 1811.07112},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{bhattacharjee_dunit_2020,
	address = {Seattle, WA, USA},
	title = {{DUNIT}: {Detection}-{Based} {Unsupervised} {Image}-to-{Image} {Translation}},
	isbn = {978-1-72817-168-5},
	shorttitle = {{DUNIT}},
	url = {https://ieeexplore.ieee.org/document/9157516/},
	doi = {10.1109/CVPR42600.2020.00484},
	abstract = {Image-to-image translation has made great strides in recent years, with current techniques being able to handle unpaired training images and to account for the multimodality of the translation problem. Despite this, most methods treat the image as a whole, which makes the results they produce for content-rich scenes less realistic. In this paper, we introduce a Detection-based Unsupervised Image-to-image Translation (DUNIT) approach that explicitly accounts for the object instances in the translation process. To this end, we extract separate representations for the global image and for the instances, which we then fuse into a common representation from which we generate the translated image. This allows us to preserve the detailed content of object instances, while still modeling the fact that we aim to produce an image of a single consistent scene. We introduce an instance consistency loss to maintain the coherence between the detections. Furthermore, by incorporating a detector into our architecture, we can still exploit object instances at test time. As evidenced by our experiments, this allows us to outperform the state-of-the-art unsupervised image-to-image translation methods. Furthermore, our approach can also be used as an unsupervised domain adaptation strategy for object detection, and it also achieves state-of-the-art performance on this task.},
	language = {en},
	urldate = {2021-02-26},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Bhattacharjee, Deblina and Kim, Seungryong and Vizier, Guillaume and Salzmann, Mathieu},
	month = jun,
	year = {2020},
	pages = {4786--4795},
}

@inproceedings{saleh_domain_2019,
	address = {Seoul, Korea (South)},
	title = {Domain {Adaptation} for {Vehicle} {Detection} from {Bird}'s {Eye} {View} {LiDAR} {Point} {Cloud} {Data}},
	isbn = {978-1-72815-023-9},
	url = {https://ieeexplore.ieee.org/document/9022327/},
	doi = {10.1109/ICCVW.2019.00404},
	abstract = {Point cloud data from 3D LiDAR sensors are one of the most crucial sensor modalities for versatile safety-critical applications such as self-driving vehicles. Since the annotations of point cloud data is an expensive and timeconsuming process, therefore recently the utilisation of simulated environments and 3D LiDAR sensors for this task started to get some popularity. However, the generated synthetic point cloud data are still missing the artefacts usually exist in point cloud data from real 3D LiDAR sensors. Thus, in this work, we are proposing a domain adaptation framework for bridging this gap between synthetic and real point cloud data. Our proposed framework is based on the deep cycle-consistent generative adversarial networks (CycleGAN) architecture. We have evaluated the performance of our proposed framework on the task of vehicle detection from a bird’s eye view (BEV) point cloud images coming from real 3D LiDAR sensors. The framework has shown competitive results with an improvement of more than 7\% in average precision score over other baseline approaches when tested on real BEV point cloud images.},
	language = {en},
	urldate = {2021-02-26},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} {Workshop} ({ICCVW})},
	publisher = {IEEE},
	author = {Saleh, Khaled and Abobakr, Ahmed and Attia, Mohammed and Iskander, Julie and Nahavandi, Darius and Hossny, Mohammed and Nahvandi, Saeid},
	month = oct,
	year = {2019},
	pages = {3235--3242},
}

@article{zhou_sparse-gan_2020,
	title = {Sparse-{GAN}: {Sparsity}-constrained {Generative} {Adversarial} {Network} for {Anomaly} {Detection} in {Retinal} {OCT} {Image}},
	shorttitle = {Sparse-{GAN}},
	url = {http://arxiv.org/abs/1911.12527},
	abstract = {With the development of convolutional neural network, deep learning has shown its success for retinal disease detection from optical coherence tomography (OCT) images. However, deep learning often relies on large scale labelled data for training, which is oftentimes challenging especially for disease with low occurrence. Moreover, a deep learning system trained from data-set with one or a few diseases is unable to detect other unseen diseases, which limits the practical usage of the system in disease screening. To address the limitation, we propose a novel anomaly detection framework termed Sparsity-constrained Generative Adversarial Network (Sparse-GAN) for disease screening where only healthy data are available in the training set. The contributions of SparseGAN are two-folds: 1) The proposed Sparse-GAN predicts the anomalies in latent space rather than image-level; 2) Sparse-GAN is constrained by a novel Sparsity Regularization Net. Furthermore, in light of the role of lesions for disease screening, we present to leverage on an anomaly activation map to show the heatmap of lesions. We evaluate our proposed Sparse-GAN on a publicly available dataset, and the results show that the proposed method outperforms the state-of-the-art methods.},
	language = {en},
	urldate = {2021-02-26},
	journal = {arXiv:1911.12527 [physics]},
	author = {Zhou, Kang and Gao, Shenghua and Cheng, Jun and Gu, Zaiwang and Fu, Huazhu and Tu, Zhi and Yang, Jianlong and Zhao, Yitian and Liu, Jiang},
	month = feb,
	year = {2020},
	note = {arXiv: 1911.12527},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing, Physics - Optics},
}

@inproceedings{mahdizadehaghdam_sparse_2019,
	address = {Seoul, Korea (South)},
	title = {Sparse {Generative} {Adversarial} {Network}},
	isbn = {978-1-72815-023-9},
	url = {https://ieeexplore.ieee.org/document/9022395/},
	doi = {10.1109/ICCVW.2019.00369},
	abstract = {We propose a new approach to Generative Adversarial Networks (GANs) to achieve an improved performance with additional robustness to its so-called and well recognized mode collapse. We ﬁrst proceed by mapping the desired data onto a frame-based space for a sparse representation to lift any limitation of small support features prior to learning the structure. To that end we start by dividing an image into multiple patches and modifying the role of the generative network from producing an entire image, at once, to creating a sparse representation vector for each image patch. We synthesize an entire image by multiplying generated sparse representations to a pre-trained dictionary and assembling the resulting patches. This approach restricts the output of the generator to a particular structure, obtained by imposing a Union of Subspaces (UoS) model to the original training data, leading to more realistic images, while maintaining a desired diversity. To further regularize GANs in generating high-quality images and to avoid the notorious mode-collapse problem, we introduce a third player in GANs, called reconstructor. This player utilizes an auto-encoding scheme to ensure that ﬁrst, the input-output relation in the generator is injective and second each real image corresponds to some input noise. We present a number of experiments, where the proposed algorithm shows a remarkably higher inception score compared to the equivalent conventional GANs.},
	language = {en},
	urldate = {2021-02-26},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} {Workshop} ({ICCVW})},
	publisher = {IEEE},
	author = {Mahdizadehaghdam, Shahin and Panahi, Ashkan and Krim, Hamid},
	month = oct,
	year = {2019},
	pages = {3063--3071},
}

@article{zhang_sparsely_2020,
	title = {Sparsely {Grouped} {Multi}-task {Generative} {Adversarial} {Networks} for {Facial} {Attribute} {Manipulation}},
	url = {http://arxiv.org/abs/1805.07509},
	abstract = {Recent Image-to-Image Translation algorithms have achieved signiﬁcant progress in neural style transfer and image attribute manipulation tasks. However, existing approaches require exhaustively labelling training data, which is labor demanding, difﬁcult to scale up, and hard to migrate into new domains. To overcome such a key limitation, we propose Sparsely Grouped Generative Adversarial Networks (SG-GAN) as a novel approach that can translate images on sparsely grouped datasets where only a few samples for training are labelled. Using a novel oneinput multi-output architecture, SG-GAN is well-suited for tackling sparsely grouped learning and multi-task learning. The proposed model can translate images among multiple groups using only a single commonly trained model. To experimentally validate advantages of the new model, we apply the proposed method to tackle a series of attribute manipulation tasks for facial images. Experimental results demonstrate that SG-GAN can generate image translation results of comparable quality with baselines methods on adequately labelled datasets and results of superior quality on sparsely grouped datasets. The ofﬁcial implementation is publicly available 1.},
	language = {en},
	urldate = {2021-02-26},
	journal = {arXiv:1805.07509 [cs]},
	author = {Zhang, Jichao and Shu, Yezhi and Xu, Songhua and Cao, Gongze and Zhong, Fan and Liu, Meng and Qin, Xueying},
	month = may,
	year = {2020},
	note = {arXiv: 1805.07509},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{wang_rethinking_2020,
	title = {Rethinking {Sampling} in {3D} {Point} {Cloud} {Generative} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/2006.07029},
	abstract = {In this paper, we examine the long-neglected yet important effects of point sampling patterns in point cloud GANs. Through extensive experiments, we show that sampling-insensitive discriminators (e.g. PointNet-Max) produce shape point clouds with point clustering artifacts while sampling-oversensitive discriminators (e.g. PointNet++, DGCNN) fail to guide valid shape generation. We propose the concept of sampling spectrum to depict the different sampling sensitivities of discriminators. We further study how different evaluation metrics weigh the sampling pattern against the geometry and propose several perceptual metrics forming a sampling spectrum of metrics. Guided by the proposed sampling spectrum, we discover a middle-point sampling-aware baseline discriminator, PointNet-Mix, which improves all existing point cloud generators by a large margin on samplingrelated metrics. We point out that, though recent research has been focused on the generator design, the main bottleneck of point cloud GAN actually lies in the discriminator design. Our work provides both suggestions and tools for building future discriminators. We will release the code to facilitate future research.},
	language = {en},
	urldate = {2021-02-26},
	journal = {arXiv:2006.07029 [cs, eess]},
	author = {Wang, He and Jiang, Zetian and Yi, Li and Mo, Kaichun and Su, Hao and Guibas, Leonidas J.},
	month = jun,
	year = {2020},
	note = {arXiv: 2006.07029},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing},
}

@article{wu_learning_nodate,
	title = {Learning a {Probabilistic} {Latent} {Space} of {Object} {Shapes} via {3D} {Generative}-{Adversarial} {Modeling}},
	language = {en},
	author = {Wu, Jiajun and Zhang, Chengkai and Xue, Tianfan and Freeman, William T and Tenenbaum, Joshua B},
	pages = {1},
}

@inproceedings{shu_3d_2019,
	address = {Seoul, Korea (South)},
	title = {{3D} {Point} {Cloud} {Generative} {Adversarial} {Network} {Based} on {Tree} {Structured} {Graph} {Convolutions}},
	isbn = {978-1-72814-803-8},
	url = {https://ieeexplore.ieee.org/document/9009495/},
	doi = {10.1109/ICCV.2019.00396},
	language = {en},
	urldate = {2021-02-26},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Shu, Dongwook and Park, Sung Woo and Kwon, Junseok},
	month = oct,
	year = {2019},
	pages = {3858--3867},
}

@article{yi_large-scale_2017,
	title = {Large-{Scale} {3D} {Shape} {Reconstruction} and {Segmentation} from {ShapeNet} {Core55}},
	url = {http://arxiv.org/abs/1710.06104},
	abstract = {We introduce a large-scale 3D shape understanding benchmark using data and annotation from ShapeNet 3D object database. The benchmark consists of two tasks: part-level segmentation of 3D shapes and 3D reconstruction from single view images. Ten teams have participated in the challenge and the best performing teams have outperformed state-of-the-art approaches on both tasks. A few novel deep learning architectures have been proposed on various 3D representations on both tasks. We report the techniques used by each team and the corresponding performances. In addition, we summarize the major discoveries from the reported results and possible trends for the future work in the ﬁeld.},
	language = {en},
	urldate = {2021-02-26},
	journal = {arXiv:1710.06104 [cs]},
	author = {Yi, Li and Shao, Lin and Savva, Manolis and Huang, Haibin and Zhou, Yang and Wang, Qirui and Graham, Benjamin and Engelcke, Martin and Klokov, Roman and Lempitsky, Victor and Gan, Yuan and Wang, Pengyu and Liu, Kun and Yu, Fenggen and Shui, Panpan and Hu, Bingyang and Zhang, Yan and Li, Yangyan and Bu, Rui and Sun, Mingchao and Wu, Wei and Jeong, Minki and Choi, Jaehoon and Kim, Changick and Geetchandra, Angom and Murthy, Narasimha and Ramu, Bhargava and Manda, Bharadwaj and Ramanathan, M. and Kumar, Gautam and Preetham, P. and Srivastava, Siddharth and Bhugra, Swati and Lall, Brejesh and Haene, Christian and Tulsiani, Shubham and Malik, Jitendra and Lafer, Jared and Jones, Ramsey and Li, Siyuan and Lu, Jie and Jin, Shi and Yu, Jingyi and Huang, Qixing and Kalogerakis, Evangelos and Savarese, Silvio and Hanrahan, Pat and Funkhouser, Thomas and Su, Hao and Guibas, Leonidas},
	month = oct,
	year = {2017},
	note = {arXiv: 1710.06104},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{you_survey_2020,
	address = {Kota Kinabalu, Malaysia},
	title = {A {Survey} on {Surface} {Reconstruction} {Techniques} for {Structured} and {Unstructured} {Data}},
	isbn = {978-1-72819-020-4},
	url = {https://ieeexplore.ieee.org/document/9293685/},
	doi = {10.1109/ICOS50156.2020.9293685},
	abstract = {Surface reconstruction of real-world objects is a commonly discussed topic in reverse engineering. Generally, 3D scanning technologies are used to scan the objects through multiple angles and represent them using point cloud. The point cloud can be either in structured or unstructured form which may contain problems such as noise, outliers and incomplete points. The point cloud is considered as unstructured form when it does not contain any connectivity information between adjacent points and structure information. Various types of surface reconstruction techniques are proposed to overcome the problems of point cloud and the limitations of existing techniques. Besides, soft computing techniques are also employed to enhance the performance and overcome the downsides of existing techniques. Therefore, the objective of this paper is to conduct a survey towards the existing techniques in the surface reconstruction on structured or unstructured data. Generally, this paper will only focus on the interpolation and approximation techniques, learning-based techniques, and soft computing techniques. Based on the analysis, it shows that learning-based techniques performed better compared to other techniques as they are able to handle the problem of unstructured point clouds. It can also form as hybrid techniques by integrating with other techniques which can improve its accuracy. The outcome of this paper can be used to assist the researchers in understanding and finding suitable surface reconstruction techniques in representing the objects and solving their case studies.},
	language = {en},
	urldate = {2021-02-26},
	booktitle = {2020 {IEEE} {Conference} on {Open} {Systems} ({ICOS})},
	publisher = {IEEE},
	author = {You, Cheng Chun and Lim, Seng Poh and Lim, Seng Chee and Tan, Joi San and Lee, Chen Kang and Khaw, Yen Min Jasmina},
	month = nov,
	year = {2020},
	pages = {37--42},
}

@article{yi_complete_2020,
	title = {Complete \& {Label}: {A} {Domain} {Adaptation} {Approach} to {Semantic} {Segmentation} of {LiDAR} {Point} {Clouds}},
	shorttitle = {Complete \& {Label}},
	url = {http://arxiv.org/abs/2007.08488},
	abstract = {We study an unsupervised domain adaptation problem for the semantic labeling of 3D point clouds, with a particular focus on domain discrepancies induced by different LiDAR sensors. Based on the observation that sparse 3D point clouds are sampled from 3D surfaces, we take a Complete and Label approach to recover the underlying surfaces before passing them to a segmentation network. Speciﬁcally, we design a Sparse Voxel Completion Network (SVCN) to complete the 3D surfaces of a sparse point cloud. Unlike semantic labels, to obtain training pairs for SVCN requires no manual labeling. We also introduce local adversarial learning to model the surface prior. The recovered 3D surfaces serve as a canonical domain, from which semantic labels can transfer across different LiDAR sensors. Experiments and ablation studies with our new benchmark for cross-domain semantic labeling of LiDAR data show that the proposed approach provides 8.2-36.6\% better performance than previous domain adaptation methods.},
	language = {en},
	urldate = {2021-02-26},
	journal = {arXiv:2007.08488 [cs]},
	author = {Yi, Li and Gong, Boqing and Funkhouser, Thomas},
	month = jul,
	year = {2020},
	note = {arXiv: 2007.08488},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{wen_geometry-aware_2020,
	title = {Geometry-{Aware} {Generation} of {Adversarial} {Point} {Clouds}},
	issn = {0162-8828, 2160-9292, 1939-3539},
	url = {https://ieeexplore.ieee.org/document/9294112/},
	doi = {10.1109/TPAMI.2020.3044712},
	abstract = {Machine learning models have been shown to be vulnerable to adversarial examples. While most of the existing methods for adversarial attack and defense work on the 2D image domain, a few recent attempts have been made to extend them to 3D point cloud data. However, adversarial results obtained by these methods typically contain point outliers, which are both noticeable and easy to defend against using the simple techniques of outlier removal. Motivated by the different mechanisms by which humans perceive 2D images and 3D shapes, in this paper we propose the new design of geometry-aware objectives, whose solutions favor (the discrete versions of) the desired surface properties of smoothness and fairness. To generate adversarial point clouds, we use a targeted attack misclassiﬁcation loss that supports continuous pursuit of increasingly malicious signals. Regularizing the targeted attack loss with our proposed geometry-aware objectives results in our proposed method, Geometry-Aware Adversarial Attack (GeoA3). The results of GeoA3 tend to be more harmful, arguably harder to defend against, and of the key adversarial characterization of being imperceptible to humans. While the main focus of this paper is to learn to generate adversarial point clouds, we also present a simple but effective algorithm termed Geo+A3-IterNormPro, with Iterative Normal Projection (IterNorPro) that solves a new objective function Geo+A3, towards surface-level adversarial attacks via generation of adversarial point clouds. We quantitatively evaluate our methods on both synthetic and physical objects in terms of attack success rate and geometric regularity. For a qualitative evaluation, we conduct subjective studies by collecting human preferences from Amazon Mechanical Turk. Comparative results in comprehensive experiments conﬁrm the advantages of our proposed methods. Our source codes are publicly available at https://github.com/Yuxin-Wen/GeoA3.},
	language = {en},
	urldate = {2021-02-26},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Wen, Yuxin and Lin, Jiehong and Chen, Ke and Chen, C. L. Philip and Jia, Kui},
	year = {2020},
	pages = {1--1},
}

@article{suo_lpd-ae_2020,
	title = {{LPD}-{AE}: {Latent} {Space} {Representation} of {Large}-{Scale} {3D} {Point} {Cloud}},
	volume = {8},
	issn = {2169-3536},
	shorttitle = {{LPD}-{AE}},
	url = {https://ieeexplore.ieee.org/document/9107146/},
	doi = {10.1109/ACCESS.2020.2999727},
	abstract = {The effective latent space representation of point cloud provides a foremost and fundamental manner that can be used for challenging tasks, including point cloud based place recognition and reconstruction, especially in large-scale dynamic environments. In this paper, we present a novel deep neural network, LPD-AE(Large-scale Place Description AutoEncoder Network), to obtain meaningful local and contextual features for the generation of latent space from 3D point cloud directly. The encoder network constructs the discriminative global descriptors to realize high accuracy and robust place recognition, which contributed by extracting the local neighbor geometric features and aggregating neighborhood relationships both in feature space and physical space. The decoder network performs hierarchical reconstruction on coarse key points and ultimately produce dense point clouds, which shows that it is capable of reconstructing a full point cloud frame from a single compact but high dimensional descriptor. Our proposed network demonstrates performance that is comparable to the state-of-the-art approaches. With the beneﬁt of the LPD-AE, many computationally complex tasks that rely directly on point clouds can be effortlessly conducted on latent space with lower memory costs, such as relocalization, loop closure detection, and map compression reconstruction. Comprehensive validations on Oxford RobotCar dataset, KITTI dataset, and our freshly collected dataset, which contains multiple trials of repeated routes in different weather and at different times, manifest its potency for real robotic and self-driving implementation. The source code is available at https://github.com/Suoivy/LPD-AE.},
	language = {en},
	urldate = {2021-02-26},
	journal = {IEEE Access},
	author = {Suo, Chuanzhe and Liu, Zhe and Mo, Lingfei and Liu, Yunhui},
	year = {2020},
	pages = {108402--108417},
}

@article{raina_fine_nodate,
	title = {Fine {Feature} {Reconstruction} in {Point} {Clouds} by {Adversarial} {Domain} {Translation}},
	abstract = {Point cloud neighborhoods are unstructured and often lacking in ﬁne details, particularly when the original surface is sparsely sampled. This has motivated the development of methods for reconstructing these ﬁne geometric features before the point cloud is converted into a mesh, usually by some form of upsampling of the point cloud. We present a novel data-driven approach to reconstructing ﬁne details of the underlying surfaces of point clouds at the local neighborhood level, along with normals and locations of edges. This is achieved by an innovative application of recent advances in domain translation using GANs. We “translate” local neighborhoods between two domains: point cloud neighborhoods and triangular mesh neighborhoods. This allows us to obtain some of the beneﬁts of meshes at training time, while still dealing with point clouds at the time of evaluation. By resampling the translated neighborhood, we can obtain a denser point cloud equipped with normals that allows the underlying surface to be easily reconstructed as a mesh. Our reconstructed meshes preserve ﬁne details of the original surface better than the state of the art in point cloud upsampling techniques, even at different input resolutions. In addition, the trained GAN can generalize to operate on low resolution point clouds even without being explicitly trained on low-resolution data. We also give an example demonstrating that the same domain translation approach we use for reconstructing local neighborhood geometry can also be used to estimate a scalar ﬁeld at the newly generated points, thus reducing the need for expensive recomputation of the scalar ﬁeld on the dense point cloud.},
	language = {en},
	author = {Raina, Prashant and Popa, Tiberiu and Mudur, Sudhir},
	pages = {13},
}

@article{lang_geometric_2020,
	title = {Geometric {Adversarial} {Attacks} and {Defenses} on {3D} {Point} {Clouds}},
	url = {http://arxiv.org/abs/2012.05657},
	abstract = {Deep neural networks are prone to adversarial examples that maliciously alter the network’s outcome. Due to the increasing popularity of 3D sensors in safety-critical systems and the vast deployment of deep learning models for 3D point sets, there is a growing interest in adversarial attacks and defenses for such models. So far, the research has focused on the semantic level, namely, deep point cloud classiﬁers. However, point clouds are also widely used in a geometric-related form that includes encoding and reconstructing the geometry.},
	language = {en},
	urldate = {2021-02-26},
	journal = {arXiv:2012.05657 [cs]},
	author = {Lang, Itai and Kotlicki, Uriel and Avidan, Shai},
	month = dec,
	year = {2020},
	note = {arXiv: 2012.05657},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{behley_semantickitti_2019,
	title = {{SemanticKITTI}: {A} {Dataset} for {Semantic} {Scene} {Understanding} of {LiDAR} {Sequences}},
	shorttitle = {{SemanticKITTI}},
	url = {http://arxiv.org/abs/1904.01416},
	abstract = {Semantic scene understanding is important for various applications. In particular, self-driving cars need a fine-grained understanding of the surfaces and objects in their vicinity. Light detection and ranging (LiDAR) provides precise geometric information about the environment and is thus a part of the sensor suites of almost all self-driving cars. Despite the relevance of semantic scene understanding for this application, there is a lack of a large dataset for this task which is based on an automotive LiDAR. In this paper, we introduce a large dataset to propel research on laser-based semantic segmentation. We annotated all sequences of the KITTI Vision Odometry Benchmark and provide dense point-wise annotations for the complete \$360{\textasciicircum}\{o\}\$ field-of-view of the employed automotive LiDAR. We propose three benchmark tasks based on this dataset: (i) semantic segmentation of point clouds using a single scan, (ii) semantic segmentation using multiple past scans, and (iii) semantic scene completion, which requires to anticipate the semantic scene in the future. We provide baseline experiments and show that there is a need for more sophisticated models to efficiently tackle these tasks. Our dataset opens the door for the development of more advanced methods, but also provides plentiful data to investigate new research directions.},
	language = {en},
	urldate = {2021-02-03},
	journal = {arXiv:1904.01416 [cs]},
	author = {Behley, Jens and Garbade, Martin and Milioto, Andres and Quenzel, Jan and Behnke, Sven and Stachniss, Cyrill and Gall, Juergen},
	month = aug,
	year = {2019},
	note = {arXiv: 1904.01416},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@article{biasutti_lu-net_2019,
	title = {{LU}-{Net}: {An} {Efficient} {Network} for {3D} {LiDAR} {Point} {Cloud} {Semantic} {Segmentation} {Based} on {End}-to-{End}-{Learned} {3D} {Features} and {U}-{Net}},
	shorttitle = {{LU}-{Net}},
	url = {http://arxiv.org/abs/1908.11656},
	abstract = {We propose LU-Net—for LiDAR U-Net, a new method for the semantic segmentation of a 3D LiDAR point cloud. Instead of applying some global 3D segmentation method such as PointNet, we propose an end-to-end architecture for LiDAR point cloud semantic segmentation that efﬁciently solves the problem as an image processing problem. We ﬁrst extract high-level 3D features for each point given its 3D neighbors. Then, these features are projected into a 2D multichannel range-image by considering the topology of the sensor. Thanks to these learned features and this projection, we can ﬁnally perform the segmentation using a simple U-Net segmentation network, which performs very well while being very efﬁcient. In this way, we can exploit both the 3D nature of the data and the speciﬁcity of the LiDAR sensor. This approach outperforms the state-of-the-art by a large margin on the KITTI dataset, as our experiments show. Moreover, this approach operates at 24fps on a single GPU. This is above the acquisition rate of common LiDAR sensors which makes it suitable for real-time applications.},
	language = {en},
	urldate = {2021-01-08},
	journal = {arXiv:1908.11656 [cs]},
	author = {Biasutti, Pierre and Lepetit, Vincent and Aujol, Jean-François and Brédif, Mathieu and Bugeau, Aurélie},
	month = aug,
	year = {2019},
	note = {arXiv: 1908.11656},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{nobis_deep_2020,
	title = {A {Deep} {Learning}-based {Radar} and {Camera} {Sensor} {Fusion} {Architecture} for {Object} {Detection}},
	url = {http://arxiv.org/abs/2005.07431},
	abstract = {Object detection in camera images, using deep learning has been proven successfully in recent years. Rising detection rates and computationally efficient network structures are pushing this technique towards application in production vehicles. Nevertheless, the sensor quality of the camera is limited in severe weather conditions and through increased sensor noise in sparsely lit areas and at night. Our approach enhances current 2D object detection networks by fusing camera data and projected sparse radar data in the network layers. The proposed CameraRadarFusionNet (CRF-Net) automatically learns at which level the fusion of the sensor data is most beneficial for the detection result. Additionally, we introduce BlackIn, a training strategy inspired by Dropout, which focuses the learning on a specific sensor type. We show that the fusion network is able to outperform a state-of-the-art image-only network for two different datasets. The code for this research will be made available to the public at: https://github.com/TUMFTM/CameraRadarFusionNet.},
	language = {en},
	urldate = {2020-12-31},
	journal = {arXiv:2005.07431 [cs]},
	author = {Nobis, Felix and Geisslinger, Maximilian and Weber, Markus and Betz, Johannes and Lienkamp, Markus},
	month = may,
	year = {2020},
	note = {arXiv: 2005.07431},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{tian_deep_2020,
	title = {Deep {Learning} on {Image} {Denoising}: {An} overview},
	shorttitle = {Deep {Learning} on {Image} {Denoising}},
	url = {http://arxiv.org/abs/1912.13171},
	abstract = {Deep learning techniques have received much attention in the area of image denoising. However, there are substantial differences in the various types of deep learning methods dealing with image denoising. Speciﬁcally, discriminative learning based on deep learning can ably address the issue of Gaussian noise. Optimization models based on deep learning are effective in estimating the real noise. However, there has thus far been little related research to summarize the different deep learning techniques for image denoising. In this paper, we offer a comparative study of deep techniques in image denoising. We ﬁrst classify the deep convolutional neural networks (CNNs) for additive white noisy images; the deep CNNs for real noisy images; the deep CNNs for blind denoising and the deep CNNs for hybrid noisy images, which represents the combination of noisy, blurred and low-resolution images. Then, we analyze the motivations and principles of the different types of deep learning methods. Next, we compare the state-of-the-art methods on public denoising datasets in terms of quantitative and qualitative analysis. Finally, we point out some potential challenges and directions of future research.},
	language = {en},
	urldate = {2020-12-31},
	journal = {arXiv:1912.13171 [cs, eess]},
	author = {Tian, Chunwei and Fei, Lunke and Zheng, Wenxian and Xu, Yong and Zuo, Wangmeng and Lin, Chia-Wen},
	month = aug,
	year = {2020},
	note = {arXiv: 1912.13171},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
}

@article{mao_multitask_2020,
	title = {Multitask {Learning} {Strengthens} {Adversarial} {Robustness}},
	url = {http://arxiv.org/abs/2007.07236},
	abstract = {Although deep networks achieve strong accuracy on a range of computer vision benchmarks, they remain vulnerable to adversarial attacks, where imperceptible input perturbations fool the network. We present both theoretical and empirical analyses that connect the adversarial robustness of a model to the number of tasks that it is trained on. Experiments on two datasets show that attack diﬃculty increases as the number of target tasks increase. Moreover, our results suggest that when models are trained on multiple tasks at once, they become more robust to adversarial attacks on individual tasks. While adversarial defense remains an open challenge, our results suggest that deep networks are vulnerable partly because they are trained on too few tasks.},
	language = {en},
	urldate = {2020-12-31},
	journal = {arXiv:2007.07236 [cs]},
	author = {Mao, Chengzhi and Gupta, Amogh and Nitin, Vikram and Ray, Baishakhi and Song, Shuran and Yang, Junfeng and Vondrick, Carl},
	month = sep,
	year = {2020},
	note = {arXiv: 2007.07236},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@article{zhang_polarnet_2020,
	title = {{PolarNet}: {An} {Improved} {Grid} {Representation} for {Online} {LiDAR} {Point} {Clouds} {Semantic} {Segmentation}},
	shorttitle = {{PolarNet}},
	url = {http://arxiv.org/abs/2003.14032},
	abstract = {The need for ﬁne-grained perception in autonomous driving systems has resulted in recently increased research on online semantic segmentation of single-scan LiDAR. Despite the emerging datasets and technological advancements, it remains challenging due to three reasons: (1) the need for near-real-time latency with limited hardware; (2) uneven or even long-tailed distribution of LiDAR points across space; and (3) an increasing number of extremely ﬁne-grained semantic classes. In an attempt to jointly tackle all the aforementioned challenges, we propose a new LiDAR-speciﬁc, nearest-neighbor-free segmentation algorithm — PolarNet. Instead of using common spherical or bird’s-eye-view projection, our polar bird’s-eye-view representation balances the points across grid cells in a polar coordinate system, indirectly aligning a segmentation network’s attention with the long-tailed distribution of the points along the radial axis. We ﬁnd that our encoding scheme greatly increases the mIoU in three drastically different segmentation datasets of real urban LiDAR single scans while retaining near real-time throughput.},
	language = {en},
	urldate = {2020-12-31},
	journal = {arXiv:2003.14032 [cs]},
	author = {Zhang, Yang and Zhou, Zixiang and David, Philip and Yue, Xiangyu and Xi, Zerong and Gong, Boqing and Foroosh, Hassan},
	month = apr,
	year = {2020},
	note = {arXiv: 2003.14032},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{huang_multimodal_2018,
	title = {Multimodal {Unsupervised} {Image}-to-{Image} {Translation}},
	url = {http://arxiv.org/abs/1804.04732},
	abstract = {Unsupervised image-to-image translation is an important and challenging problem in computer vision. Given an image in the source domain, the goal is to learn the conditional distribution of corresponding images in the target domain, without seeing any examples of corresponding image pairs. While this conditional distribution is inherently multimodal, existing approaches make an overly simpliﬁed assumption, modeling it as a deterministic one-to-one mapping. As a result, they fail to generate diverse outputs from a given source domain image. To address this limitation, we propose a Multimodal Unsupervised Image-to-image Translation (MUNIT) framework. We assume that the image representation can be decomposed into a content code that is domain-invariant, and a style code that captures domain-speciﬁc properties. To translate an image to another domain, we recombine its content code with a random style code sampled from the style space of the target domain. We analyze the proposed framework and establish several theoretical results. Extensive experiments with comparisons to state-of-the-art approaches further demonstrate the advantage of the proposed framework. Moreover, our framework allows users to control the style of translation outputs by providing an example style image. Code and pretrained models are available at https://github.com/nvlabs/MUNIT.},
	language = {en},
	urldate = {2020-12-31},
	journal = {arXiv:1804.04732 [cs, stat]},
	author = {Huang, Xun and Liu, Ming-Yu and Belongie, Serge and Kautz, Jan},
	month = aug,
	year = {2018},
	note = {arXiv: 1804.04732},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{zhu_unpaired_2020,
	title = {Unpaired {Image}-to-{Image} {Translation} using {Cycle}-{Consistent} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1703.10593},
	abstract = {Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain X to a target domain Y in the absence of paired examples. Our goal is to learn a mapping G : X → Y such that the distribution of images from G(X) is indistinguishable from the distribution Y using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping F : Y → X and introduce a cycle consistency loss to enforce F (G(X)) ≈ X (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transﬁguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.},
	language = {en},
	urldate = {2020-12-31},
	journal = {arXiv:1703.10593 [cs]},
	author = {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A.},
	month = aug,
	year = {2020},
	note = {arXiv: 1703.10593},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{alsaiari_image_2019,
	address = {Kahului, HI, USA},
	title = {Image {Denoising} {Using} {A} {Generative} {Adversarial} {Network}},
	isbn = {978-1-72813-323-2},
	url = {https://ieeexplore.ieee.org/document/8710893/},
	doi = {10.1109/INFOCT.2019.8710893},
	abstract = {Animation studios render 3D scenes using a technique called path tracing which enables them to create high quality photorealistic frames. Path tracing involves shooting 1000’s of rays into a pixel randomly (Monte Carlo) which will then hit the objects in the scene and, based on the reflective property of the object, these rays reflect or refract or get absorbed. The colors returned by these rays are averaged to determine the color of the pixel. This process is repeated for all the pixels. Due to the computational complexity it might take 8-16 hours to render a single frame. We implemented a neural network-based solution to reduce the time it takes to render a frame to less than a second using a generative adversarial network (GAN), once the network is trained. The main idea behind this proposed method is to render the image using a much smaller number of samples per pixel than is normal for path tracing (e.g., 1, 4, or 8 samples instead of, say, 32,000 samples) and then pass the noisy, incompletely rendered image to our network, which is capable of generating a highquality photorealistic image.},
	language = {en},
	urldate = {2020-12-10},
	booktitle = {2019 {IEEE} 2nd {International} {Conference} on {Information} and {Computer} {Technologies} ({ICICT})},
	publisher = {IEEE},
	author = {Alsaiari, Abeer and Rustagi, Ridhi and Alhakamy, A'aeshah and Thomas, Manu Mathew and Forbes, Angus G.},
	month = mar,
	year = {2019},
	pages = {126--132},
}

@inproceedings{heinzler_weather_2019,
	address = {Paris, France},
	title = {Weather {Influence} and {Classification} with {Automotive} {Lidar} {Sensors}},
	isbn = {978-1-72810-560-4},
	url = {https://ieeexplore.ieee.org/document/8814205/},
	doi = {10.1109/IVS.2019.8814205},
	abstract = {Lidar sensors are often used in mobile robots and autonomous vehicles to complement camera, radar and ultrasonic sensors for environment perception. Typically, perception algorithms are trained to only detect moving and static objects as well as ground estimation, but intentionally ignore weather effects to reduce false detections. In this work, we present an in-depth analysis of automotive lidar performance under harsh weather conditions, i.e. heavy rain and dense fog. An extensive data set has been recorded for various fog and rain conditions, which is the basis for the conducted indepth analysis of the point cloud under changing environmental conditions. In addition, we introduce a novel approach to detect and classify rain or fog with lidar sensors only and achieve an mean union over intersection of 97.14 \% for a data set in controlled environments. The analysis of weather inﬂuences on the performance of lidar sensors and the weather detection are important steps towards improving safety levels for autonomous driving in adverse weather conditions by providing reliable information to adapt vehicle behavior.},
	language = {en},
	urldate = {2020-11-30},
	booktitle = {2019 {IEEE} {Intelligent} {Vehicles} {Symposium} ({IV})},
	publisher = {IEEE},
	author = {Heinzler, Robin and Schindler, Philipp and Seekircher, Jurgen and Ritter, Werner and Stork, Wilhelm},
	month = jun,
	year = {2019},
	pages = {1527--1534},
}

@article{piewak_boosting_2018,
	title = {Boosting {LiDAR}-based {Semantic} {Labeling} by {Cross}-{Modal} {Training} {Data} {Generation}},
	url = {http://arxiv.org/abs/1804.09915},
	abstract = {Mobile robots and autonomous vehicles rely on multi-modal sensor setups to perceive and understand their surroundings. Aside from cameras, LiDAR sensors represent a central component of state-of-theart perception systems. In addition to accurate spatial perception, a comprehensive semantic understanding of the environment is essential for eﬃcient and safe operation. In this paper we present a novel deep neural network architecture called LiLaNet for point-wise, multi-class semantic labeling of semi-dense LiDAR data. The network utilizes virtual image projections of the 3D point clouds for eﬃcient inference. Further, we propose an automated process for large-scale cross-modal training data generation called Autolabeling, in order to boost semantic labeling performance while keeping the manual annotation eﬀort low. The eﬀectiveness of the proposed network architecture as well as the automated data generation process is demonstrated on a manually annotated ground truth dataset. LiLaNet is shown to signiﬁcantly outperform current state-ofthe-art CNN architectures for LiDAR data. Applying our automatically generated large-scale training data yields a boost of up to 14 percentage points compared to networks trained on manually annotated data only.},
	language = {en},
	urldate = {2020-11-26},
	journal = {arXiv:1804.09915 [cs]},
	author = {Piewak, Florian and Pinggera, Peter and Schäfer, Manuel and Peter, David and Schwarz, Beate and Schneider, Nick and Pfeiffer, David and Enzweiler, Markus and Zöllner, Marius},
	month = apr,
	year = {2018},
	note = {arXiv: 1804.09915},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{charron_-noising_nodate,
	title = {De-noising of {Lidar} {Point} {Clouds} {Corrupted} by {Snowfall}},
	abstract = {A common problem in autonomous driving is designing a system that can operate in adverse weather conditions. Falling rain and snow tends to corrupt sensor measurements, particularly for lidar sensors. Surprisingly, very little research has been published on methods to de-noise point clouds which are collected by lidar in rainy or snowy weather conditions. In this paper, we present a method for removing snow noise by processing point clouds using a 3D outlier detection algorithm. Our method, the dynamic radius outlier removal ﬁlter, accounts for the variation in point cloud density with increasing distance from the sensor, with the goal of removing the noise caused by snow while retaining detail in environmental features (which is necessary for autonomous localization and navigation).},
	language = {en},
	author = {Charron, Nicholas and Phillips, Stephen and Waslander, Steven L},
	pages = {8},
}

@article{zhao_ntgan_nodate,
	title = {{NTGAN}: {Learning} {Blind} {Image} {Denoising} without {Clean} {Reference}},
	abstract = {Recent studies on learning-based image denoising have achieved promising performance on various noise reduction tasks. Most of these deep denoisers are trained either under the supervision of clean references, or unsupervised on synthetic noise. The assumption with the synthetic noise leads to poor generalization when facing real photographs. To address this issue, we propose a novel deep unsupervised image-denoising method by regarding the noise reduction task as a special case of the noise transference task. Learning noise transference enables the network to acquire the denoising ability by only observing the corrupted samples. The results on real-world denoising benchmarks demonstrate that our proposed method achieves state-of-the-art performance on removing realistic noises, making it a potential solution to practical noise reduction problems.},
	language = {en},
	author = {Zhao, Rui},
	pages = {13},
}

@article{li_pu-gan_nodate,
	title = {{PU}-{GAN}: {A} {Point} {Cloud} {Upsampling} {Adversarial} {Network}},
	abstract = {Point clouds acquired from range scans are often sparse, noisy, and non-uniform. This paper presents a new point cloud upsampling network called PU-GAN 1, which is formulated based on a generative adversarial network (GAN), to learn a rich variety of point distributions from the latent space and upsample points over patches on object surfaces. To realize a working GAN network, we construct an up-down-up expansion unit in the generator for upsampling point features with error feedback and self-correction, and formulate a self-attention unit to enhance the feature integration. Further, we design a compound loss with adversarial, uniform and reconstruction terms, to encourage the discriminator to learn more latent patterns and enhance the output point distribution uniformity. Qualitative and quantitative evaluations demonstrate the quality of our results over the state-of-the-arts in terms of distribution uniformity, proximity-to-surface, and 3D reconstruction quality.},
	language = {en},
	author = {Li, Ruihui and Li, Xianzhi and Fu, Chi-Wing and Cohen-Or, Daniel and Heng, Pheng-Ann},
	pages = {10},
}

@article{vargas_rivero_weather_2020,
	title = {Weather {Classification} {Using} an {Automotive} {LIDAR} {Sensor} {Based} on {Detections} on {Asphalt} and {Atmosphere}},
	volume = {20},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/20/15/4306},
	doi = {10.3390/s20154306},
	abstract = {A semi-/autonomous driving car requires local weather information to identify if it is working inside its operational design domain and adapt itself accordingly. This information can be extracted from changes in the detections of a light detection and ranging (LIDAR) sensor. These changes are caused by modiﬁcations in the volumetric scattering of the atmosphere or surface reﬂection of objects in the ﬁeld of view of the LIDAR. In order to evaluate the use of an automotive LIDAR as a weather sensor, a LIDAR is placed outdoor in a ﬁxed position for a period of 9 months covering all seasons. As target, an asphalt region from a parking lot is chosen. The collected sensor raw data is labeled depending on the occurring weather conditions as: clear, rain, fog and snow, and the presence of sunlight: with or without background radiation. The inﬂuence of diﬀerent weather types and background radiations on the measurement results is analyzed and diﬀerent parameters are chosen in order to maximize the classiﬁcation accuracy. The classiﬁcation is done per frame in order to provide fast update rates while still keeping an F1 score higher than 80\%. Additionally, the ﬁeld of view is divided into two regions: atmosphere and street, where the inﬂuences of diﬀerent weather types are most notable. The resulting classiﬁers can be used separately or together increasing the versatility of the system. A possible way of extending the method for a moving platform and alternatives to virtually simulate the scene are also discussed.},
	language = {en},
	number = {15},
	urldate = {2020-11-14},
	journal = {Sensors},
	author = {Vargas Rivero, Jose Roberto and Gerbich, Thiemo and Teiluf, Valentina and Buschardt, Boris and Chen, Jia},
	month = aug,
	year = {2020},
	pages = {4306},
}

@article{park_fast_2020,
	title = {Fast and {Accurate} {Desnowing} {Algorithm} for {LiDAR} {Point} {Clouds}},
	volume = {8},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9180326/},
	doi = {10.1109/ACCESS.2020.3020266},
	abstract = {LiDAR sensors have the advantage of being able to generate high-resolution imaging quickly during both day and night; however, their performance is severely limited in adverse weather conditions such as snow, rain, and dense fog. Consequently, many researchers are actively working to overcome these limitations by applying sensor fusion with radar and optical cameras to LiDAR. While studies on the denoising of point clouds acquired by LiDAR in adverse weather have been conducted recently, the results are still insufﬁcient for application to autonomous vehicles because of speed and accuracy performance limitations. Therefore, we propose a new intensity-based ﬁlter that differs from the existing distance-based ﬁlter, which limits the speed. The proposed method showed overwhelming performance advantages in terms of both speed and accuracy by removing only snow particles while leaving important environmental features. The intensity criteria for snow removal were derived based on an analysis of the properties of laser light and snow particles.},
	language = {en},
	urldate = {2020-11-14},
	journal = {IEEE Access},
	author = {Park, Ji-Il and Park, Jihyuk and Kim, Kyung-Soo},
	year = {2020},
	pages = {160202--160212},
}

@article{yang_lanoising_nodate,
	title = {{LaNoising}: {A} {Data}-{Driven} {Approach} for 903nm {ToF} {LiDAR} {Performance} {Modeling} under {Fog}},
	abstract = {As a critical sensor for high-level autonomous vehicles, LiDAR’s limitations in adverse weather (e.g. rain, fog, snow, etc.) impede the deployment of self-driving cars in all weather conditions. In this paper, we model the performance of a popular 903nm ToF LiDAR under various fog conditions based on a LiDAR dataset collected in a well-controlled artiﬁcial fog chamber. Speciﬁcally, a two-stage data-driven method, called LaNoising (la for laser), is proposed for generating LiDAR measurements under fog conditions. In the ﬁrst stage, the Gaussian Process Regression (GPR) model is established to predict whether a laser can successfully output a true detection range or not, given certain fog visibility values. If not, then in the second stage, the Mixture Density Network (MDN) is used to provide a probability prediction of the noisy measurement range. The performance of the proposed method has been quantitatively and qualitatively evaluated. Experimental results show that our approach can provide a promising description of 903nm ToF LiDAR performance under fog.},
	language = {en},
	author = {Yang, Tao and Li, You and Ruichek, Yassine and Yan, Zhi},
	pages = {8},
}

@article{heinzler_cnn-based_2020,
	title = {{CNN}-based {Lidar} {Point} {Cloud} {De}-{Noising} in {Adverse} {Weather}},
	volume = {5},
	issn = {2377-3766, 2377-3774},
	url = {http://arxiv.org/abs/1912.03874},
	doi = {10.1109/LRA.2020.2972865},
	abstract = {Lidar sensors are frequently used in environment perception for autonomous vehicles and mobile robotics to complement camera, radar, and ultrasonic sensors. Adverse weather conditions are signiﬁcantly impacting the performance of lidarbased scene understanding by causing undesired measurement points that in turn effect missing detections and false positives. In heavy rain or dense fog, water drops could be misinterpreted as objects in front of the vehicle which brings a mobile robot to a full stop.},
	language = {en},
	number = {2},
	urldate = {2020-11-05},
	journal = {IEEE Robotics and Automation Letters},
	author = {Heinzler, Robin and Piewak, Florian and Schindler, Philipp and Stork, Wilhelm},
	month = apr,
	year = {2020},
	note = {arXiv: 1912.03874},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
	pages = {2514--2521},
}

@inproceedings{chen_image_2018,
	address = {Salt Lake City, UT},
	title = {Image {Blind} {Denoising} with {Generative} {Adversarial} {Network} {Based} {Noise} {Modeling}},
	isbn = {978-1-5386-6420-9},
	url = {https://ieeexplore.ieee.org/document/8578431/},
	doi = {10.1109/CVPR.2018.00333},
	abstract = {In this paper, we consider a typical image blind denoising problem, which is to remove unknown noise from noisy images. As we all know, discriminative learning based methods, such as DnCNN, can achieve state-of-the-art denoising results, but they are not applicable to this problem due to the lack of paired training data. To tackle the barrier, we propose a novel two-step framework. First, a Generative Adversarial Network (GAN) is trained to estimate the noise distribution over the input noisy images and to generate noise samples. Second, the noise patches sampled from the ﬁrst step are utilized to construct a paired training dataset, which is used, in turn, to train a deep Convolutional Neural Network (CNN) for denoising. Extensive experiments have been done to demonstrate the superiority of our approach in image blind denoising.},
	language = {en},
	urldate = {2020-11-05},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Chen, Jingwen and Chen, Jiawei and Chao, Hongyang and Yang, Ming},
	month = jun,
	year = {2018},
	pages = {3155--3164},
}

@article{xiao_generating_2019,
	title = {Generating {Adversarial} {Examples} with {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1801.02610},
	abstract = {Deep neural networks (DNNs) have been found to be vulnerable to adversarial examples resulting from adding small-magnitude perturbations to inputs. Such adversarial examples can mislead DNNs to produce adversary-selected results. Different attack strategies have been proposed to generate adversarial examples, but how to produce them with high perceptual quality and more efficiently requires more research efforts. In this paper, we propose AdvGAN to generate adversarial examples with generative adversarial networks (GANs), which can learn and approximate the distribution of original instances. For AdvGAN, once the generator is trained, it can generate adversarial perturbations efficiently for any instance, so as to potentially accelerate adversarial training as defenses. We apply AdvGAN in both semi-whitebox and black-box attack settings. In semi-whitebox attacks, there is no need to access the original target model after the generator is trained, in contrast to traditional white-box attacks. In black-box attacks, we dynamically train a distilled model for the black-box model and optimize the generator accordingly. Adversarial examples generated by AdvGAN on different target models have high attack success rate under state-of-the-art defenses compared to other attacks. Our attack has placed the first with 92.76\% accuracy on a public MNIST black-box attack challenge.},
	language = {en},
	urldate = {2020-11-05},
	journal = {arXiv:1801.02610 [cs, stat]},
	author = {Xiao, Chaowei and Li, Bo and Zhu, Jun-Yan and He, Warren and Liu, Mingyan and Song, Dawn},
	month = feb,
	year = {2019},
	note = {arXiv: 1801.02610},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Statistics - Machine Learning},
}

@incollection{vedaldi_advpc_2020,
	address = {Cham},
	title = {{AdvPC}: {Transferable} {Adversarial} {Perturbations} on {3D} {Point} {Clouds}},
	volume = {12357},
	isbn = {978-3-030-58609-6 978-3-030-58610-2},
	shorttitle = {{AdvPC}},
	url = {http://link.springer.com/10.1007/978-3-030-58610-2_15},
	abstract = {Deep neural networks are vulnerable to adversarial attacks, in which imperceptible perturbations to their input lead to erroneous network predictions. This phenomenon has been extensively studied in the image domain, and has only recently been extended to 3D point clouds. In this work, we present novel data-driven adversarial attacks against 3D point cloud networks. We aim to address the following problems in current 3D point cloud adversarial attacks: they do not transfer well between diﬀerent networks, and they are easy to defend against via simple statistical methods. To this extent, we develop a new point cloud attack (dubbed AdvPC) that exploits the input data distribution by adding an adversarial loss, after Auto-Encoder reconstruction, to the objective it optimizes. AdvPC leads to perturbations that are resilient against current defenses, while remaining highly transferable compared to state-of-theart attacks. We test AdvPC using four popular point cloud networks: PointNet, PointNet++ (MSG and SSG), and DGCNN. Our proposed attack increases the attack success rate by up to 40\% for those transferred to unseen networks (transferability), while maintaining a high success rate on the attacked network. AdvPC also increases the ability to break defenses by up to 38\% as compared to other baselines on the ModelNet40 dataset. The code is available at https://github.com/ajhamdi/AdvPC.},
	language = {en},
	urldate = {2020-11-05},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Hamdi, Abdullah and Rojas, Sara and Thabet, Ali and Ghanem, Bernard},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	doi = {10.1007/978-3-030-58610-2_15},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {241--257},
}

@article{rovid_towards_2020,
	title = {Towards {Reliable} {Multisensory} {Perception} and {Its} {Automotive} {Applications}},
	volume = {48},
	issn = {1587-3811, 0303-7800},
	url = {https://pp.bme.hu/tr/article/view/15921},
	doi = {10.3311/PPtr.15921},
	abstract = {Autonomous driving poses numerous challenging problems, one of which is perceiving and understanding the environment. Since selfdriving is safety critical and many actions taken during driving rely on the outcome of various perception algorithms (for instance all traffic participants and infrastructural objects in the vehicle's surroundings must reliably be recognized and localized), thus the perception might be considered as one of the most critical subsystems in an autonomous vehicle. Although the perception itself might further be decomposed into various sub-problems, such as object detection, lane detection, traffic sign detection, environment modeling, etc. In this paper the focus is on fusion models in general (giving support for multisensory data processing) and some related automotive applications such as object detection, traffic sign recognition, end-to-end driving models and an example of taking decisions in multi-criterial traffic situations that are complex for both human drivers and for the self-driving vehicles as well.},
	language = {en},
	number = {4},
	urldate = {2020-11-01},
	journal = {Periodica Polytechnica Transportation Engineering},
	author = {Rövid, András and Remeli, Viktor and Paufler, Norbert and Lengyel, Henrietta and Zöldy, Máté and Szalay, Zsolt},
	month = jul,
	year = {2020},
	pages = {334--340},
}

@article{wang_towards_2020,
	title = {Towards {Robust} {Sensor} {Fusion} in {Visual} {Perception}},
	url = {http://arxiv.org/abs/2006.13192},
	abstract = {We study the problem of robust sensor fusion in visual perception, especially under the autonomous driving settings. We evaluate the robustness of RGB camera and LiDAR sensor fusion for binary classiﬁcation and object detection. In this work, we are interested in the behavior of different fusion methods under adversarial attacks on different sensors. We ﬁrst train both classiﬁcation and detection models with early fusion and late fusion, then apply different combinations of adversarial attacks on both sensor inputs for evaluation. We also study the effectiveness of adversarial attacks with varying budgets. Experiment results show that while sensor fusion models are generally vulnerable to adversarial attacks, late fusion method is more robust than early fusion. The results also provide insights on further obtaining robust sensor fusion models.},
	language = {en},
	urldate = {2020-11-01},
	journal = {arXiv:2006.13192 [cs]},
	author = {Wang, Shaojie and Wu, Tong and Vorobeychik, Yevgeniy},
	month = jun,
	year = {2020},
	note = {arXiv: 2006.13192},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{tu_physically_2020,
	address = {Seattle, WA, USA},
	title = {Physically {Realizable} {Adversarial} {Examples} for {LiDAR} {Object} {Detection}},
	isbn = {978-1-72817-168-5},
	url = {https://ieeexplore.ieee.org/document/9156447/},
	doi = {10.1109/CVPR42600.2020.01373},
	abstract = {Modern autonomous driving systems rely heavily on deep learning models to process point cloud sensory data; meanwhile, deep models have been shown to be susceptible to adversarial attacks with visually imperceptible perturbations. Despite the fact that this poses a security concern for the self-driving industry, there has been very little exploration in terms of 3D perception, as most adversarial attacks have only been applied to 2D ﬂat images. In this paper, we address this issue and present a method to generate universal 3D adversarial objects to fool LiDAR detectors. In particular, we demonstrate that placing an adversarial object on the rooftop of any target vehicle to hide the vehicle entirely from LiDAR detectors with a success rate of 80\%. We report attack results on a suite of detectors using various input representation of point clouds. We also conduct a pilot study on adversarial defense using data augmentation. This is one step closer towards safer self-driving under unseen conditions from limited training data.},
	language = {en},
	urldate = {2020-11-01},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Tu, James and Ren, Mengye and Manivasagam, Sivabalan and Liang, Ming and Yang, Bin and Du, Richard and Cheng, Frank and Urtasun, Raquel},
	month = jun,
	year = {2020},
	pages = {13713--13722},
}

@article{ren_faster_2016,
	title = {Faster {R}-{CNN}: {Towards} {Real}-{Time} {Object} {Detection} with {Region} {Proposal} {Networks}},
	shorttitle = {Faster {R}-{CNN}},
	url = {http://arxiv.org/abs/1506.01497},
	abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
	urldate = {2020-10-28},
	journal = {arXiv:1506.01497 [cs]},
	author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
	month = jan,
	year = {2016},
	note = {arXiv: 1506.01497},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{li_robust_2019,
	title = {Robust {Adversarial} {Perturbation} on {Deep} {Proposal}-based {Models}},
	url = {http://arxiv.org/abs/1809.05962},
	abstract = {Adversarial noises are useful tools to probe the weakness of deep learning based computer vision algorithms. In this paper, we describe a robust adversarial perturbation (RAP) method to attack deep proposal-based object detectors and instance segmentation algorithms. Our method focuses on attacking the common component in these algorithms, namely Region Proposal Network (RPN), to universally degrade their performance in a black-box fashion. To do so, we design a loss function that combines a label loss and a novel shape loss, and optimize it with respect to image using a gradient based iterative algorithm. Evaluations are performed on the MS COCO 2014 dataset for the adversarial attacking of 6 state-of-the-art object detectors and 2 instance segmentation algorithms. Experimental results demonstrate the efﬁcacy of the proposed method.},
	language = {en},
	urldate = {2020-10-27},
	journal = {arXiv:1809.05962 [cs]},
	author = {Li, Yuezun and Tian, Daniel and Chang, Ming-Ching and Bian, Xiao and Lyu, Siwei},
	month = nov,
	year = {2019},
	note = {arXiv: 1809.05962},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{chen_shapeshifter_2019,
	title = {{ShapeShifter}: {Robust} {Physical} {Adversarial} {Attack} on {Faster} {R}-{CNN} {Object} {Detector}},
	volume = {11051},
	shorttitle = {{ShapeShifter}},
	url = {http://arxiv.org/abs/1804.05810},
	doi = {10.1007/978-3-030-10925-7_4},
	abstract = {Given the ability to directly manipulate image pixels in the digital input space, an adversary can easily generate imperceptible perturbations to fool a Deep Neural Network (DNN) image classiﬁer, as demonstrated in prior work. In this work, we propose ShapeShifter, an attack that tackles the more challenging problem of crafting physical adversarial perturbations to fool image-based object detectors like Faster R-CNN. Attacking an object detector is more difﬁcult than attacking an image classiﬁer, as it needs to mislead the classiﬁcation results in multiple bounding boxes with different scales. Extending the digital attack to the physical world adds another layer of difﬁculty, because it requires the perturbation to be robust enough to survive real-world distortions due to different viewing distances and angles, lighting conditions, and camera limitations. We show that the Expectation over Transformation technique, which was originally proposed to enhance the robustness of adversarial perturbations in image classiﬁcation, can be successfully adapted to the object detection setting. ShapeShifter can generate adversarially perturbed stop signs that are consistently mis-detected by Faster RCNN as other objects, posing a potential threat to autonomous vehicles and other safety-critical computer vision systems.},
	language = {en},
	urldate = {2020-10-27},
	journal = {arXiv:1804.05810 [cs, stat]},
	author = {Chen, Shang-Tse and Cornelius, Cory and Martin, Jason and Chau, Duen Horng},
	year = {2019},
	note = {arXiv: 1804.05810},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {52--68},
}

@article{wei_transferable_2019,
	title = {Transferable {Adversarial} {Attacks} for {Image} and {Video} {Object} {Detection}},
	url = {http://arxiv.org/abs/1811.12641},
	abstract = {Identifying adversarial examples is beneﬁcial for understanding deep networks and developing robust models. However, existing attacking methods for image object detection have two limitations: weak transferability—the generated adversarial examples often have a low success rate to attack other kinds of detection methods, and high computation cost—they need much time to deal with video data, where many frames need polluting. To address these issues, we present a generative method to obtain adversarial images and videos, thereby significantly reducing the processing time. To enhance transferability, we manipulate the feature maps extracted by a feature network, which usually constitutes the basis of object detectors. Our method is based on the Generative Adversarial Network (GAN) framework, where we combine a high-level class loss and a low-level feature loss to jointly train the adversarial example generator. Experimental results on PASCAL VOC and ImageNet VID datasets show that our method efﬁciently generates image and video adversarial examples, and more importantly, these adversarial examples have better transferability, therefore being able to simultaneously attack two kinds of representative object detection models: proposal based models like FasterRCNN and regression based models like SSD.},
	language = {en},
	urldate = {2020-10-24},
	journal = {arXiv:1811.12641 [cs]},
	author = {Wei, Xingxing and Liang, Siyuan and Chen, Ning and Cao, Xiaochun},
	month = may,
	year = {2019},
	note = {arXiv: 1811.12641},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{xie_adversarial_2017,
	title = {Adversarial {Examples} for {Semantic} {Segmentation} and {Object} {Detection}},
	url = {http://arxiv.org/abs/1703.08603},
	abstract = {It has been well demonstrated that adversarial examples, i.e., natural images with visually imperceptible perturbations added, cause deep networks to fail on image classiﬁcation. In this paper, we extend adversarial examples to semantic segmentation and object detection which are much more difﬁcult. Our observation is that both segmentation and detection are based on classifying multiple targets on an image (e.g., the target is a pixel or a receptive ﬁeld in segmentation, and an object proposal in detection). This inspires us to optimize a loss function over a set of pixels/proposals for generating adversarial perturbations. Based on this idea, we propose a novel algorithm named Dense Adversary Generation (DAG), which generates a large family of adversarial examples, and applies to a wide range of state-of-the-art deep networks for segmentation and detection. We also ﬁnd that the adversarial perturbations can be transferred across networks with different training data, based on different architectures, and even for different recognition tasks. In particular, the transferability across networks with the same architecture is more signiﬁcant than in other cases. Besides, summing up heterogeneous perturbations often leads to better transfer performance, which provides an effective method of blackbox adversarial attack.},
	language = {en},
	urldate = {2020-10-24},
	journal = {arXiv:1703.08603 [cs]},
	author = {Xie, Cihang and Wang, Jianyu and Zhang, Zhishuai and Zhou, Yuyin and Xie, Lingxi and Yuille, Alan},
	month = jul,
	year = {2017},
	note = {arXiv: 1703.08603},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{qi_pointnet_2017,
	title = {{PointNet}++: {Deep} {Hierarchical} {Feature} {Learning} on {Point} {Sets} in a {Metric} {Space}},
	shorttitle = {{PointNet}++},
	url = {http://arxiv.org/abs/1706.02413},
	abstract = {Few prior works study deep learning on point sets. PointNet [20] is a pioneer in this direction. However, by design PointNet does not capture local structures induced by the metric space points live in, limiting its ability to recognize ﬁne-grained patterns and generalizability to complex scenes. In this work, we introduce a hierarchical neural network that applies PointNet recursively on a nested partitioning of the input point set. By exploiting metric space distances, our network is able to learn local features with increasing contextual scales. With further observation that point sets are usually sampled with varying densities, which results in greatly decreased performance for networks trained on uniform densities, we propose novel set learning layers to adaptively combine features from multiple scales. Experiments show that our network called PointNet++ is able to learn deep point set features efﬁciently and robustly. In particular, results signiﬁcantly better than state-of-the-art have been obtained on challenging benchmarks of 3D point clouds.},
	language = {en},
	urldate = {2020-08-01},
	journal = {arXiv:1706.02413 [cs]},
	author = {Qi, Charles R. and Yi, Li and Su, Hao and Guibas, Leonidas J.},
	month = jun,
	year = {2017},
	note = {arXiv: 1706.02413},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{vora_pointpainting_2020,
	title = {{PointPainting}: {Sequential} {Fusion} for {3D} {Object} {Detection}},
	shorttitle = {{PointPainting}},
	url = {http://arxiv.org/abs/1911.10150},
	abstract = {Camera and lidar are important sensor modalities for robotics in general and self-driving cars in particular. The sensors provide complementary information offering an opportunity for tight sensor-fusion. Surprisingly, lidar-only methods outperform fusion methods on the main benchmark datasets, suggesting a gap in the literature. In this work, we propose PointPainting: a sequential fusion method to ﬁll this gap. PointPainting works by projecting lidar points into the output of an image-only semantic segmentation network and appending the class scores to each point. The appended (painted) point cloud can then be fed to any lidaronly method. Experiments show large improvements on three different state-of-the art methods, Point-RCNN, VoxelNet and PointPillars on the KITTI and nuScenes datasets. The painted version of PointRCNN represents a new state of the art on the KITTI leaderboard for the bird’s-eye view detection task. In ablation, we study how the effects of Painting depends on the quality and format of the semantic segmentation output, and demonstrate how latency can be minimized through pipelining.},
	language = {en},
	urldate = {2020-07-31},
	journal = {arXiv:1911.10150 [cs, eess, stat]},
	author = {Vora, Sourabh and Lang, Alex H. and Helou, Bassam and Beijbom, Oscar},
	month = may,
	year = {2020},
	note = {arXiv: 1911.10150},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing, Statistics - Machine Learning},
}

@article{wang_pillar-based_2020,
	title = {Pillar-based {Object} {Detection} for {Autonomous} {Driving}},
	url = {http://arxiv.org/abs/2007.10323},
	abstract = {We present a simple and ﬂexible object detection framework optimized for autonomous driving. Building on the observation that point clouds in this application are extremely sparse, we propose a practical pillar-based approach to ﬁx the imbalance issue caused by anchors. In particular, our algorithm incorporates a cylindrical projection into multiview feature learning, predicts bounding box parameters per pillar rather than per point or per anchor, and includes an aligned pillar-to-point projection module to improve the ﬁnal prediction. Our anchor-free approach avoids hyperparameter search associated with past methods, simplifying 3D object detection while signiﬁcantly improving upon state-of-the-art.},
	language = {en},
	urldate = {2020-07-31},
	journal = {arXiv:2007.10323 [cs]},
	author = {Wang, Yue and Fathi, Alireza and Kundu, Abhijit and Ross, David and Pantofaru, Caroline and Funkhouser, Thomas and Solomon, Justin},
	month = jul,
	year = {2020},
	note = {arXiv: 2007.10323},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@article{hahner_quantifying_2020,
	title = {Quantifying {Data} {Augmentation} for {LiDAR} based {3D} {Object} {Detection}},
	url = {http://arxiv.org/abs/2004.01643},
	abstract = {In this work, we shed light on different data augmentation techniques commonly used in Light Detection and Ranging (LiDAR) based 3D Object Detection. We, therefore, utilize a state of the art voxel based 3D Object Detection pipeline called PointPillars [1] and carry out our experiments on the well established KITTI [2] dataset. We investigate a variety of global and local augmentation techniques, where global augmentation techniques are applied to the entire point cloud of a scene and local augmentation techniques are only applied to points belonging to individual objects in the scene. Our ﬁndings show that both types of data augmentation can lead to performance increases, but it also turns out, that some augmentation techniques, such as individual object translation, for example, can be counterproductive and can hurt overall performance. We show that when we apply our ﬁndings to the data augmentation policy of PointPillars [1] we can easily increase its performance by up to 2\%. In order to provide reproducibility, our code will be publicly available at www.trace.ethz.ch/3D Object Detection.},
	language = {en},
	urldate = {2020-06-18},
	journal = {arXiv:2004.01643 [cs, eess]},
	author = {Hahner, Martin and Dai, Dengxin and Liniger, Alexander and Van Gool, Luc},
	month = apr,
	year = {2020},
	note = {arXiv: 2004.01643},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing},
}

@inproceedings{zhou_dup-net_2019,
	address = {Seoul, Korea (South)},
	title = {{DUP}-{Net}: {Denoiser} and {Upsampler} {Network} for {3D} {Adversarial} {Point} {Clouds} {Defense}},
	isbn = {978-1-72814-803-8},
	shorttitle = {{DUP}-{Net}},
	url = {https://ieeexplore.ieee.org/document/9010939/},
	doi = {10.1109/ICCV.2019.00205},
	abstract = {Neural networks are vulnerable to adversarial examples, which poses a threat to their application in security sensitive systems. We propose a Denoiser and UPsampler Network (DUP-Net) structure as defenses for 3D adversarial point cloud classiﬁcation, where the two modules reconstruct surface smoothness by dropping or adding points. In this paper, statistical outlier removal (SOR) and a datadriven upsampling network are considered as denoiser and upsampler respectively. Compared with baseline defenses, DUP-Net has three advantages. First, with DUP-Net as a defense, the target model is more robust to white-box adversarial attacks. Second, the statistical outlier removal provides added robustness since it is a non-differentiable denoising operation. Third, the upsampler network can be trained on a small dataset and defends well against adversarial attacks generated from other point cloud datasets. We conduct various experiments to validate that DUP-Net is very effective as defense in practice. Our best defense eliminates 83.8\% of C\&W and l2 loss based attack (point shifting), 50.0\% of C\&W and Hausdorff distance loss based attack (point adding) and 9.0\% of saliency map based attack (point dropping) under 200 dropped points on PointNet.},
	language = {en},
	urldate = {2020-05-07},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Zhou, Hang and Chen, Kejiang and Zhang, Weiming and Fang, Han and Zhou, Wenbo and Yu, Nenghai},
	month = oct,
	year = {2019},
	pages = {1961--1970},
}

@article{zhou_lg-gan_nodate,
	title = {{LG}-{GAN}: {Label} {Guided} {Adversarial} {Network} for {Flexible} {Targeted} {Attack} of {Point} {Cloud}-based {Deep} {Networks}},
	abstract = {Deep neural networks have made tremendous progress in 3D point-cloud recognition. Recent works have shown that these 3D recognition networks are also vulnerable to adversarial samples produced from various attack methods, including optimization-based 3D Carlini-Wagner attack, gradient-based iterative fast gradient method, and skeleton-detach based point-dropping. However, after a careful analysis, these methods are either extremely slow because of the optimization/iterative scheme, or not ﬂexible to support targeted attack of a speciﬁc category. To overcome these shortcomings, this paper proposes a novel label guided adversarial network (LG-GAN) for real-time ﬂexible targeted point cloud attack. To the best of our knowledge, this is the ﬁrst generation based 3D point cloud attack method. By feeding the original point clouds and target attack label into LG-GAN, it can learn how to deform the point clouds to mislead the recognition network into the speciﬁc label only with a single forward pass. In detail, LGGAN ﬁrst leverages one multi-branch adversarial network to extract hierarchical features of the input point clouds, then incorporates the speciﬁed label information into multiple intermediate features using the label encoder. Finally, the encoded features will be fed into the coordinate reconstruction decoder to generate the target adversarial sample. By evaluating different point-cloud recognition models (e.g., PointNet, PointNet++ and DGCNN), we demonstrate that the proposed LG-GAN can support ﬂexible targeted attack on the ﬂy while guaranteeing good attack performance and higher efﬁciency simultaneously.},
	language = {en},
	author = {Zhou, Hang and Chen, Dongdong and Liao, Jing and Chen, Kejiang and Dong, Xiaoyi and Liu, Kunlin and Zhang, Weiming and Hua, Gang and Yu, Nenghai},
	pages = {10},
}

@article{liu_adversarial_2019,
	title = {Adversarial point perturbations on {3D} objects},
	url = {http://arxiv.org/abs/1908.06062},
	abstract = {The importance of training robust neural network grows as 3D data is increasingly utilized in deep learning for vision tasks, like autonomous driving. We examine this problem from the perspective of the attacker, which is necessary in understanding how neural networks can be exploited, and thus defended. More specifically, we propose adversarial attacks based on solving different optimization problems, like minimizing the perceptibility of our generated adversarial examples, or maintaining a uniform density distribution of points across the adversarial object surfaces. Our four proposed algorithms for attacking 3D point cloud classification are all highly successful on existing neural networks, and we find that some of them are even effective against previously proposed point removal defenses.},
	language = {en},
	urldate = {2020-04-24},
	journal = {arXiv:1908.06062 [cs, eess, stat]},
	author = {Liu, Daniel and Yu, Ronald and Su, Hao},
	month = aug,
	year = {2019},
	note = {arXiv: 1908.06062},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing, Statistics - Machine Learning},
}

@article{liu_extending_2019,
	title = {Extending {Adversarial} {Attacks} and {Defenses} to {Deep} {3D} {Point} {Cloud} {Classifiers}},
	url = {http://arxiv.org/abs/1901.03006},
	abstract = {3D object classification and segmentation using deep neural networks has been extremely successful. As the problem of identifying 3D objects has many safety-critical applications, the neural networks have to be robust against adversarial changes to the input data set. There is a growing body of research on generating human-imperceptible adversarial attacks and defenses against them in the 2D image classification domain. However, 3D objects have various differences with 2D images, and this specific domain has not been rigorously studied so far. We present a preliminary evaluation of adversarial attacks on deep 3D point cloud classifiers, namely PointNet and PointNet++, by evaluating both white-box and black-box adversarial attacks that were proposed for 2D images and extending those attacks to reduce the perceptibility of the perturbations in 3D space. We also show the high effectiveness of simple defenses against those attacks by proposing new defenses that exploit the unique structure of 3D point clouds. Finally, we attempt to explain the effectiveness of the defenses through the intrinsic structures of both the point clouds and the neural network architectures. Overall, we find that networks that process 3D point cloud data are weak to adversarial attacks, but they are also more easily defensible compared to 2D image classifiers. Our investigation will provide the groundwork for future studies on improving the robustness of deep neural networks that handle 3D data.},
	language = {en},
	urldate = {2020-04-21},
	journal = {arXiv:1901.03006 [cs, stat]},
	author = {Liu, Daniel and Yu, Ronald and Su, Hao},
	month = jun,
	year = {2019},
	note = {arXiv: 1901.03006},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{zhao_isometry_2020,
	title = {On {Isometry} {Robustness} of {Deep} {3D} {Point} {Cloud} {Models} under {Adversarial} {Attacks}},
	url = {http://arxiv.org/abs/2002.12222},
	abstract = {While deep learning in 3D domain has achieved revolutionary performance in many tasks, the robustness of these models has not been sufﬁciently studied or explored. Regarding the 3D adversarial samples, most existing works focus on manipulation of local points, which may fail to invoke the global geometry properties, like robustness under linear projection that preserves the Euclidean distance, i.e., isometry. In this work, we show that existing state-ofthe-art deep 3D models are extremely vulnerable to isometry transformations. Armed with the Thompson Sampling, we develop a black-box attack with success rate over 95\% on ModelNet40 data set. Incorporating with the Restricted Isometry Property, we propose a novel framework of whitebox attack on top of spectral norm based perturbation. In contrast to previous works, our adversarial samples are experimentally shown to be strongly transferable. Evaluated on a sequence of prevailing 3D models, our white-box attack achieves success rates from 98.88\% to 100\%. It maintains a successful attack rate over 95\% even within an imperceptible rotation range [±2.81◦].},
	language = {en},
	urldate = {2020-04-08},
	journal = {arXiv:2002.12222 [cs, stat]},
	author = {Zhao, Yue and Wu, Yuwei and Chen, Caihua and Lim, Andrew},
	month = mar,
	year = {2020},
	note = {arXiv: 2002.12222},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{yang_adversarial_2019,
	title = {Adversarial {Attack} and {Defense} on {Point} {Sets}},
	url = {http://arxiv.org/abs/1902.10899},
	abstract = {Emergence of the utility of 3D point cloud data in critical vision tasks (e.g., ADAS) urges researchers to pay more attention to the robustness of 3D representations and deep networks. To this end, we develop an attack and defense scheme, dedicated to 3D point cloud data, for preventing 3D point clouds from manipulated as well as pursuing noise-tolerable 3D representation. A set of novel 3D point cloud attack operations are proposed via pointwise gradient perturbation and adversarial point attachment / detachment. We then develop a flexible perturbation-measurement scheme for 3D point cloud data to detect potential attack data or noisy sensing data. Extensive experimental results on common point cloud benchmarks demonstrate the validity of the proposed 3D attack and defense framework.},
	language = {en},
	urldate = {2020-03-11},
	journal = {arXiv:1902.10899 [cs]},
	author = {Yang, Jiancheng and Zhang, Qiang and Fang, Rongyao and Ni, Bingbing and Liu, Jinxian and Tian, Qi},
	month = feb,
	year = {2019},
	note = {arXiv: 1902.10899},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@article{cao_adversarial_2019,
	title = {Adversarial {Objects} {Against} {LiDAR}-{Based} {Autonomous} {Driving} {Systems}},
	url = {http://arxiv.org/abs/1907.05418},
	abstract = {Deep neural networks (DNNs) are found to be vulnerable against adversarial examples, which are carefully crafted inputs with a small magnitude of perturbation aiming to induce arbitrarily incorrect predictions. Recent studies show that adversarial examples can pose a threat to real-world security-critical applications: a “physically adversarial Stop Sign” can be synthesized such that the autonomous driving cars will misrecognize it as others (e.g., a speed limit sign). However, these image-based adversarial examples cannot easily alter 3D scans such as widely equipped LiDAR or radar on autonomous vehicles. In this paper, we reveal the potential vulnerabilities of LiDAR-based autonomous driving detection systems, by proposing an optimization based approach LiDAR-Adv to generate real-world adversarial objects that can evade the LiDAR-based detection systems under various conditions. We ﬁrst explore the vulnerabilities of LiDAR using an evolutionbased blackbox attack algorithm, and then propose a strong attack strategy, using our gradient-based approach LiDAR-Adv. We test the generated adversarial objects on the Baidu Apollo autonomous driving platform and show that such physical systems are indeed vulnerable to the proposed attacks. We 3D-print our adversarial objects and perform physical experiments with LiDAR equipped cars to illustrate the effectiveness of LiDARAdv. Please ﬁnd more visualizations and physical experimental results on this website: https://sites.google.com/view/lidar-adv.},
	language = {en},
	urldate = {2020-03-11},
	journal = {arXiv:1907.05418 [cs, stat]},
	author = {Cao, Yulong and Xiao, Chaowei and Yang, Dawei and Fang, Jing and Yang, Ruigang and Liu, Mingyan and Li, Bo},
	month = jul,
	year = {2019},
	note = {arXiv: 1907.05418},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{xiang_generating_2019,
	title = {Generating {3D} {Adversarial} {Point} {Clouds}},
	url = {http://arxiv.org/abs/1809.07016},
	abstract = {Deep neural networks are known to be vulnerable to adversarial examples which are carefully crafted instances to cause the models to make wrong predictions. While adversarial examples for 2D images and CNNs have been extensively studied, less attention has been paid to 3D data such as point clouds. Given many safety-critical 3D applications such as autonomous driving, it is important to study how adversarial point clouds could affect current deep 3D models. In this work, we propose several novel algorithms to craft adversarial point clouds against PointNet, a widely used deep neural network for point cloud processing. Our algorithms work in two ways: adversarial point perturbation and adversarial point generation. For point perturbation, we shift existing points negligibly. For point generation, we generate either a set of independent and scattered points or a small number (1-3) of point clusters with meaningful shapes such as balls and airplanes which could be hidden in the human psyche. In addition, we formulate six perturbation measurement metrics tailored to the attacks in point clouds and conduct extensive experiments to evaluate the proposed algorithms on the ModelNet40 3D shape classiﬁcation dataset. Overall, our attack algorithms achieve a success rate higher than 99\% for all targeted attacks 1.},
	language = {en},
	urldate = {2020-03-11},
	journal = {arXiv:1809.07016 [cs]},
	author = {Xiang, Chong and Qi, Charles R. and Li, Bo},
	month = jul,
	year = {2019},
	note = {arXiv: 1809.07016},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@article{qi_pointnet_2017-1,
	title = {{PointNet}: {Deep} {Learning} on {Point} {Sets} for {3D} {Classification} and {Segmentation}},
	shorttitle = {{PointNet}},
	url = {http://arxiv.org/abs/1612.00593},
	abstract = {Point cloud is an important type of geometric data structure. Due to its irregular format, most researchers transform such data to regular 3D voxel grids or collections of images. This, however, renders data unnecessarily voluminous and causes issues. In this paper, we design a novel type of neural network that directly consumes point clouds, which well respects the permutation invariance of points in the input. Our network, named PointNet, provides a uniﬁed architecture for applications ranging from object classiﬁcation, part segmentation, to scene semantic parsing. Though simple, PointNet is highly efﬁcient and effective. Empirically, it shows strong performance on par or even better than state of the art. Theoretically, we provide analysis towards understanding of what the network has learnt and why the network is robust with respect to input perturbation and corruption.},
	language = {en},
	urldate = {2020-03-03},
	journal = {arXiv:1612.00593 [cs]},
	author = {Qi, Charles R. and Su, Hao and Mo, Kaichun and Guibas, Leonidas J.},
	month = apr,
	year = {2017},
	note = {arXiv: 1612.00593},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{zheng_pointcloud_2019,
	title = {{PointCloud} {Saliency} {Maps}},
	url = {http://arxiv.org/abs/1812.01687},
	abstract = {3D point-cloud recognition with PointNet and its variants has received remarkable progress. A missing ingredient, however, is the ability to automatically evaluate point-wise importance w.r.t.{\textbackslash}! classification performance, which is usually reflected by a saliency map. A saliency map is an important tool as it allows one to perform further processes on point-cloud data. In this paper, we propose a novel way of characterizing critical points and segments to build point-cloud saliency maps. Our method assigns each point a score reflecting its contribution to the model-recognition loss. The saliency map explicitly explains which points are the key for model recognition. Furthermore, aggregations of highly-scored points indicate important segments/subsets in a point-cloud. Our motivation for constructing a saliency map is by point dropping, which is a non-differentiable operator. To overcome this issue, we approximate point-dropping with a differentiable procedure of shifting points towards the cloud centroid. Consequently, each saliency score can be efficiently measured by the corresponding gradient of the loss w.r.t the point under the spherical coordinates. Extensive evaluations on several state-of-the-art point-cloud recognition models, including PointNet, PointNet++ and DGCNN, demonstrate the veracity and generality of our proposed saliency map. Code for experiments is released on {\textbackslash}url\{https://github.com/tianzheng4/PointCloud-Saliency-Maps\}.},
	language = {en},
	urldate = {2020-03-02},
	journal = {arXiv:1812.01687 [cs]},
	author = {Zheng, Tianhang and Chen, Changyou and Yuan, Junsong and Li, Bo and Ren, Kui},
	month = sep,
	year = {2019},
	note = {arXiv: 1812.01687},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
}

@article{lang_pointpillars_2019,
	title = {{PointPillars}: {Fast} {Encoders} for {Object} {Detection} from {Point} {Clouds}},
	shorttitle = {{PointPillars}},
	url = {http://arxiv.org/abs/1812.05784},
	abstract = {Object detection in point clouds is an important aspect of many robotics applications such as autonomous driving. In this paper we consider the problem of encoding a point cloud into a format appropriate for a downstream detection pipeline. Recent literature suggests two types of encoders; ﬁxed encoders tend to be fast but sacriﬁce accuracy, while encoders that are learned from data are more accurate, but slower. In this work we propose PointPillars, a novel encoder which utilizes PointNets to learn a representation of point clouds organized in vertical columns (pillars). While the encoded features can be used with any standard 2D convolutional detection architecture, we further propose a lean downstream network. Extensive experimentation shows that PointPillars outperforms previous encoders with respect to both speed and accuracy by a large margin. Despite only using lidar, our full detection pipeline signiﬁcantly outperforms the state of the art, even among fusion methods, with respect to both the 3D and bird’s eye view KITTI benchmarks. This detection performance is achieved while running at 62 Hz: a 2 - 4 fold runtime improvement. A faster version of our method matches the state of the art at 105 Hz. These benchmarks suggest that PointPillars is an appropriate encoding for object detection in point clouds.},
	language = {en},
	urldate = {2020-03-02},
	journal = {arXiv:1812.05784 [cs, stat]},
	author = {Lang, Alex H. and Vora, Sourabh and Caesar, Holger and Zhou, Lubing and Yang, Jiong and Beijbom, Oscar},
	month = may,
	year = {2019},
	note = {arXiv: 1812.05784},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}
