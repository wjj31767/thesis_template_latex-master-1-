
@article{enzweiler_monocular_2009,
	title = {Monocular {Pedestrian} {Detection}: {Survey} and {Experiments}},
	volume = {31},
	issn = {0162-8828},
	shorttitle = {Monocular {Pedestrian} {Detection}},
	url = {http://ieeexplore.ieee.org/document/4657363/},
	doi = {10.1109/TPAMI.2008.260},
	abstract = {Pedestrian detection is a rapidly evolving area in computer vision with key applications in intelligent vehicles, surveillance, and advanced robotics. The objective of this paper is to provide an overview of the current state of the art from both methodological and experimental perspectives. The first part of the paper consists of a survey. We cover the main components of a pedestrian detection system and the underlying models. The second (and larger) part of the paper contains a corresponding experimental study. We consider a diverse set of state-of-the-art systems: wavelet-based AdaBoost cascade [74], HOG/linSVM [11], NN/LRF [75], and combined shape-texture detection [23]. Experiments are performed on an extensive data set captured onboard a vehicle driving through urban environment. The data set includes many thousands of training samples as well as a 27-minute test sequence involving more than 20,000 images with annotated pedestrian locations. We consider a generic evaluation setting and one specific to pedestrian detection onboard a vehicle. Results indicate a clear advantage of HOG/linSVM at higher image resolutions and lower processing speeds, and a superiority of the wavelet-based AdaBoost cascade approach at lower image resolutions and (near) real-time processing speeds. The data set (8.5 GB) is made public for benchmarking purposes.},
	language = {en},
	number = {12},
	urldate = {2021-06-13},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Enzweiler, M. and Gavrila, D.M.},
	month = dec,
	year = {2009},
	pages = {2179--2195},
}

@article{kurakinAdversarialMachineLearning2017,
	title = {Adversarial {Machine} {Learning} at {Scale}},
	url = {http://arxiv.org/abs/1611.01236},
	abstract = {Adversarial examples are malicious inputs designed to fool machine learning models. They often transfer from one model to another, allowing attackers to mount black box attacks without knowledge of the target model’s parameters. Adversarial training is the process of explicitly training a model on adversarial examples, in order to make it more robust to attack or to reduce its test error on clean inputs. So far, adversarial training has primarily been applied to small problems. In this research, we apply adversarial training to ImageNet (Russakovsky et al., 2014). Our contributions include: (1) recommendations for how to succesfully scale adversarial training to large models and datasets, (2) the observation that adversarial training confers robustness to single-step attack methods, (3) the ﬁnding that multi-step attack methods are somewhat less transferable than singlestep attack methods, so single-step attacks are the best for mounting black-box attacks, and (4) resolution of a “label leaking” effect that causes adversarially trained models to perform better on adversarial examples than on clean examples, because the adversarial example construction process uses the true label and the model can learn to exploit regularities in the construction process.},
	language = {en},
	urldate = {2021-06-13},
	journal = {arXiv:1611.01236 [cs, stat]},
	author = {Kurakin, Alexey and Goodfellow, Ian and Bengio, Samy},
	month = feb,
	year = {2017},
	note = {arXiv: 1611.01236},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{tramer_ensemble_2020,
	title = {Ensemble {Adversarial} {Training}: {Attacks} and {Defenses}},
	shorttitle = {Ensemble {Adversarial} {Training}},
	url = {http://arxiv.org/abs/1705.07204},
	abstract = {Adversarial examples are perturbed inputs designed to fool machine learning models. Adversarial training injects such examples into training data to increase robustness. To scale this technique to large datasets, perturbations are crafted using fast single-step methods that maximize a linear approximation of the model’s loss. We show that this form of adversarial training converges to a degenerate global minimum, wherein small curvature artifacts near the data points obfuscate a linear approximation of the loss. The model thus learns to generate weak perturbations, rather than defend against strong ones. As a result, we ﬁnd that adversarial training remains vulnerable to black-box attacks, where we transfer perturbations computed on undefended models, as well as to a powerful novel single-step attack that escapes the non-smooth vicinity of the input data via a small random step.},
	language = {en},
	urldate = {2021-06-13},
	journal = {arXiv:1705.07204 [cs, stat]},
	author = {Tramèr, Florian and Kurakin, Alexey and Papernot, Nicolas and Goodfellow, Ian and Boneh, Dan and McDaniel, Patrick},
	month = apr,
	year = {2020},
	note = {arXiv: 1705.07204},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{geiger_are_2012,
	address = {Providence, RI},
	title = {Are we ready for autonomous driving? {The} {KITTI} vision benchmark suite},
	isbn = {978-1-4673-1228-8 978-1-4673-1226-4 978-1-4673-1227-1},
	shorttitle = {Are we ready for autonomous driving?},
	url = {http://ieeexplore.ieee.org/document/6248074/},
	doi = {10.1109/CVPR.2012.6248074},
	language = {en},
	urldate = {2021-06-12},
	booktitle = {2012 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Geiger, A. and Lenz, P. and Urtasun, R.},
	month = jun,
	year = {2012},
	pages = {3354--3361},
}

@inproceedings{szegedy_going_2015,
	address = {Boston, MA, USA},
	title = {Going deeper with convolutions},
	isbn = {978-1-4673-6964-0},
	url = {http://ieeexplore.ieee.org/document/7298594/},
	doi = {10.1109/CVPR.2015.7298594},
	abstract = {We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classiﬁcation and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classiﬁcation and detection.},
	language = {en},
	urldate = {2021-06-09},
	booktitle = {2015 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Szegedy, Christian and {Wei Liu} and {Yangqing Jia} and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
	month = jun,
	year = {2015},
	pages = {1--9},
}

@article{rusu_towards_2008,
	title = {Towards {3D} {Point} cloud based object maps for household environments},
	volume = {56},
	issn = {09218890},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0921889008001140},
	doi = {10.1016/j.robot.2008.08.005},
	abstract = {This article investigates the problem of acquiring 3D object maps of indoor household environments, in particular kitchens. The objects modeled in these maps include cupboards, tables, drawers and shelves, which are of particular importance for a household robotic assistant. Our mapping approach is based on PCD (point cloud data) representations. Sophisticated interpretation methods operating on these representations eliminate noise and resample the data without deleting the important details, and interpret the improved point clouds in terms of rectangular planes and 3D geometric shapes. We detail the steps of our mapping approach and explain the key techniques that make it work. The novel techniques include statistical analysis, persistent histogram features estimation that allows for a consistent registration, resampling with additional robust fitting techniques, and segmentation of the environment into meaningful regions.},
	language = {en},
	number = {11},
	urldate = {2021-06-09},
	journal = {Robotics and Autonomous Systems},
	author = {Rusu, Radu Bogdan and Marton, Zoltan Csaba and Blodow, Nico and Dolha, Mihai and Beetz, Michael},
	month = nov,
	year = {2008},
	pages = {927--941},
}

@article{wicker_robustness_2019,
	title = {Robustness of {3D} {Deep} {Learning} in an {Adversarial} {Setting}},
	url = {http://arxiv.org/abs/1904.00923},
	abstract = {Understanding the spatial arrangement and nature of real-world objects is of paramount importance to many complex engineering tasks, including autonomous navigation. Deep learning has revolutionized state-of-the-art performance for tasks in 3D environments; however, relatively little is known about the robustness of these approaches in an adversarial setting. The lack of comprehensive analysis makes it difﬁcult to justify deployment of 3D deep learning models in real-world, safety-critical applications. In this work, we develop an algorithm for analysis of pointwise robustness of neural networks that operate on 3D data. We show that current approaches presented for understanding the resilience of state-of-the-art models vastly overestimate their robustness. We then use our algorithm to evaluate an array of state-of-the-art models in order to demonstrate their vulnerability to occlusion attacks. We show that, in the worst case, these networks can be reduced to 0\% classiﬁcation accuracy after the occlusion of at most 6.5\% of the occupied input space.},
	language = {en},
	urldate = {2021-06-09},
	journal = {arXiv:1904.00923 [cs]},
	author = {Wicker, Matthew and Kwiatkowska, Marta},
	month = apr,
	year = {2019},
	note = {arXiv: 1904.00923},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@article{song_constructing_2018,
	title = {Constructing {Unrestricted} {Adversarial} {Examples} with {Generative} {Models}},
	url = {http://arxiv.org/abs/1805.07894},
	abstract = {Adversarial examples are typically constructed by perturbing an existing data point within a small matrix norm, and current defense methods are focused on guarding against this type of attack. In this paper, we propose unrestricted adversarial examples, a new threat model where the attackers are not restricted to small normbounded perturbations. Different from perturbation-based attacks, we propose to synthesize unrestricted adversarial examples entirely from scratch using conditional generative models. Speciﬁcally, we ﬁrst train an Auxiliary Classiﬁer Generative Adversarial Network (AC-GAN) to model the class-conditional distribution over data samples. Then, conditioned on a desired class, we search over the AC-GAN latent space to ﬁnd images that are likely under the generative model and are misclassiﬁed by a target classiﬁer. We demonstrate through human evaluation that unrestricted adversarial examples generated this way are legitimate and belong to the desired class. Our empirical results on the MNIST, SVHN, and CelebA datasets show that unrestricted adversarial examples can bypass strong adversarial training and certiﬁed defense methods designed for traditional adversarial attacks.},
	language = {en},
	urldate = {2021-06-09},
	journal = {arXiv:1805.07894 [cs, stat]},
	author = {Song, Yang and Shu, Rui and Kushman, Nate and Ermon, Stefano},
	month = dec,
	year = {2018},
	note = {arXiv: 1805.07894},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{odena_conditional_2017,
	title = {Conditional {Image} {Synthesis} {With} {Auxiliary} {Classifier} {GANs}},
	url = {http://arxiv.org/abs/1610.09585},
	abstract = {In this paper we introduce new methods for the improved training of generative adversarial networks (GANs) for image synthesis. We construct a variant of GANs employing label conditioning that results in 128 × 128 resolution image samples exhibiting global coherence. We expand on previous work for image quality assessment to provide two new analyses for assessing the discriminability and diversity of samples from class-conditional image synthesis models. These analyses demonstrate that high resolution samples provide class information not present in low resolution samples. Across 1000 ImageNet classes, 128 × 128 samples are more than twice as discriminable as artiﬁcially resized 32 × 32 samples. In addition, 84.7\% of the classes have samples exhibiting diversity comparable to real ImageNet data.},
	language = {en},
	urldate = {2021-06-08},
	journal = {arXiv:1610.09585 [cs, stat]},
	author = {Odena, Augustus and Olah, Christopher and Shlens, Jonathon},
	month = jul,
	year = {2017},
	note = {arXiv: 1610.09585},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
}

@inproceedings{sharif_accessorize_2016,
	address = {Vienna Austria},
	title = {Accessorize to a {Crime}: {Real} and {Stealthy} {Attacks} on {State}-of-the-{Art} {Face} {Recognition}},
	isbn = {978-1-4503-4139-4},
	shorttitle = {Accessorize to a {Crime}},
	url = {https://dl.acm.org/doi/10.1145/2976749.2978392},
	doi = {10.1145/2976749.2978392},
	abstract = {Machine learning is enabling a myriad innovations, including new algorithms for cancer diagnosis and self-driving cars. The broad use of machine learning makes it important to understand the extent to which machine-learning algorithms are subject to attack, particularly when used in applications where physical security or safety is at risk.},
	language = {en},
	urldate = {2021-06-08},
	booktitle = {Proceedings of the 2016 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {ACM},
	author = {Sharif, Mahmood and Bhagavatula, Sruti and Bauer, Lujo and Reiter, Michael K.},
	month = oct,
	year = {2016},
	pages = {1528--1540},
}

@article{chen_ead_2018,
	title = {{EAD}: {Elastic}-{Net} {Attacks} to {Deep} {Neural} {Networks} via {Adversarial} {Examples}},
	shorttitle = {{EAD}},
	url = {http://arxiv.org/abs/1709.04114},
	abstract = {Recent studies have highlighted the vulnerability of deep neural networks (DNNs) to adversarial examples - a visually indistinguishable adversarial image can easily be crafted to cause a well-trained model to misclassify. Existing methods for crafting adversarial examples are based on L2 and L∞ distortion metrics. However, despite the fact that L1 distortion accounts for the total variation and encourages sparsity in the perturbation, little has been developed for crafting L1based adversarial examples.},
	language = {en},
	urldate = {2021-06-08},
	journal = {arXiv:1709.04114 [cs, stat]},
	author = {Chen, Pin-Yu and Sharma, Yash and Zhang, Huan and Yi, Jinfeng and Hsieh, Cho-Jui},
	month = feb,
	year = {2018},
	note = {arXiv: 1709.04114},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{rottmann_detection_2021,
	title = {Detection of {Iterative} {Adversarial} {Attacks} via {Counter} {Attack}},
	url = {http://arxiv.org/abs/2009.11397},
	abstract = {Deep neural networks (DNNs) have proven to be powerful tools for processing unstructured data. However for high-dimensional data, like images, they are inherently vulnerable to adversarial attacks. Small almost invisible perturbations added to the input can be used to fool DNNs. Various attacks, hardening methods and detection methods have been introduced in recent years. Notoriously, Carlini-Wagner (CW) type attacks computed by iterative minimization belong to those that are most difficult to detect. In this work we outline a mathematical proof that the CW attack can be used as a detector itself. That is, under certain assumptions and in the limit of attack iterations this detector provides asymptotically optimal separation of original and attacked images. In numerical experiments, we experimentally validate this statement and furthermore obtain AUROC values up to 99.73\% on CIFAR10 and ImageNet. This is in the upper part of the spectrum of current state-of-the-art detection rates for CW attacks.},
	language = {en},
	urldate = {2021-06-08},
	journal = {arXiv:2009.11397 [cs, stat]},
	author = {Rottmann, Matthias and Maag, Kira and Peyron, Mathis and Krejic, Natasa and Gottschalk, Hanno},
	month = mar,
	year = {2021},
	note = {arXiv: 2009.11397},
	keywords = {68T45, 62-07, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{papernot_distillation_2016,
	title = {Distillation as a {Defense} to {Adversarial} {Perturbations} against {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1511.04508},
	abstract = {Deep learning algorithms have been shown to perform extremely well on many classical machine learning problems. However, recent studies have shown that deep learning, like other machine learning techniques, is vulnerable to adversarial samples: inputs crafted to force a deep neural network (DNN) to provide adversary-selected outputs. Such attacks can seriously undermine the security of the system supported by the DNN, sometimes with devastating consequences. For example, autonomous vehicles can be crashed, illicit or illegal content can bypass content ﬁlters, or biometric authentication systems can be manipulated to allow improper access. In this work, we introduce a defensive mechanism called defensive distillation to reduce the effectiveness of adversarial samples on DNNs. We analytically investigate the generalizability and robustness properties granted by the use of defensive distillation when training DNNs. We also empirically study the effectiveness of our defense mechanisms on two DNNs placed in adversarial settings. The study shows that defensive distillation can reduce effectiveness of sample creation from 95\% to less than 0.5\% on a studied DNN. Such dramatic gains can be explained by the fact that distillation leads gradients used in adversarial sample creation to be reduced by a factor of 1030. We also ﬁnd that distillation increases the average minimum number of features that need to be modiﬁed to create adversarial samples by about 800\% on one of the DNNs we tested.},
	language = {en},
	urldate = {2021-06-08},
	journal = {arXiv:1511.04508 [cs, stat]},
	author = {Papernot, Nicolas and McDaniel, Patrick and Wu, Xi and Jha, Somesh and Swami, Ananthram},
	month = mar,
	year = {2016},
	note = {arXiv: 1511.04508},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{combey_probabilistic_2020,
	title = {Probabilistic {Jacobian}-based {Saliency} {Maps} {Attacks}},
	url = {http://arxiv.org/abs/2007.06032},
	abstract = {Neural network classiﬁers (NNCs) are known to be vulnerable to malicious adversarial perturbations of inputs including those modifying a small fraction of the input features named sparse or L0 attacks. Effective and fast L0 attacks, such as the widely used Jacobian-based Saliency Map Attack (JSMA) are practical to fool NNCs but also to improve their robustness. In this paper, we show that penalising saliency maps of JSMA by the output probabilities and the input features of the NNC allows to obtain more powerful attack algorithms that better take into account each input’s characteristics. This leads us to introduce improved versions of JSMA, named Weighted JSMA (WJSMA) and Taylor JSMA (TJSMA), and demonstrate through a variety of white-box and black-box experiments on three different datasets (MNIST, CIFAR-10 and GTSRB), that they are both signiﬁcantly faster and more efﬁcient than the original targeted and non-targeted versions of JSMA. Experiments also demonstrate, in some cases, very competitive results of our attacks in comparison with the Carlini-Wagner (CW) L0 attack, while remaining, like JSMA, signiﬁcantly faster (WJSMA and TJSMA are more than 50 times faster than CW L0 on CIFAR-10). Therefore, our new attacks provide good trade-offs between JSMA and CW for L0 real-time adversarial testing on datasets such as the ones previously cited.},
	language = {en},
	urldate = {2021-06-08},
	journal = {arXiv:2007.06032 [cs]},
	author = {Combey, Théo and Loison, António and Faucher, Maxime and Hajri, Hatem},
	month = dec,
	year = {2020},
	note = {arXiv: 2007.06032},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@article{wiyatno_maximal_2018,
	title = {Maximal {Jacobian}-based {Saliency} {Map} {Attack}},
	url = {http://arxiv.org/abs/1808.07945},
	abstract = {The Jacobian-based Saliency Map Attack is a family of adversarial attack methods for fooling classification models, such as deep neural networks for image classification tasks. By saturating a few pixels in a given image to their maximum or minimum values, JSMA can cause the model to misclassify the resulting adversarial image as a specified erroneous target class. We propose two variants of JSMA, one which removes the requirement to specify a target class, and another that additionally does not need to specify whether to only increase or decrease pixel intensities. Our experiments highlight the competitive speeds and qualities of these variants when applied to datasets of hand-written digits and natural scenes.},
	language = {en},
	urldate = {2021-06-08},
	journal = {arXiv:1808.07945 [cs, stat]},
	author = {Wiyatno, Rey and Xu, Anqi},
	month = aug,
	year = {2018},
	note = {arXiv: 1808.07945},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{zeng_adversarial_2019,
	title = {Adversarial {Attacks} {Beyond} the {Image} {Space}},
	url = {http://arxiv.org/abs/1711.07183},
	abstract = {Generating adversarial examples is an intriguing problem and an important way of understanding the working mechanism of deep neural networks. Most existing approaches generated perturbations in the image space, i.e., each pixel can be modiﬁed independently. However, in this paper we pay special attention to the subset of adversarial examples that correspond to meaningful changes in 3D physical properties (like rotation and translation, illumination condition, etc.). These adversaries arguably pose a more serious concern, as they demonstrate the possibility of causing neural network failure by easy perturbations of real-world 3D objects and scenes.},
	language = {en},
	urldate = {2021-06-08},
	journal = {arXiv:1711.07183 [cs]},
	author = {Zeng, Xiaohui and Liu, Chenxi and Wang, Yu-Siang and Qiu, Weichao and Xie, Lingxi and Tai, Yu-Wing and Tang, Chi Keung and Yuille, Alan L.},
	month = apr,
	year = {2019},
	note = {arXiv: 1711.07183},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{lu_no_2017,
	title = {{NO} {Need} to {Worry} about {Adversarial} {Examples} in {Object} {Detection} in {Autonomous} {Vehicles}},
	url = {http://arxiv.org/abs/1707.03501},
	abstract = {It has been shown that most machine learning algorithms are susceptible to adversarial perturbations. Slightly perturbing an image in a carefully chosen direction in the image space may cause a trained neural network model to misclassify it. Recently, it was shown that physical adversarial examples exist: printing perturbed images then taking pictures of them would still result in misclassiﬁcation. This raises security and safety concerns.},
	language = {en},
	urldate = {2021-06-08},
	journal = {arXiv:1707.03501 [cs]},
	author = {Lu, Jiajun and Sibai, Hussein and Fabry, Evan and Forsyth, David},
	month = jul,
	year = {2017},
	note = {arXiv: 1707.03501},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security},
}

@article{liu_detection_2018,
	title = {Detection based {Defense} against {Adversarial} {Examples} from the {Steganalysis} {Point} of {View}},
	url = {http://arxiv.org/abs/1806.09186},
	abstract = {Deep Neural Networks (DNNs) have recently led to signiﬁcant improvements in many ﬁelds. However, DNNs are vulnerable to adversarial examples which are samples with imperceptible perturbations while dramatically misleading the DNNs. Moreover, adversarial examples can be used to perform an attack on various kinds of DNN based systems, even if the adversary has no access to the underlying model. Many defense methods have been proposed, such as obfuscating gradients of the networks or detecting adversarial examples. However it is proved out that these defense methods are not effective or cannot resist secondary adversarial attacks. In this paper, we point out that steganalysis can be applied to adversarial examples detection, and propose a method to enhance steganalysis features by estimating the probability of modiﬁcations caused by adversarial attacks. Experimental results show that the proposed method can accurately detect adversarial examples. Moreover, secondary adversarial attacks cannot be directly performed to our method because our method is not based on a neural network but based on high-dimensional artiﬁcial features and FLD (Fisher Linear Discriminant) ensemble.},
	language = {en},
	urldate = {2021-06-08},
	journal = {arXiv:1806.09186 [cs, stat]},
	author = {Liu, Jiayang and Zhang, Weiming and Zhang, Yiwei and Hou, Dongdong and Liu, Yujia and Zha, Hongyue and Yu, Nenghai},
	month = dec,
	year = {2018},
	note = {arXiv: 1806.09186},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{meng_magnet_2017,
	title = {{MagNet}: a {Two}-{Pronged} {Defense} against {Adversarial} {Examples}},
	shorttitle = {{MagNet}},
	url = {http://arxiv.org/abs/1705.09064},
	abstract = {Deep learning has shown impressive performance on hard perceptual problems. However, researchers found deep learning systems to be vulnerable to small, specially crafted perturbations that are imperceptible to humans. Such perturbations cause deep learning systems to mis-classify adversarial examples, with potentially disastrous consequences where safety or security is crucial. Prior defenses against adversarial examples either targeted specific attacks or were shown to be ineffective. We propose MagNet, a framework for defending neural network classifiers against adversarial examples. MagNet neither modifies the protected classifier nor requires knowledge of the process for generating adversarial examples. MagNet includes one or more separate detector networks and a reformer network. The detector networks learn to differentiate between normal and adversarial examples by approximating the manifold of normal examples. Since they assume no specific process for generating adversarial examples, they generalize well. The reformer network moves adversarial examples towards the manifold of normal examples, which is effective for correctly classifying adversarial examples with small perturbation. We discuss the intrinsic difficulties in defending against whitebox attack and propose a mechanism to defend against graybox attack. Inspired by the use of randomness in cryptography, we use diversity to strengthen MagNet. We show empirically that MagNet is effective against the most advanced state-of-the-art attacks in blackbox and graybox scenarios without sacrificing false positive rate on normal examples.},
	language = {en},
	urldate = {2021-06-08},
	journal = {arXiv:1705.09064 [cs]},
	author = {Meng, Dongyu and Chen, Hao},
	month = sep,
	year = {2017},
	note = {arXiv: 1705.09064},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@article{raghunathan_certified_2020,
	title = {Certified {Defenses} against {Adversarial} {Examples}},
	url = {http://arxiv.org/abs/1801.09344},
	abstract = {While neural networks have achieved high accuracy on standard image classiﬁcation benchmarks, their accuracy drops to nearly zero in the presence of small adversarial perturbations to test inputs. Defenses based on regularization and adversarial training have been proposed, but often followed by new, stronger attacks that defeat these defenses. Can we somehow end this arms race? In this work, we study this problem for neural networks with one hidden layer. We ﬁrst propose a method based on a semideﬁnite relaxation that outputs a certiﬁcate that for a given network and test input, no attack can force the error to exceed a certain value. Second, as this certiﬁcate is differentiable, we jointly optimize it with the network parameters, providing an adaptive regularizer that encourages robustness against all attacks. On MNIST, our approach produces a network and a certiﬁcate that no attack that perturbs each pixel by at most = 0.1 can cause more than 35\% test error.},
	language = {en},
	urldate = {2021-06-08},
	journal = {arXiv:1801.09344 [cs]},
	author = {Raghunathan, Aditi and Steinhardt, Jacob and Liang, Percy},
	month = oct,
	year = {2020},
	note = {arXiv: 1801.09344},
	keywords = {Computer Science - Machine Learning},
}

@article{athalye_synthesizing_2018,
	title = {Synthesizing {Robust} {Adversarial} {Examples}},
	url = {http://arxiv.org/abs/1707.07397},
	abstract = {Standard methods for generating adversarial examples for neural networks do not consistently fool neural network classiﬁers in the physical world due to a combination of viewpoint shifts, camera noise, and other natural transformations, limiting their relevance to real-world systems. We demonstrate the existence of robust 3D adversarial objects, and we present the ﬁrst algorithm for synthesizing examples that are adversarial over a chosen distribution of transformations. We synthesize two-dimensional adversarial images that are robust to noise, distortion, and afﬁne transformation. We apply our algorithm to complex three-dimensional objects, using 3D-printing to manufacture the ﬁrst physical adversarial objects. Our results demonstrate the existence of 3D adversarial objects in the physical world.},
	language = {en},
	urldate = {2021-06-08},
	journal = {arXiv:1707.07397 [cs]},
	author = {Athalye, Anish and Engstrom, Logan and Ilyas, Andrew and Kwok, Kevin},
	month = jun,
	year = {2018},
	note = {arXiv: 1707.07397},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{luo_random_2020,
	title = {{RANDOM} {MASK}: {Towards} {Robust} {Convolutional} {Neural} {Networks}},
	shorttitle = {{RANDOM} {MASK}},
	url = {http://arxiv.org/abs/2007.14249},
	abstract = {Robustness of neural networks has recently been highlighted by the adversarial examples, i.e., inputs added with well-designed perturbations which are imperceptible to humans but can cause the network to give incorrect outputs. In this paper, we design a new CNN architecture that by itself has good robustness. We introduce a simple but powerful technique, Random Mask, to modify existing CNN structures. We show that CNNs with Random Mask achieve state-of-the-art performance against black-box adversarial attacks without applying any adversarial training. We next investigate the adversarial examples which “fool” a CNN with Random Mask. Surprisingly, we ﬁnd that these adversarial examples often “fool” humans as well. This raises fundamental questions on how to deﬁne adversarial examples and robustness properly.},
	language = {en},
	urldate = {2021-06-08},
	journal = {arXiv:2007.14249 [cs]},
	author = {Luo, Tiange and Cai, Tianle and Zhang, Mengxiao and Chen, Siyu and Wang, Liwei},
	month = jul,
	year = {2020},
	note = {arXiv: 2007.14249},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{xu_feature_2018,
	title = {Feature {Squeezing}: {Detecting} {Adversarial} {Examples} in {Deep} {Neural} {Networks}},
	shorttitle = {Feature {Squeezing}},
	url = {http://arxiv.org/abs/1704.01155},
	doi = {10.14722/ndss.2018.23198},
	abstract = {Although deep neural networks (DNNs) have achieved great success in many tasks, they can often be fooled by adversarial examples that are generated by adding small but purposeful distortions to natural examples. Previous studies to defend against adversarial examples mostly focused on reﬁning the DNN models, but have either shown limited success or required expensive computation. We propose a new strategy, feature squeezing, that can be used to harden DNN models by detecting adversarial examples. Feature squeezing reduces the search space available to an adversary by coalescing samples that correspond to many diﬀerent feature vectors in the original space into a single sample. By comparing a DNN model’s prediction on the original input with that on squeezed inputs, feature squeezing detects adversarial examples with high accuracy and few false positives. This paper explores two feature squeezing methods: reducing the color bit depth of each pixel and spatial smoothing. These simple strategies are inexpensive and complementary to other defenses, and can be combined in a joint detection framework to achieve high detection rates against state-of-the-art attacks.},
	language = {en},
	urldate = {2021-06-08},
	journal = {Proceedings 2018 Network and Distributed System Security Symposium},
	author = {Xu, Weilin and Evans, David and Qi, Yanjun},
	year = {2018},
	note = {arXiv: 1704.01155},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@article{xie_feature_2019,
	title = {Feature {Denoising} for {Improving} {Adversarial} {Robustness}},
	url = {http://arxiv.org/abs/1812.03411},
	abstract = {Adversarial attacks to image classiﬁcation systems present challenges to convolutional networks and opportunities for understanding them. This study suggests that adversarial perturbations on images lead to noise in the features constructed by these networks. Motivated by this observation, we develop new network architectures that increase adversarial robustness by performing feature denoising. Speciﬁcally, our networks contain blocks that denoise the features using non-local means or other ﬁlters; the entire networks are trained end-to-end. When combined with adversarial training, our feature denoising networks substantially improve the state-of-the-art in adversarial robustness in both white-box and black-box attack settings. On ImageNet, under 10-iteration PGD white-box attacks where prior art has 27.9\% accuracy, our method achieves 55.7\%; even under extreme 2000-iteration PGD white-box attacks, our method secures 42.6\% accuracy. Our method was ranked ﬁrst in Competition on Adversarial Attacks and Defenses (CAAD) 2018 — it achieved 50.6\% classiﬁcation accuracy on a secret, ImageNet-like test dataset against 48 unknown attackers, surpassing the runner-up approach by ∼10\%. Code is available at https://github.com/facebookresearch/ ImageNet-Adversarial-Training.},
	language = {en},
	urldate = {2021-06-08},
	journal = {arXiv:1812.03411 [cs]},
	author = {Xie, Cihang and Wu, Yuxin and van der Maaten, Laurens and Yuille, Alan and He, Kaiming},
	month = mar,
	year = {2019},
	note = {arXiv: 1812.03411},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{carlini_towards_2017,
	title = {Towards {Evaluating} the {Robustness} of {Neural} {Networks}},
	url = {http://arxiv.org/abs/1608.04644},
	abstract = {Neural networks provide state-of-the-art results for most machine learning tasks. Unfortunately, neural networks are vulnerable to adversarial examples: given an input x and any target classiﬁcation t, it is possible to ﬁnd a new input x that is similar to x but classiﬁed as t. This makes it difﬁcult to apply neural networks in security-critical areas. Defensive distillation is a recently proposed approach that can take an arbitrary neural network, and increase its robustness, reducing the success rate of current attacks’ ability to ﬁnd adversarial examples from 95\% to 0.5\%.},
	language = {en},
	urldate = {2021-06-08},
	journal = {arXiv:1608.04644 [cs]},
	author = {Carlini, Nicholas and Wagner, David},
	month = mar,
	year = {2017},
	note = {arXiv: 1608.04644},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security},
}

@article{papernot_practical_2017,
	title = {Practical {Black}-{Box} {Attacks} against {Machine} {Learning}},
	url = {http://arxiv.org/abs/1602.02697},
	abstract = {Machine learning (ML) models, e.g., deep neural networks (DNNs), are vulnerable to adversarial examples: malicious inputs modiﬁed to yield erroneous model outputs, while appearing unmodiﬁed to human observers. Potential attacks include having malicious content like malware identiﬁed as legitimate or controlling vehicle behavior. Yet, all existing adversarial example attacks require knowledge of either the model internals or its training data. We introduce the ﬁrst practical demonstration of an attacker controlling a remotely hosted DNN with no such knowledge. Indeed, the only capability of our black-box adversary is to observe labels given by the DNN to chosen inputs. Our attack strategy consists in training a local model to substitute for the target DNN, using inputs synthetically generated by an adversary and labeled by the target DNN. We use the local substitute to craft adversarial examples, and ﬁnd that they are misclassiﬁed by the targeted DNN. To perform a real-world and properly-blinded evaluation, we attack a DNN hosted by MetaMind, an online deep learning API. We ﬁnd that their DNN misclassiﬁes 84.24\% of the adversarial examples crafted with our substitute. We demonstrate the general applicability of our strategy to many ML techniques by conducting the same attack against models hosted by Amazon and Google, using logistic regression substitutes. They yield adversarial examples misclassiﬁed by Amazon and Google at rates of 96.19\% and 88.94\%. We also ﬁnd that this black-box attack strategy is capable of evading defense strategies previously found to make adversarial example crafting harder.},
	language = {en},
	urldate = {2021-06-08},
	journal = {arXiv:1602.02697 [cs]},
	author = {Papernot, Nicolas and McDaniel, Patrick and Goodfellow, Ian and Jha, Somesh and Celik, Z. Berkay and Swami, Ananthram},
	month = mar,
	year = {2017},
	note = {arXiv: 1602.02697},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@incollection{ferrari_gray-box_2018,
	address = {Cham},
	title = {Gray-{Box} {Adversarial} {Training}},
	volume = {11219},
	isbn = {978-3-030-01266-3 978-3-030-01267-0},
	url = {http://link.springer.com/10.1007/978-3-030-01267-0_13},
	abstract = {Adversarial samples are perturbed inputs crafted to mislead the machine learning systems. A training mechanism, called adversarial training, which presents adversarial samples along with clean samples has been introduced to learn robust models. In order to scale adversarial training for large datasets, these perturbations can only be crafted using fast and simple methods (e.g., gradient ascent). However, it is shown that adversarial training converges to a degenerate minimum, where the model appears to be robust by generating weaker adversaries. As a result, the models are vulnerable to simple black-box attacks.},
	language = {en},
	urldate = {2021-06-08},
	booktitle = {Computer {Vision} – {ECCV} 2018},
	publisher = {Springer International Publishing},
	author = {Vivek, B. S. and Mopuri, Konda Reddy and Babu, R. Venkatesh},
	editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
	year = {2018},
	doi = {10.1007/978-3-030-01267-0_13},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {213--228},
}

@article{shaham_understanding_2018,
	title = {Understanding {Adversarial} {Training}: {Increasing} {Local} {Stability} of {Neural} {Nets} through {Robust} {Optimization}},
	volume = {307},
	issn = {09252312},
	shorttitle = {Understanding {Adversarial} {Training}},
	url = {http://arxiv.org/abs/1511.05432},
	doi = {10.1016/j.neucom.2018.04.027},
	abstract = {We propose a general framework for increasing local stability of Artiﬁcial Neural Nets (ANNs) using Robust Optimization (RO). We achieve this through an alternating minimization-maximization procedure, in which the loss of the network is minimized over perturbed examples that are generated at each parameter update. We show that adversarial training of ANNs is in fact robustiﬁcation of the network optimization, and that our proposed framework generalizes previous approaches for increasing local stability of ANNs. Experimental results reveal that our approach increases the robustness of the network to existing adversarial examples, while making it harder to generate new ones. Furthermore, our algorithm improves the accuracy of the network also on the original test data.},
	language = {en},
	urldate = {2021-06-08},
	journal = {Neurocomputing},
	author = {Shaham, Uri and Yamada, Yutaro and Negahban, Sahand},
	month = sep,
	year = {2018},
	note = {arXiv: 1511.05432},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	pages = {195--204},
}

@article{bastani_measuring_2017,
	title = {Measuring {Neural} {Net} {Robustness} with {Constraints}},
	url = {http://arxiv.org/abs/1605.07262},
	abstract = {Despite having high accuracy, neural nets have been shown to be susceptible to adversarial examples, where a small perturbation to an input can cause it to become mislabeled. We propose metrics for measuring the robustness of a neural net and devise a novel algorithm for approximating these metrics based on an encoding of robustness as a linear program. We show how our metrics can be used to evaluate the robustness of deep neural nets with experiments on the MNIST and CIFAR-10 datasets. Our algorithm generates more informative estimates of robustness metrics compared to estimates based on existing algorithms. Furthermore, we show how existing approaches to improving robustness “overﬁt” to adversarial examples generated using a speciﬁc algorithm. Finally, we show that our techniques can be used to additionally improve neural net robustness both according to the metrics that we propose, but also according to previously proposed metrics.},
	language = {en},
	urldate = {2021-06-08},
	journal = {arXiv:1605.07262 [cs]},
	author = {Bastani, Osbert and Ioannou, Yani and Lampropoulos, Leonidas and Vytiniotis, Dimitrios and Nori, Aditya and Criminisi, Antonio},
	month = jun,
	year = {2017},
	note = {arXiv: 1605.07262},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@article{gu_towards_2015,
	title = {Towards {Deep} {Neural} {Network} {Architectures} {Robust} to {Adversarial} {Examples}},
	url = {http://arxiv.org/abs/1412.5068},
	abstract = {Recent work has shown deep neural networks (DNNs) to be highly susceptible to well-designed, small perturbations at the input layer, or so-called adversarial examples. Taking images as an example, such distortions are often imperceptible, but can result in 100\% mis-classiﬁcation for a state of the art DNN. We study the structure of adversarial examples and explore network topology, pre-processing and training strategies to improve the robustness of DNNs. We perform various experiments to assess the removability of adversarial examples by corrupting with additional noise and pre-processing with denoising autoencoders (DAEs). We ﬁnd that DAEs can remove substantial amounts of the adversarial noise. However, when stacking the DAE with the original DNN, the resulting network can again be attacked by new adversarial examples with even smaller distortion. As a solution, we propose Deep Contractive Network, a model with a new end-to-end training procedure that includes a smoothness penalty inspired by the contractive autoencoder (CAE). This increases the network robustness to adversarial examples, without a signiﬁcant performance penalty.},
	language = {en},
	urldate = {2021-06-08},
	journal = {arXiv:1412.5068 [cs]},
	author = {Gu, Shixiang and Rigazio, Luca},
	month = apr,
	year = {2015},
	note = {arXiv: 1412.5068},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@article{huang_learning_2016,
	title = {Learning with a {Strong} {Adversary}},
	url = {http://arxiv.org/abs/1511.03034},
	abstract = {The robustness of neural networks to intended perturbations has recently attracted signiﬁcant attention. In this paper, we propose a new method, learning with a strong adversary, that learns robust classiﬁers from supervised data by generating adversarial examples as an intermediate step. A new and simple way of ﬁnding adversarial examples is presented that is empirically stronger than existing approaches in terms of the accuracy reduction as a function of perturbation magnitude. Experimental results demonstrate that resulting learning method greatly improves the robustness of the classiﬁcation models produced.},
	language = {en},
	urldate = {2021-06-08},
	journal = {arXiv:1511.03034 [cs]},
	author = {Huang, Ruitong and Xu, Bing and Schuurmans, Dale and Szepesvari, Csaba},
	month = jan,
	year = {2016},
	note = {arXiv: 1511.03034},
	keywords = {Computer Science - Machine Learning},
}

@article{naseer_cross-domain_2019,
	title = {Cross-{Domain} {Transferability} of {Adversarial} {Perturbations}},
	url = {http://arxiv.org/abs/1905.11736},
	abstract = {Adversarial examples reveal the blind spots of deep neural networks (DNNs) and represent a major concern for security-critical applications. The transferability of adversarial examples makes real-world attacks possible in black-box settings, where the attacker is forbidden to access the internal parameters of the model. The underlying assumption in most adversary generation methods, whether learning an instance-specific or an instance-agnostic perturbation, is the direct or indirect reliance on the original domain-specific data distribution. In this work, for the first time, we demonstrate the existence of domain-invariant adversaries, thereby showing common adversarial space among different datasets and models. To this end, we propose a framework capable of launching highly transferable attacks that crafts adversarial patterns to mislead networks trained on wholly different domains. For instance, an adversarial function learned on Paintings, Cartoons or Medical images can successfully perturb ImageNet samples to fool the classifier, with success rates as high as \${\textbackslash}sim\$99{\textbackslash}\% (\${\textbackslash}ell\_\{{\textbackslash}infty\} {\textbackslash}le 10\$). The core of our proposed adversarial function is a generative network that is trained using a relativistic supervisory signal that enables domain-invariant perturbations. Our approach sets the new state-of-the-art for fooling rates, both under the white-box and black-box scenarios. Furthermore, despite being an instance-agnostic perturbation function, our attack outperforms the conventionally much stronger instance-specific attack methods.},
	language = {en},
	urldate = {2021-06-08},
	journal = {arXiv:1905.11736 [cs]},
	author = {Naseer, Muzammal and Khan, Salman H. and Khan, Harris and Khan, Fahad Shahbaz and Porikli, Fatih},
	month = oct,
	year = {2019},
	note = {arXiv: 1905.11736},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{papernot_transferability_2016,
	title = {Transferability in {Machine} {Learning}: from {Phenomena} to {Black}-{Box} {Attacks} using {Adversarial} {Samples}},
	shorttitle = {Transferability in {Machine} {Learning}},
	url = {http://arxiv.org/abs/1605.07277},
	abstract = {Many machine learning models are vulnerable to adversarial examples: inputs that are specially crafted to cause a machine learning model to produce an incorrect output. Adversarial examples that aﬀect one model often aﬀect another model, even if the two models have diﬀerent architectures or were trained on diﬀerent training sets, so long as both models were trained to perform the same task. An attacker may therefore train their own substitute model, craft adversarial examples against the substitute, and transfer them to a victim model, with very little information about the victim. Recent work has further developed a technique that uses the victim model as an oracle to label a synthetic training set for the substitute, so the attacker need not even collect a training set to mount the attack. We extend these recent techniques using reservoir sampling to greatly enhance the eﬃciency of the training procedure for the substitute model. We introduce new transferability attacks between previously unexplored (substitute, victim) pairs of machine learning model classes, most notably SVMs and decision trees. We demonstrate our attacks on two commercial machine learning classiﬁcation systems from Amazon (96.19\% misclassiﬁcation rate) and Google (88.94\%) using only 800 queries of the victim model, thereby showing that existing machine learning approaches are in general vulnerable to systematic black-box attacks regardless of their structure.},
	language = {en},
	urldate = {2021-06-08},
	journal = {arXiv:1605.07277 [cs]},
	author = {Papernot, Nicolas and McDaniel, Patrick and Goodfellow, Ian},
	month = may,
	year = {2016},
	note = {arXiv: 1605.07277},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@article{liu_delving_2017,
	title = {Delving into {Transferable} {Adversarial} {Examples} and {Black}-box {Attacks}},
	url = {http://arxiv.org/abs/1611.02770},
	abstract = {An intriguing property of deep neural networks is the existence of adversarial examples, which can transfer among different architectures. These transferable adversarial examples may severely hinder deep neural network-based applications. Previous works mostly study the transferability using small scale datasets. In this work, we are the ﬁrst to conduct an extensive study of the transferability over large models and a large scale dataset, and we are also the ﬁrst to study the transferability of targeted adversarial examples with their target labels. We study both non-targeted and targeted adversarial examples, and show that while transferable non-targeted adversarial examples are easy to ﬁnd, targeted adversarial examples generated using existing approaches almost never transfer with their target labels. Therefore, we propose novel ensemble-based approaches to generating transferable adversarial examples. Using such approaches, we observe a large proportion of targeted adversarial examples that are able to transfer with their target labels for the ﬁrst time. We also present some geometric studies to help understanding the transferable adversarial examples. Finally, we show that the adversarial examples generated using ensemble-based approaches can successfully attack Clarifai.com, which is a black-box image classiﬁcation system.},
	language = {en},
	urldate = {2021-06-08},
	journal = {arXiv:1611.02770 [cs]},
	author = {Liu, Yanpei and Chen, Xinyun and Liu, Chang and Song, Dawn},
	month = feb,
	year = {2017},
	note = {arXiv: 1611.02770},
	keywords = {Computer Science - Machine Learning},
}

@article{szegedy_intriguing_2014,
	title = {Intriguing properties of neural networks},
	url = {http://arxiv.org/abs/1312.6199},
	abstract = {Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties.},
	language = {en},
	urldate = {2021-06-08},
	journal = {arXiv:1312.6199 [cs]},
	author = {Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
	month = feb,
	year = {2014},
	note = {arXiv: 1312.6199},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@article{lecun_backpropagation_1989,
	title = {Backpropagation {Applied} to {Handwritten} {Zip} {Code} {Recognition}},
	volume = {1},
	issn = {0899-7667, 1530-888X},
	url = {https://direct.mit.edu/neco/article/1/4/541-551/5515},
	doi = {10.1162/neco.1989.1.4.541},
	abstract = {The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification.},
	language = {en},
	number = {4},
	urldate = {2021-06-08},
	journal = {Neural Computation},
	author = {LeCun, Y. and Boser, B. and Denker, J. S. and Henderson, D. and Howard, R. E. and Hubbard, W. and Jackel, L. D.},
	month = dec,
	year = {1989},
	pages = {541--551},
}

@article{montemerlo_fastslam_nodate,
	title = {{FastSLAM}: {A} {Factored} {Solution} to the {Simultaneous} {Localization} and {Mapping} {Problem}},
	abstract = {The ability to simultaneously localize a robot and accurately map its surroundings is considered by many to be a key prerequisite of truly autonomous robots. However, few approaches to this problem scale up to handle the very large number of landmarks present in real environments. Kalman ﬁlter-based algorithms, for example, require time quadratic in the number of landmarks to incorporate each sensor observation. This paper presents FastSLAM, an algorithm that recursively estimates the full posterior distribution over robot pose and landmark locations, yet scales logarithmically with the number of landmarks in the map. This algorithm is based on an exact factorization of the posterior into a product of conditional landmark distributions and a distribution over robot paths. The algorithm has been run successfully on as many as 50,000 landmarks, environments far beyond the reach of previous approaches. Experimental results demonstrate the advantages and limitations of the FastSLAM algorithm on both simulated and realworld data.},
	language = {en},
	author = {Montemerlo, Michael and Thrun, Sebastian and Koller, Daphne and Wegbreit, Ben},
	pages = {6},
}

@article{smith_estimating_2013,
	title = {Estimating {Uncertain} {Spatial} {Relationships} in {Robotics}},
	url = {http://arxiv.org/abs/1304.3111},
	abstract = {In this paper, we describe a representation for spatial information, called the stochastic map, and associated procedures for building it, reading information from it, and revising it incrementally as new information is obtained. The map contains the estimates of relationships among objects in the map, and their uncertainties, given all the available information. The procedures provide a general solution to the problem of estimating uncertain relative spatial relationships. The estimates are probabilistic in nature, an advance over the previous, very conservative, worst-case approaches to the problem. Finally, the procedures are developed in the context of state-estimation and filtering theory, which provides a solid basis for numerous extensions.},
	urldate = {2021-06-08},
	journal = {arXiv:1304.3111 [cs]},
	author = {Smith, Randall and Self, Matthew and Cheeseman, Peter},
	month = mar,
	year = {2013},
	note = {arXiv: 1304.3111},
	keywords = {Computer Science - Artificial Intelligence},
}

@article{cordts_understanding_nodate,
	title = {Understanding {Cityscapes}: {Efficient} {Urban} {Semantic} {Scene} {Understanding}},
	language = {en},
	author = {Cordts, Marius},
	pages = {187},
}

@article{sun_scalability_2020,
	title = {Scalability in {Perception} for {Autonomous} {Driving}: {Waymo} {Open} {Dataset}},
	shorttitle = {Scalability in {Perception} for {Autonomous} {Driving}},
	url = {http://arxiv.org/abs/1912.04838},
	abstract = {The research community has increasing interest in autonomous driving research, despite the resource intensity of obtaining representative real world data. Existing selfdriving datasets are limited in the scale and variation of the environments they capture, even though generalization within and between operating regions is crucial to the overall viability of the technology. In an effort to help align the research community’s contributions with real-world selfdriving problems, we introduce a new large-scale, high quality, diverse dataset. Our new dataset consists of 1150 scenes that each span 20 seconds, consisting of well synchronized and calibrated high quality LiDAR and camera data captured across a range of urban and suburban geographies. It is 15x more diverse than the largest camera+LiDAR dataset available based on our proposed geographical coverage metric. We exhaustively annotated this data with 2D (camera image) and 3D (LiDAR) bounding boxes, with consistent identiﬁers across frames. Finally, we provide strong baselines for 2D as well as 3D detection and tracking tasks. We further study the effects of dataset size and generalization across geographies on 3D detection methods. Find data, code and more up-to-date information at http://www.waymo.com/open.},
	language = {en},
	urldate = {2021-06-04},
	journal = {arXiv:1912.04838 [cs, stat]},
	author = {Sun, Pei and Kretzschmar, Henrik and Dotiwalla, Xerxes and Chouard, Aurelien and Patnaik, Vijaysai and Tsui, Paul and Guo, James and Zhou, Yin and Chai, Yuning and Caine, Benjamin and Vasudevan, Vijay and Han, Wei and Ngiam, Jiquan and Zhao, Hang and Timofeev, Aleksei and Ettinger, Scott and Krivokon, Maxim and Gao, Amy and Joshi, Aditya and Zhao, Sheng and Cheng, Shuyang and Zhang, Yu and Shlens, Jonathon and Chen, Zhifeng and Anguelov, Dragomir},
	month = may,
	year = {2020},
	note = {arXiv: 1912.04838},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{tu_exploring_2021,
	title = {Exploring {Adversarial} {Robustness} of {Multi}-{Sensor} {Perception} {Systems} in {Self} {Driving}},
	url = {http://arxiv.org/abs/2101.06784},
	abstract = {Modern self-driving perception systems have been shown to improve upon processing complementary inputs such as LiDAR with images. In isolation, 2D images have been found to be extremely vulnerable to adversarial attacks. Yet, there have been limited studies on the adversarial robustness of multi-modal models that fuse LiDAR features with image features. Furthermore, existing works do not consider physically realizable perturbations that are consistent across the input modalities. In this paper, we showcase practical susceptibilities of multi-sensor detection by placing an adversarial object on top of a host vehicle. We focus on physically realizable and input-agnostic attacks as they are feasible to execute in practice, and show that a single universal adversary can hide different host vehicles from state-of-the-art multi-modal detectors. Our experiments demonstrate that successful attacks are primarily caused by easily corrupted image features. Furthermore, we ﬁnd that in modern sensor fusion methods which project image features into 3D, adversarial attacks can exploit the projection process to generate false positives across distant regions in 3D. Towards more robust multi-modal perception systems, we show that adversarial training with feature denoising can boost robustness to such attacks signiﬁcantly. However, we ﬁnd that standard adversarial defenses still struggle to prevent false positives which are also caused by inaccurate associations between 3D LiDAR points and 2D pixels.},
	language = {en},
	urldate = {2021-06-04},
	journal = {arXiv:2101.06784 [cs]},
	author = {Tu, James and Li, Huichen and Yan, Xinchen and Ren, Mengye and Chen, Yun and Liang, Ming and Bitar, Eilyan and Yumer, Ersin and Urtasun, Raquel},
	month = jan,
	year = {2021},
	note = {arXiv: 2101.06784},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{hau_object_2021,
	title = {Object {Removal} {Attacks} on {LiDAR}-based {3D} {Object} {Detectors}},
	url = {http://arxiv.org/abs/2102.03722},
	doi = {10.14722/autosec.2021.23},
	abstract = {LiDARs play a critical role in Autonomous Vehicles’ (AVs) perception and their safe operations. Recent works have demonstrated that it is possible to spoof LiDAR return signals to elicit fake objects. In this work we demonstrate how the same physical capabilities can be used to mount a new, even more dangerous class of attacks, namely Object Removal Attacks (ORAs). ORAs aim to force 3D object detectors to fail. We leverage the default setting of LiDARs that record a single return signal per direction to perturb point clouds in the region of interest (RoI) of 3D objects. By injecting illegitimate points behind the target object, we effectively shift points away from the target objects’ RoIs. Our initial results using a simple random point selection strategy show that the attack is effective in degrading the performance of commonly used 3D object detection models.},
	language = {en},
	urldate = {2021-06-04},
	journal = {arXiv:2102.03722 [cs]},
	author = {Hau, Zhongyuan and Co, Kenneth T. and Demetriou, Soteris and Lupu, Emil C.},
	month = feb,
	year = {2021},
	note = {arXiv: 2102.03722},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@article{abdelfattah_towards_2021,
	title = {Towards {Universal} {Physical} {Attacks} {On} {Cascaded} {Camera}-{Lidar} {3D} {Object} {Detection} {Models}},
	url = {http://arxiv.org/abs/2101.10747},
	abstract = {We propose a universal and physically realizable adversarial attack on a cascaded multi-modal deep learning network (DNN), in the context of self-driving cars. DNNs have achieved high performance in 3D object detection, but they are known to be vulnerable to adversarial attacks. These attacks have been heavily investigated in the RGB image domain and more recently in the point cloud domain, but rarely in both domains simultaneously - a gap to be ﬁlled in this paper. We use a single 3D mesh and differentiable rendering to explore how perturbing the mesh’s geometry and texture can reduce the robustness of DNNs to adversarial attacks. We attack a prominent cascaded multi-modal DNN, the FrustumPointnet model. Using the popular KITTI benchmark, we showed that the proposed universal multi-modal attack was successful in reducing the model’s ability to detect a car by nearly 73\%. This work can aid in the understanding of what the cascaded RGB-point cloud DNN learns and its vulnerability to adversarial attacks.},
	language = {en},
	urldate = {2021-06-04},
	journal = {arXiv:2101.10747 [cs, eess]},
	author = {Abdelfattah, Mazen and Yuan, Kaiwen and Wang, Z. Jane and Ward, Rabab},
	month = jan,
	year = {2021},
	note = {arXiv: 2101.10747},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
}

@article{sun_towards_nodate,
	title = {Towards {Robust} {LiDAR}-based {Perception} in {Autonomous} {Driving}: {General} {Black}-box {Adversarial} {Sensor} {Attack} and {Countermeasures}},
	abstract = {Perception plays a pivotal role in autonomous driving systems, which utilizes onboard sensors like cameras and LiDARs (Light Detection and Ranging) to assess surroundings. Recent studies have demonstrated that LiDAR-based perception is vulnerable to spooﬁng attacks, in which adversaries spoof a fake vehicle in front of a victim self-driving car by strategically transmitting laser signals to the victim’s LiDAR sensor. However, existing attacks suffer from effectiveness and generality limitations. In this work, we perform the ﬁrst study to explore the general vulnerability of current LiDAR-based perception architectures and discover that the ignored occlusion patterns in LiDAR point clouds make self-driving cars vulnerable to spooﬁng attacks. We construct the ﬁrst black-box spooﬁng attack based on our identiﬁed vulnerability, which universally achieves around 80\% mean success rates on all target models. We perform the ﬁrst defense study, proposing CARLO to mitigate LiDAR spooﬁng attacks. CARLO detects spoofed data by treating ignored occlusion patterns as invariant physical features, which reduces the mean attack success rate to 5.5\%. Meanwhile, we take the ﬁrst step towards exploring a general architecture for robust LiDAR-based perception, and propose SVF that embeds the neglected physical features into end-to-end learning. SVF further reduces the mean attack success rate to around 2.3\%.},
	language = {en},
	author = {Sun, Jiachen and Cao, Yulong and Chen, Qi Alfred and Mao, Z Morley},
	pages = {19},
}

@article{cao_adversarial_2019,
	title = {Adversarial {Objects} {Against} {LiDAR}-{Based} {Autonomous} {Driving} {Systems}},
	url = {http://arxiv.org/abs/1907.05418},
	abstract = {Deep neural networks (DNNs) are found to be vulnerable against adversarial examples, which are carefully crafted inputs with a small magnitude of perturbation aiming to induce arbitrarily incorrect predictions. Recent studies show that adversarial examples can pose a threat to real-world security-critical applications: a “physically adversarial Stop Sign” can be synthesized such that the autonomous driving cars will misrecognize it as others (e.g., a speed limit sign). However, these image-based adversarial examples cannot easily alter 3D scans such as widely equipped LiDAR or radar on autonomous vehicles. In this paper, we reveal the potential vulnerabilities of LiDAR-based autonomous driving detection systems, by proposing an optimization based approach LiDAR-Adv to generate real-world adversarial objects that can evade the LiDAR-based detection systems under various conditions. We ﬁrst explore the vulnerabilities of LiDAR using an evolutionbased blackbox attack algorithm, and then propose a strong attack strategy, using our gradient-based approach LiDAR-Adv. We test the generated adversarial objects on the Baidu Apollo autonomous driving platform and show that such physical systems are indeed vulnerable to the proposed attacks. We 3D-print our adversarial objects and perform physical experiments with LiDAR equipped cars to illustrate the effectiveness of LiDARAdv. Please ﬁnd more visualizations and physical experimental results on this website: https://sites.google.com/view/lidar-adv.},
	language = {en},
	urldate = {2021-06-04},
	journal = {arXiv:1907.05418 [cs, stat]},
	author = {Cao, Yulong and Xiao, Chaowei and Yang, Dawei and Fang, Jing and Yang, Ruigang and Liu, Mingyan and Li, Bo},
	month = jul,
	year = {2019},
	note = {arXiv: 1907.05418},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{cao_adversarial_2019-1,
	address = {London United Kingdom},
	title = {Adversarial {Sensor} {Attack} on {LiDAR}-based {Perception} in {Autonomous} {Driving}},
	isbn = {978-1-4503-6747-9},
	url = {https://dl.acm.org/doi/10.1145/3319535.3339815},
	doi = {10.1145/3319535.3339815},
	abstract = {In Autonomous Vehicles (AVs), one fundamental pillar is perception, which leverages sensors like cameras and LiDARs (Light Detection and Ranging) to understand the driving environment. Due to its direct impact on road safety, multiple prior efforts have been made to study its the security of perception systems. In contrast to prior work that concentrates on camera-based perception, in this work we perform the first security study of LiDAR-based perception in AV settings, which is highly important but unexplored. We consider LiDAR spoofing attacks as the threat model and set the attack goal as spoofing obstacles close to the front of a victim AV. We find that blindly applying LiDAR spoofing is insufficient to achieve this goal due to the machine learning-based object detection process. Thus, we then explore the possibility of strategically controlling the spoofed attack to fool the machine learning model. We formulate this task as an optimization problem and design modeling methods for the input perturbation function and the objective function. We also identify the inherent limitations of directly solving the problem using optimization and design an algorithm that combines optimization and global sampling, which improves the attack success rates to around 75\%. As a case study to understand the attack impact at the AV driving decision level, we construct and evaluate two attack scenarios that may damage road safety and mobility. We also discuss defense directions at the AV system, sensor, and machine learning model levels.},
	language = {en},
	urldate = {2021-06-04},
	booktitle = {Proceedings of the 2019 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {ACM},
	author = {Cao, Yulong and Xiao, Chaowei and Cyr, Benjamin and Zhou, Yimeng and Park, Won and Rampazzi, Sara and Chen, Qi Alfred and Fu, Kevin and Mao, Z. Morley},
	month = nov,
	year = {2019},
	pages = {2267--2281},
}

@inproceedings{gatt_micro-doppler_2000,
	address = {Orlando, FL},
	title = {Micro-{Doppler} lidar signals and noise mechanisms: theory and experiment},
	shorttitle = {Micro-{Doppler} lidar signals and noise mechanisms},
	url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?doi=10.1117/12.397813},
	doi = {10.1117/12.397813},
	language = {en},
	urldate = {2021-06-04},
	author = {Gatt, Philip and Henderson, Sammy W. and Thomson, J. Alex L. and Bruns, Dale L.},
	editor = {Kamerman, Gary W. and Singh, Upendra N. and Werner, Christian and Molebny, Vasyl V.},
	month = sep,
	year = {2000},
	pages = {422},
}

@inproceedings{cordts_cityscapes_2016,
	address = {Las Vegas, NV, USA},
	title = {The {Cityscapes} {Dataset} for {Semantic} {Urban} {Scene} {Understanding}},
	isbn = {978-1-4673-8851-1},
	url = {http://ieeexplore.ieee.org/document/7780719/},
	doi = {10.1109/CVPR.2016.350},
	abstract = {Visual understanding of complex urban street scenes is an enabling factor for a wide range of applications. Object detection has beneﬁted enormously from large-scale datasets, especially in the context of deep learning. For semantic urban scene understanding, however, no current dataset adequately captures the complexity of real-world urban scenes. To address this, we introduce Cityscapes, a benchmark suite and large-scale dataset to train and test approaches for pixel-level and instance-level semantic labeling. Cityscapes is comprised of a large, diverse set of stereo video sequences recorded in streets from 50 different cities. 5000 of these images have high quality pixel-level annotations; 20 000 additional images have coarse annotations to enable methods that leverage large volumes of weakly-labeled data. Crucially, our effort exceeds previous attempts in terms of dataset size, annotation richness, scene variability, and complexity. Our accompanying empirical study provides an in-depth analysis of the dataset characteristics, as well as a performance evaluation of several state-of-the-art approaches based on our benchmark.},
	language = {en},
	urldate = {2021-06-04},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Cordts, Marius and Omran, Mohamed and Ramos, Sebastian and Rehfeld, Timo and Enzweiler, Markus and Benenson, Rodrigo and Franke, Uwe and Roth, Stefan and Schiele, Bernt},
	month = jun,
	year = {2016},
	pages = {3213--3223},
}

@article{li_lidar_2020,
	title = {Lidar for {Autonomous} {Driving}: {The} {Principles}, {Challenges}, and {Trends} for {Automotive} {Lidar} and {Perception} {Systems}},
	volume = {37},
	issn = {1053-5888, 1558-0792},
	shorttitle = {Lidar for {Autonomous} {Driving}},
	url = {https://ieeexplore.ieee.org/document/9127855/},
	doi = {10.1109/MSP.2020.2973615},
	language = {en},
	number = {4},
	urldate = {2021-06-04},
	journal = {IEEE Signal Processing Magazine},
	author = {Li, You and Ibanez-Guzman, Javier},
	month = jul,
	year = {2020},
	pages = {50--61},
}

@inproceedings{le_pointgrid_2018,
	address = {Salt Lake City, UT},
	title = {{PointGrid}: {A} {Deep} {Network} for {3D} {Shape} {Understanding}},
	isbn = {978-1-5386-6420-9},
	shorttitle = {{PointGrid}},
	url = {https://ieeexplore.ieee.org/document/8579057/},
	doi = {10.1109/CVPR.2018.00959},
	abstract = {Volumetric grid is widely used for 3D deep learning due to its regularity. However the use of relatively lower order local approximation functions such as piece-wise constant function (occupancy grid) or piece-wise linear function (distance ﬁeld) to approximate 3D shape means that it needs a very high-resolution grid to represent ﬁner geometry details, which could be memory and computationally inefﬁcient. In this work, we propose the PointGrid, a 3D convolutional network that incorporates a constant number of points within each grid cell thus allowing the network to learn higher order local approximation functions that could better represent the local geometry shape details. With experiments on popular shape recognition benchmarks, PointGrid demonstrates state-of-the-art performance over existing deep learning methods on both classiﬁcation and segmentation.},
	language = {en},
	urldate = {2021-06-03},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Le, Truc and Duan, Ye},
	month = jun,
	year = {2018},
	pages = {9204--9214},
}

@article{wu_3d_2015,
	title = {{3D} {ShapeNets}: {A} {Deep} {Representation} for {Volumetric} {Shapes}},
	shorttitle = {{3D} {ShapeNets}},
	url = {http://arxiv.org/abs/1406.5670},
	abstract = {3D shape is a crucial but heavily underutilized cue in today's computer vision systems, mostly due to the lack of a good generic shape representation. With the recent availability of inexpensive 2.5D depth sensors (e.g. Microsoft Kinect), it is becoming increasingly important to have a powerful 3D shape representation in the loop. Apart from category recognition, recovering full 3D shapes from view-based 2.5D depth maps is also a critical part of visual understanding. To this end, we propose to represent a geometric 3D shape as a probability distribution of binary variables on a 3D voxel grid, using a Convolutional Deep Belief Network. Our model, 3D ShapeNets, learns the distribution of complex 3D shapes across different object categories and arbitrary poses from raw CAD data, and discovers hierarchical compositional part representations automatically. It naturally supports joint object recognition and shape completion from 2.5D depth maps, and it enables active object recognition through view planning. To train our 3D deep learning model, we construct ModelNet -- a large-scale 3D CAD model dataset. Extensive experiments show that our 3D deep representation enables significant performance improvement over the-state-of-the-arts in a variety of tasks.},
	language = {en},
	urldate = {2021-06-03},
	journal = {arXiv:1406.5670 [cs]},
	author = {Wu, Zhirong and Song, Shuran and Khosla, Aditya and Yu, Fisher and Zhang, Linguang and Tang, Xiaoou and Xiao, Jianxiong},
	month = apr,
	year = {2015},
	note = {arXiv: 1406.5670},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{maturana_voxnet_2015,
	address = {Hamburg, Germany},
	title = {{VoxNet}: {A} {3D} {Convolutional} {Neural} {Network} for real-time object recognition},
	isbn = {978-1-4799-9994-1},
	shorttitle = {{VoxNet}},
	url = {http://ieeexplore.ieee.org/document/7353481/},
	doi = {10.1109/IROS.2015.7353481},
	abstract = {Robust object recognition is a crucial skill for robots operating autonomously in real world environments. Range sensors such as LiDAR and RGBD cameras are increasingly found in modern robotic systems, providing a rich source of 3D information that can aid in this task. However, many current systems do not fully utilize this information and have trouble efﬁciently dealing with large amounts of point cloud data. In this paper, we propose VoxNet, an architecture to tackle this problem by integrating a volumetric Occupancy Grid representation with a supervised 3D Convolutional Neural Network (3D CNN). We evaluate our approach on publicly available benchmarks using LiDAR, RGBD, and CAD data. VoxNet achieves accuracy beyond the state of the art while labeling hundreds of instances per second.},
	language = {en},
	urldate = {2021-06-03},
	booktitle = {2015 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	publisher = {IEEE},
	author = {Maturana, Daniel and Scherer, Sebastian},
	month = sep,
	year = {2015},
	pages = {922--928},
}

@article{lai_large-scale_nodate,
	title = {A {Large}-{Scale} {Hierarchical} {Multi}-{View} {RGB}-{D} {Object} {Dataset}},
	abstract = {Over the last decade, the availability of public image repositories and recognition benchmarks has enabled rapid progress in visual object category and instance detection. Today we are witnessing the birth of a new generation of sensing technologies capable of providing high quality synchronized videos of both color and depth, the RGB-D (Kinectstyle) camera. With its advanced sensing capabilities and the potential for mass adoption, this technology represents an opportunity to dramatically increase robotic object recognition, manipulation, navigation, and interaction capabilities. In this paper, we introduce a large-scale, hierarchical multi-view object dataset collected using an RGB-D camera. The dataset contains 300 objects organized into 51 categories and has been made publicly available to the research community so as to enable rapid progress based on this promising technology. This paper describes the dataset collection procedure and introduces techniques for RGB-D based object recognition and detection, demonstrating that combining color and depth information substantially improves quality of results.},
	language = {en},
	author = {Lai, Kevin and Bo, Liefeng and Ren, Xiaofeng and Fox, Dieter},
	pages = {8},
}

@inproceedings{wei_view-gcn_2020,
	address = {Seattle, WA, USA},
	title = {View-{GCN}: {View}-{Based} {Graph} {Convolutional} {Network} for {3D} {Shape} {Analysis}},
	isbn = {978-1-72817-168-5},
	shorttitle = {View-{GCN}},
	url = {https://ieeexplore.ieee.org/document/9156567/},
	doi = {10.1109/CVPR42600.2020.00192},
	abstract = {View-based approach that recognizes 3D shape through its projected 2D images has achieved state-of-the-art results for 3D shape recognition. The major challenge for view-based approach is how to aggregate multi-view features to be a global shape descriptor. In this work, we propose a novel view-based Graph Convolutional Neural Network, dubbed as view-GCN, to recognize 3D shape based on graph representation of multiple views in ﬂexible view conﬁgurations. We ﬁrst construct view-graph with multiple views as graph nodes, then design a graph convolutional neural network over view-graph to hierarchically learn discriminative shape descriptor considering relations of multiple views. The view-GCN is a hierarchical network based on local and non-local graph convolution for feature transform, and selective view-sampling for graph coarsening. Extensive experiments on benchmark datasets show that view-GCN achieves state-of-the-art results for 3D shape classiﬁcation and retrieval.},
	language = {en},
	urldate = {2021-06-03},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Wei, Xin and Yu, Ruixuan and Sun, Jian},
	month = jun,
	year = {2020},
	pages = {1847--1856},
}

@inproceedings{su_multi-view_2015,
	address = {Santiago, Chile},
	title = {Multi-view {Convolutional} {Neural} {Networks} for {3D} {Shape} {Recognition}},
	isbn = {978-1-4673-8391-2},
	url = {http://ieeexplore.ieee.org/document/7410471/},
	doi = {10.1109/ICCV.2015.114},
	abstract = {A longstanding question in computer vision concerns the representation of 3D shapes for recognition: should 3D shapes be represented with descriptors operating on their native 3D formats, such as voxel grid or polygon mesh, or can they be effectively represented with view-based descriptors? We address this question in the context of learning to recognize 3D shapes from a collection of their rendered views on 2D images. We ﬁrst present a standard CNN architecture trained to recognize the shapes’ rendered views independently of each other, and show that a 3D shape can be recognized even from a single view at an accuracy far higher than using state-of-the-art 3D shape descriptors. Recognition rates further increase when multiple views of the shapes are provided. In addition, we present a novel CNN architecture that combines information from multiple views of a 3D shape into a single and compact shape descriptor offering even better recognition performance. The same architecture can be applied to accurately recognize human hand-drawn sketches of shapes. We conclude that a collection of 2D views can be highly informative for 3D shape recognition and is amenable to emerging CNN architectures and their derivatives.},
	language = {en},
	urldate = {2021-06-03},
	booktitle = {2015 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Su, Hang and Maji, Subhransu and Kalogerakis, Evangelos and Learned-Miller, Erik},
	month = dec,
	year = {2015},
	pages = {945--953},
}

@inproceedings{liu_mi-fgsm_2020,
	address = {Xi'an China},
	title = {{MI}-{FGSM} on {Faster} {R}-{CNN} {Object} {Detector}},
	isbn = {978-1-4503-8907-5},
	url = {https://dl.acm.org/doi/10.1145/3447450.3447455},
	doi = {10.1145/3447450.3447455},
	abstract = {The adversarial examples show the vulnerability of deep neural networks, which makes adversarial attacks widely concerned. However, most of the attack methods are based on image classification model. In this paper, we use Momentum Iterative Fast Gradient Sign Method (MI-FGSM), which stabilize optimization and escape from poor local maxima, to generate adversarial examples on the Faster R-CNN object detector. We have made some improvements on the previous object detection attack methods. The best current attack method, Project Gradient Descent (PGD) on object detection, starts from a random value, resulting in the uncertainty of the attack result. In contrast, our attacks are more stable and powerful in both white-box attacks and black-box attacks, and can better adapt to various neural network architectures. Experiment on Pascal VOC2007 shows that, under same setting of white-box attack, PGD has 0.23\% mean average precision (mAP) on Faster R-CNN with VGG16, while our method achieves 0.17\%. In addition, we analyze the difference between classification and detection attacks, and find that in addition to misclassification, the adversarial examples produced by detection models can also lead to mislocation.},
	language = {en},
	urldate = {2021-06-03},
	booktitle = {2020 {The} 4th {International} {Conference} on {Video} and {Image} {Processing}},
	publisher = {ACM},
	author = {Liu, Zhenghao and Peng, Wenyu and Zhou, Jun and Wu, Zifeng and Zhang, Jintao and Zhang, Yunchun},
	month = dec,
	year = {2020},
	pages = {27--32},
}

@article{arnab_robustness_nodate,
	title = {On the {Robustness} of {Semantic} {Segmentation} {Models} to {Adversarial} {Attacks}},
	abstract = {Deep Neural Networks (DNNs) have been demonstrated to perform exceptionally well on most recognition tasks such as image classiﬁcation and segmentation. However, they have also been shown to be vulnerable to adversarial examples. This phenomenon has recently attracted a lot of attention but it has not been extensively studied on multiple, large-scale datasets and complex tasks such as semantic segmentation which often require more specialised networks with additional components such as CRFs, dilated convolutions, skip-connections and multiscale processing.},
	language = {en},
	author = {Arnab, Anurag and Miksik, Ondrej and Torr, Philip H S},
	pages = {10},
}

@article{lin_microsoft_2015,
	title = {Microsoft {COCO}: {Common} {Objects} in {Context}},
	shorttitle = {Microsoft {COCO}},
	url = {http://arxiv.org/abs/1405.0312},
	abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
	language = {en},
	urldate = {2021-06-03},
	journal = {arXiv:1405.0312 [cs]},
	author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Bourdev, Lubomir and Girshick, Ross and Hays, James and Perona, Pietro and Ramanan, Deva and Zitnick, C. Lawrence and Dollár, Piotr},
	month = feb,
	year = {2015},
	note = {arXiv: 1405.0312},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{arnab_pixelwise_2017,
	title = {Pixelwise {Instance} {Segmentation} with a {Dynamically} {Instantiated} {Network}},
	url = {http://arxiv.org/abs/1704.02386},
	abstract = {Semantic segmentation and object detection research have recently achieved rapid progress. However, the former task has no notion of different instances of the same object, and the latter operates at a coarse, bounding-box level. We propose an Instance Segmentation system that produces a segmentation map where each pixel is assigned an object class and instance identity label. Most approaches adapt object detectors to produce segments instead of boxes. In contrast, our method is based on an initial semantic segmentation module, which feeds into an instance subnetwork. This subnetwork uses the initial category-level segmentation, along with cues from the output of an object detector, within an end-to-end CRF to predict instances. This part of our model is dynamically instantiated to produce a variable number of instances per image. Our end-to-end approach requires no post-processing and considers the image holistically, instead of processing independent proposals. Therefore, unlike some related work, a pixel cannot belong to multiple instances. Furthermore, far more precise segmentations are achieved, as shown by our substantial improvements at high AP r thresholds.},
	language = {en},
	urldate = {2021-06-03},
	journal = {arXiv:1704.02386 [cs]},
	author = {Arnab, Anurag and Torr, Philip H. S.},
	month = apr,
	year = {2017},
	note = {arXiv: 1704.02386},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{girshick_fast_2015,
	title = {Fast {R}-{CNN}},
	url = {http://arxiv.org/abs/1504.08083},
	abstract = {This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efﬁciently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9× faster than R-CNN, is 213× faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3× faster, tests 10× faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https: //github.com/rbgirshick/fast-rcnn.},
	language = {en},
	urldate = {2021-06-03},
	journal = {arXiv:1504.08083 [cs]},
	author = {Girshick, Ross},
	month = sep,
	year = {2015},
	note = {arXiv: 1504.08083},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{he_mask_2018,
	title = {Mask {R}-{CNN}},
	url = {http://arxiv.org/abs/1703.06870},
	abstract = {We present a conceptually simple, ﬂexible, and general framework for object instance segmentation. Our approach efﬁciently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, boundingbox object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: https://github.com/ facebookresearch/Detectron.},
	language = {en},
	urldate = {2021-06-03},
	journal = {arXiv:1703.06870 [cs]},
	author = {He, Kaiming and Gkioxari, Georgia and Dollár, Piotr and Girshick, Ross},
	month = jan,
	year = {2018},
	note = {arXiv: 1703.06870},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{lin_feature_2017,
	title = {Feature {Pyramid} {Networks} for {Object} {Detection}},
	url = {http://arxiv.org/abs/1612.03144},
	abstract = {Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But recent deep learning object detectors have avoided pyramid representations, in part because they are compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A topdown architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network (FPN), shows signiﬁcant improvement as a generic feature extractor in several applications. Using FPN in a basic Faster R-CNN system, our method achieves state-of-the-art singlemodel results on the COCO detection benchmark without bells and whistles, surpassing all existing single-model entries including those from the COCO 2016 challenge winners. In addition, our method can run at 6 FPS on a GPU and thus is a practical and accurate solution to multi-scale object detection. Code will be made publicly available.},
	language = {en},
	urldate = {2021-06-03},
	journal = {arXiv:1612.03144 [cs]},
	author = {Lin, Tsung-Yi and Dollár, Piotr and Girshick, Ross and He, Kaiming and Hariharan, Bharath and Belongie, Serge},
	month = apr,
	year = {2017},
	note = {arXiv: 1612.03144},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{dai_r-fcn_2016,
	title = {R-{FCN}: {Object} {Detection} via {Region}-based {Fully} {Convolutional} {Networks}},
	shorttitle = {R-{FCN}},
	url = {http://arxiv.org/abs/1605.06409},
	abstract = {We present region-based, fully convolutional networks for accurate and efﬁcient object detection. In contrast to previous region-based detectors such as Fast/Faster R-CNN [6, 18] that apply a costly per-region subnetwork hundreds of times, our region-based detector is fully convolutional with almost all computation shared on the entire image. To achieve this goal, we propose position-sensitive score maps to address a dilemma between translation-invariance in image classiﬁcation and translation-variance in object detection. Our method can thus naturally adopt fully convolutional image classiﬁer backbones, such as the latest Residual Networks (ResNets) [9], for object detection. We show competitive results on the PASCAL VOC datasets (e.g., 83.6\% mAP on the 2007 set) with the 101-layer ResNet. Meanwhile, our result is achieved at a test-time speed of 170ms per image, 2.5-20× faster than the Faster R-CNN counterpart. Code is made publicly available at: https://github.com/daijifeng001/r-fcn.},
	language = {en},
	urldate = {2021-06-03},
	journal = {arXiv:1605.06409 [cs]},
	author = {Dai, Jifeng and Li, Yi and He, Kaiming and Sun, Jian},
	month = jun,
	year = {2016},
	note = {arXiv: 1605.06409},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{papernot_limitations_2015,
	title = {The {Limitations} of {Deep} {Learning} in {Adversarial} {Settings}},
	url = {http://arxiv.org/abs/1511.07528},
	abstract = {Deep learning takes advantage of large datasets and computationally efﬁcient training algorithms to outperform other approaches at various machine learning tasks. However, imperfections in the training phase of deep neural networks make them vulnerable to adversarial samples: inputs crafted by adversaries with the intent of causing deep neural networks to misclassify. In this work, we formalize the space of adversaries against deep neural networks (DNNs) and introduce a novel class of algorithms to craft adversarial samples based on a precise understanding of the mapping between inputs and outputs of DNNs. In an application to computer vision, we show that our algorithms can reliably produce samples correctly classiﬁed by human subjects but misclassiﬁed in speciﬁc targets by a DNN with a 97\% adversarial success rate while only modifying on average 4.02\% of the input features per sample. We then evaluate the vulnerability of different sample classes to adversarial perturbations by deﬁning a hardness measure. Finally, we describe preliminary work outlining defenses against adversarial samples by deﬁning a predictive measure of distance between a benign input and a target classiﬁcation.},
	language = {en},
	urldate = {2021-06-02},
	journal = {arXiv:1511.07528 [cs, stat]},
	author = {Papernot, Nicolas and McDaniel, Patrick and Jha, Somesh and Fredrikson, Matt and Celik, Z. Berkay and Swami, Ananthram},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.07528},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{yuan_adversarial_2018,
	title = {Adversarial {Examples}: {Attacks} and {Defenses} for {Deep} {Learning}},
	shorttitle = {Adversarial {Examples}},
	url = {http://arxiv.org/abs/1712.07107},
	abstract = {With rapid progress and significant successes in a wide spectrum of applications, deep learning is being applied in many safety-critical environments. However, deep neural networks have been recently found vulnerable to well-designed input samples, called adversarial examples. Adversarial examples are imperceptible to human but can easily fool deep neural networks in the testing/deploying stage. The vulnerability to adversarial examples becomes one of the major risks for applying deep neural networks in safety-critical environments. Therefore, attacks and defenses on adversarial examples draw great attention. In this paper, we review recent findings on adversarial examples for deep neural networks, summarize the methods for generating adversarial examples, and propose a taxonomy of these methods. Under the taxonomy, applications for adversarial examples are investigated. We further elaborate on countermeasures for adversarial examples and explore the challenges and the potential solutions.},
	urldate = {2021-06-02},
	journal = {arXiv:1712.07107 [cs, stat]},
	author = {Yuan, Xiaoyong and He, Pan and Zhu, Qile and Li, Xiaolin},
	month = jul,
	year = {2018},
	note = {arXiv: 1712.07107},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{simonelli_disentangling_2019,
	address = {Seoul, Korea (South)},
	title = {Disentangling {Monocular} {3D} {Object} {Detection}},
	isbn = {978-1-72814-803-8},
	url = {https://ieeexplore.ieee.org/document/9010618/},
	doi = {10.1109/ICCV.2019.00208},
	abstract = {In this paper we propose an approach for monocular 3D object detection from a single RGB image, which leverages a novel disentangling transformation for 2D and 3D detection losses and a novel, self-supervised conﬁdence score for 3D bounding boxes. Our proposed loss disentanglement has the twofold advantage of simplifying the training dynamics in the presence of losses with complex interactions of parameters, and sidestepping the issue of balancing independent regression terms. Our solution overcomes these issues by isolating the contribution made by groups of parameters to a given loss, without changing its nature. We further apply loss disentanglement to another novel, signed Intersection-over-Union criterion-driven loss for improving 2D detection results. Besides our methodological innovations, we critically review the AP metric used in KITTI3D, which emerged as the most important dataset for comparing 3D detection results. We identify and resolve a ﬂaw in the 11-point interpolated AP metric, affecting all previously published detection results and particularly biases the results of monocular 3D detection. We provide extensive experimental evaluations and ablation studies on the KITTI3D and nuScenes datasets, setting new state-of-theart results on object category car by large margins.},
	language = {en},
	urldate = {2021-06-02},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Simonelli, Andrea and Bulo, Samuel Rota and Porzi, Lorenzo and Lopez-Antequera, Manuel and Kontschieder, Peter},
	month = oct,
	year = {2019},
	pages = {1991--1999},
}

@inproceedings{milioto_rangenet_2019,
	address = {Macau, China},
	title = {{RangeNet} ++: {Fast} and {Accurate} {LiDAR} {Semantic} {Segmentation}},
	isbn = {978-1-72814-004-9},
	shorttitle = {{RangeNet} ++},
	url = {https://ieeexplore.ieee.org/document/8967762/},
	doi = {10.1109/IROS40897.2019.8967762},
	abstract = {Perception in autonomous vehicles is often carried out through a suite of different sensing modalities. Given the massive amount of openly available labeled RGB data and the advent of high-quality deep learning algorithms for image-based recognition, high-level semantic perception tasks are pre-dominantly solved using high-resolution cameras. As a result of that, other sensor modalities potentially useful for this task are often ignored. In this paper, we push the state of the art in LiDAR-only semantic segmentation forward in order to provide another independent source of semantic information to the vehicle. Our approach can accurately perform full semantic segmentation of LiDAR point clouds at sensor frame rate. We exploit range images as an intermediate representation in combination with a Convolutional Neural Network (CNN) exploiting the rotating LiDAR sensor model. To obtain accurate results, we propose a novel postprocessing algorithm that deals with problems arising from this intermediate representation such as discretization errors and blurry CNN outputs. We implemented and thoroughly evaluated our approach including several comparisons to the state of the art. Our experiments show that our approach outperforms state-of-the-art approaches, while still running online on a single embedded GPU. The code can be accessed at https://github.com/PRBonn/lidar-bonnetal.},
	language = {en},
	urldate = {2021-06-01},
	booktitle = {2019 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	publisher = {IEEE},
	author = {Milioto, Andres and Vizzo, Ignacio and Behley, Jens and Stachniss, Cyrill},
	month = nov,
	year = {2019},
	pages = {4213--4220},
}

@article{wu_squeezesegv2_2018,
	title = {{SqueezeSegV2}: {Improved} {Model} {Structure} and {Unsupervised} {Domain} {Adaptation} for {Road}-{Object} {Segmentation} from a {LiDAR} {Point} {Cloud}},
	shorttitle = {{SqueezeSegV2}},
	url = {http://arxiv.org/abs/1809.08495},
	abstract = {Earlier work demonstrates the promise of deeplearning-based approaches for point cloud segmentation; however, these approaches need to be improved to be practically useful. To this end, we introduce a new model SqueezeSegV2 that is more robust to dropout noise in LiDAR point clouds. With improved model structure, training loss, batch normalization and additional input channel, SqueezeSegV2 achieves signiﬁcant accuracy improvement when trained on real data. Training models for point cloud segmentation requires large amounts of labeled point-cloud data, which is expensive to obtain. To sidestep the cost of collection and annotation, simulators such as GTA-V can be used to create unlimited amounts of labeled, synthetic data. However, due to domain shift, models trained on synthetic data often do not generalize well to the real world. We address this problem with a domainadaptation training pipeline consisting of three major components: 1) learned intensity rendering, 2) geodesic correlation alignment, and 3) progressive domain calibration. When trained on real data, our new model exhibits segmentation accuracy improvements of 6.0-8.6\% over the original SqueezeSeg. When training our new model on synthetic data using the proposed domain adaptation pipeline, we nearly double test accuracy on real-world data, from 29.0\% to 57.4\%. Our source code and synthetic dataset will be open-sourced.},
	language = {en},
	urldate = {2021-06-01},
	journal = {arXiv:1809.08495 [cs]},
	author = {Wu, Bichen and Zhou, Xuanyu and Zhao, Sicheng and Yue, Xiangyu and Keutzer, Kurt},
	month = sep,
	year = {2018},
	note = {arXiv: 1809.08495},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{iandola_squeezenet_2016,
	title = {{SqueezeNet}: {AlexNet}-level accuracy with 50x fewer parameters and {\textless}0.{5MB} model size},
	shorttitle = {{SqueezeNet}},
	url = {http://arxiv.org/abs/1602.07360},
	abstract = {Recent research on deep convolutional neural networks (CNNs) has focused primarily on improving accuracy. For a given accuracy level, it is typically possible to identify multiple CNN architectures that achieve that accuracy level. With equivalent accuracy, smaller CNN architectures offer at least three advantages: (1) Smaller CNNs require less communication across servers during distributed training. (2) Smaller CNNs require less bandwidth to export a new model from the cloud to an autonomous car. (3) Smaller CNNs are more feasible to deploy on FPGAs and other hardware with limited memory. To provide all of these advantages, we propose a small CNN architecture called SqueezeNet. SqueezeNet achieves AlexNet-level accuracy on ImageNet with 50x fewer parameters. Additionally, with model compression techniques, we are able to compress SqueezeNet to less than 0.5MB (510× smaller than AlexNet).},
	language = {en},
	urldate = {2021-06-01},
	journal = {arXiv:1602.07360 [cs]},
	author = {Iandola, Forrest N. and Han, Song and Moskewicz, Matthew W. and Ashraf, Khalid and Dally, William J. and Keutzer, Kurt},
	month = nov,
	year = {2016},
	note = {arXiv: 1602.07360},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
}

@article{wu_squeezeseg_2017,
	title = {{SqueezeSeg}: {Convolutional} {Neural} {Nets} with {Recurrent} {CRF} for {Real}-{Time} {Road}-{Object} {Segmentation} from {3D} {LiDAR} {Point} {Cloud}},
	shorttitle = {{SqueezeSeg}},
	url = {http://arxiv.org/abs/1710.07368},
	abstract = {In this paper, we address semantic segmentation of road-objects from 3D LiDAR point clouds. In particular, we wish to detect and categorize instances of interest, such as cars, pedestrians and cyclists. We formulate this problem as a pointwise classiﬁcation problem, and propose an end-to-end pipeline called SqueezeSeg based on convolutional neural networks (CNN): the CNN takes a transformed LiDAR point cloud as input and directly outputs a point-wise label map, which is then reﬁned by a conditional random ﬁeld (CRF) implemented as a recurrent layer. Instance-level labels are then obtained by conventional clustering algorithms. Our CNN model is trained on LiDAR point clouds from the KITTI [1] dataset, and our point-wise segmentation labels are derived from 3D bounding boxes from KITTI. To obtain extra training data, we built a LiDAR simulator into Grand Theft Auto V (GTA-V), a popular video game, to synthesize large amounts of realistic training data. Our experiments show that SqueezeSeg achieves high accuracy with astonishingly fast and stable runtime (8.7 ± 0.5 ms per frame), highly desirable for autonomous driving applications. Furthermore, additionally training on synthesized data boosts validation accuracy on real-world data. Our source code and synthesized data will be open-sourced.},
	language = {en},
	urldate = {2021-06-01},
	journal = {arXiv:1710.07368 [cs]},
	author = {Wu, Bichen and Wan, Alvin and Yue, Xiangyu and Keutzer, Kurt},
	month = oct,
	year = {2017},
	note = {arXiv: 1710.07368},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{zhou_voxelnet_2017,
	title = {{VoxelNet}: {End}-to-{End} {Learning} for {Point} {Cloud} {Based} {3D} {Object} {Detection}},
	shorttitle = {{VoxelNet}},
	url = {http://arxiv.org/abs/1711.06396},
	abstract = {Accurate detection of objects in 3D point clouds is a central problem in many applications, such as autonomous navigation, housekeeping robots, and augmented/virtual reality. To interface a highly sparse LiDAR point cloud with a region proposal network (RPN), most existing efforts have focused on hand-crafted feature representations, for example, a bird’s eye view projection. In this work, we remove the need of manual feature engineering for 3D point clouds and propose VoxelNet, a generic 3D detection network that uniﬁes feature extraction and bounding box prediction into a single stage, end-to-end trainable deep network. Speciﬁcally, VoxelNet divides a point cloud into equally spaced 3D voxels and transforms a group of points within each voxel into a uniﬁed feature representation through the newly introduced voxel feature encoding (VFE) layer. In this way, the point cloud is encoded as a descriptive volumetric representation, which is then connected to a RPN to generate detections. Experiments on the KITTI car detection benchmark show that VoxelNet outperforms the state-of-the-art LiDAR based 3D detection methods by a large margin. Furthermore, our network learns an effective discriminative representation of objects with various geometries, leading to encouraging results in 3D detection of pedestrians and cyclists, based on only LiDAR.},
	language = {en},
	urldate = {2021-06-01},
	journal = {arXiv:1711.06396 [cs]},
	author = {Zhou, Yin and Tuzel, Oncel},
	month = nov,
	year = {2017},
	note = {arXiv: 1711.06396},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{graham_3d_2017,
	title = {{3D} {Semantic} {Segmentation} with {Submanifold} {Sparse} {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1711.10275},
	abstract = {Convolutional networks are the de-facto standard for analyzing spatio-temporal data such as images, videos, and 3D shapes. Whilst some of this data is naturally dense (e.g., photos), many other data sources are inherently sparse. Examples include 3D point clouds that were obtained using a LiDAR scanner or RGB-D camera. Standard “dense” implementations of convolutional networks are very inefﬁcient when applied on such sparse data. We introduce new sparse convolutional operations that are designed to process spatially-sparse data more efﬁciently, and use them to develop spatially-sparse convolutional networks. We demonstrate the strong performance of the resulting models, called submanifold sparse convolutional networks (SSCNs), on two tasks involving semantic segmentation of 3D point clouds. In particular, our models outperform all prior state-of-the-art on the test set of a recent semantic segmentation competition.},
	language = {en},
	urldate = {2021-06-01},
	journal = {arXiv:1711.10275 [cs]},
	author = {Graham, Benjamin and Engelcke, Martin and van der Maaten, Laurens},
	month = nov,
	year = {2017},
	note = {arXiv: 1711.10275},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{qi_frustum_2018,
	title = {Frustum {PointNets} for {3D} {Object} {Detection} from {RGB}-{D} {Data}},
	url = {http://arxiv.org/abs/1711.08488},
	abstract = {In this work, we study 3D object detection from RGBD data in both indoor and outdoor scenes. While previous methods focus on images or 3D voxels, often obscuring natural 3D patterns and invariances of 3D data, we directly operate on raw point clouds by popping up RGB-D scans. However, a key challenge of this approach is how to efﬁciently localize objects in point clouds of large-scale scenes (region proposal). Instead of solely relying on 3D proposals, our method leverages both mature 2D object detectors and advanced 3D deep learning for object localization, achieving efﬁciency as well as high recall for even small objects. Beneﬁted from learning directly in raw point clouds, our method is also able to precisely estimate 3D bounding boxes even under strong occlusion or with very sparse points. Evaluated on KITTI and SUN RGB-D 3D detection benchmarks, our method outperforms the state of the art by remarkable margins while having real-time capability.},
	language = {en},
	urldate = {2021-06-01},
	journal = {arXiv:1711.08488 [cs]},
	author = {Qi, Charles R. and Liu, Wei and Wu, Chenxia and Su, Hao and Guibas, Leonidas J.},
	month = apr,
	year = {2018},
	note = {arXiv: 1711.08488},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{ku_joint_2018,
	title = {Joint {3D} {Proposal} {Generation} and {Object} {Detection} from {View} {Aggregation}},
	url = {http://arxiv.org/abs/1712.02294},
	abstract = {We present AVOD, an Aggregate View Object Detection network for autonomous driving scenarios. The proposed neural network architecture uses LIDAR point clouds and RGB images to generate features that are shared by two subnetworks: a region proposal network (RPN) and a second stage detector network. The proposed RPN uses a novel architecture capable of performing multimodal feature fusion on high resolution feature maps to generate reliable 3D object proposals for multiple object classes in road scenes. Using these proposals, the second stage detection network performs accurate oriented 3D bounding box regression and category classification to predict the extents, orientation, and classification of objects in 3D space. Our proposed architecture is shown to produce state of the art results on the KITTI 3D object detection benchmark while running in real time with a low memory footprint, making it a suitable candidate for deployment on autonomous vehicles. Code is at: https://github.com/kujason/avod},
	language = {en},
	urldate = {2021-06-01},
	journal = {arXiv:1712.02294 [cs]},
	author = {Ku, Jason and Mozifian, Melissa and Lee, Jungwook and Harakeh, Ali and Waslander, Steven},
	month = jul,
	year = {2018},
	note = {arXiv: 1712.02294},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{chen_multi-view_2017,
	title = {Multi-{View} {3D} {Object} {Detection} {Network} for {Autonomous} {Driving}},
	url = {http://arxiv.org/abs/1611.07759},
	abstract = {This paper aims at high-accuracy 3D object detection in autonomous driving scenario. We propose Multi-View 3D networks (MV3D), a sensory-fusion framework that takes both LIDAR point cloud and RGB images as input and predicts oriented 3D bounding boxes. We encode the sparse 3D point cloud with a compact multi-view representation. The network is composed of two subnetworks: one for 3D object proposal generation and another for multi-view feature fusion. The proposal network generates 3D candidate boxes efﬁciently from the bird’s eye view representation of 3D point cloud. We design a deep fusion scheme to combine region-wise features from multiple views and enable interactions between intermediate layers of different paths. Experiments on the challenging KITTI benchmark show that our approach outperforms the state-of-the-art by around 25\% and 30\% AP on the tasks of 3D localization and 3D detection. In addition, for 2D detection, our approach obtains 10.3\% higher AP than the state-of-the-art on the hard data among the LIDAR-based methods.},
	language = {en},
	urldate = {2021-06-01},
	journal = {arXiv:1611.07759 [cs]},
	author = {Chen, Xiaozhi and Ma, Huimin and Wan, Ji and Li, Bo and Xia, Tian},
	month = jun,
	year = {2017},
	note = {arXiv: 1611.07759},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{rehman_dbscan_2014,
	address = {Bangalore, India},
	title = {{DBSCAN}: {Past}, present and future},
	isbn = {978-1-4799-2259-8 978-1-4799-2258-1},
	shorttitle = {{DBSCAN}},
	url = {http://ieeexplore.ieee.org/document/6814687/},
	doi = {10.1109/ICADIWT.2014.6814687},
	abstract = {Data Mining is all about data analysis techniques. It is useful for extracting hidden and interesting patterns from large datasets. Clustering techniques are important when it comes to extracting knowledge from large amount of spatial data collected from various applications including GIS, satellite images, X-ray crystallography, remote sensing and environmental assessment and planning etc. To extract useful pattern from these complex data sources several popular spatial data clustering techniques have been proposed. DBSCAN (Density Based Spatial Clustering of Applications with Noise) is a pioneer density based algorithm. It can discover clusters of any arbitrary shape and size in databases containing even noise and outliers. DBSCAN however are known to have a number of problems such as: (a) it requires user’s input to specify parameter values for executing the algorithm; (b) it is prone to dilemma in deciding meaningful clusters from datasets with varying densities; (c) and it incurs certain computational complexity. Many researchers attempted to enhance the basic DBSCAN algorithm, in order to overcome these drawbacks, such as VDBSCAN, FDBSCAN, DD\_DBSCAN, and IDBSCAN. In this study, we survey over different variations of DBSCAN algorithms that were proposed so far. These variations are critically evaluated and their limitations are also listed.},
	language = {en},
	urldate = {2021-05-31},
	booktitle = {The {Fifth} {International} {Conference} on the {Applications} of {Digital} {Information} and {Web} {Technologies} ({ICADIWT} 2014)},
	publisher = {IEEE},
	author = {Rehman, Saif Ur and Asghar, Sohail and Fong, Simon and Sarasvady, S.},
	month = feb,
	year = {2014},
	pages = {232--238},
}

@article{fischler_random_1981,
	title = {Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography},
	volume = {24},
	issn = {0001-0782, 1557-7317},
	shorttitle = {Random sample consensus},
	url = {https://dl.acm.org/doi/10.1145/358669.358692},
	doi = {10.1145/358669.358692},
	abstract = {A new paradigm, Random Sample Consensus (RANSAC), for fitting a model to experimental data is introduced. RANSAC is capable of interpreting/smoothing data containing a significant percentage of gross errors, and is thus ideally suited for applications in automated image analysis where interpretation is based on the data provided by error-prone feature detectors. A major portion of this paper describes the application of RANSAC to the Location Determination Problem (LDP): Given an image depicting a set of landmarks with known locations, determine that point in space from which the image was obtained. In response to a RANSAC requirement, new results are derived on the minimum number of landmarks needed to obtain a solution, and algorithms are presented for computing these minimum-landmark solutions in closed form. These results provide the basis for an automatic system that can solve the LDP under difficult viewing},
	language = {en},
	number = {6},
	urldate = {2021-05-31},
	journal = {Communications of the ACM},
	author = {Fischler, Martin A. and Bolles, Robert C.},
	month = jun,
	year = {1981},
	pages = {381--395},
}

@article{qiPointNetDeepHierarchical2017,
	title = {{PointNet}++: {Deep} {Hierarchical} {Feature} {Learning} on {Point} {Sets} in a {Metric} {Space}},
	shorttitle = {{PointNet}++},
	url = {http://arxiv.org/abs/1706.02413},
	abstract = {Few prior works study deep learning on point sets. PointNet [20] is a pioneer in this direction. However, by design PointNet does not capture local structures induced by the metric space points live in, limiting its ability to recognize ﬁne-grained patterns and generalizability to complex scenes. In this work, we introduce a hierarchical neural network that applies PointNet recursively on a nested partitioning of the input point set. By exploiting metric space distances, our network is able to learn local features with increasing contextual scales. With further observation that point sets are usually sampled with varying densities, which results in greatly decreased performance for networks trained on uniform densities, we propose novel set learning layers to adaptively combine features from multiple scales. Experiments show that our network called PointNet++ is able to learn deep point set features efﬁciently and robustly. In particular, results signiﬁcantly better than state-of-the-art have been obtained on challenging benchmarks of 3D point clouds.},
	language = {en},
	urldate = {2021-05-28},
	journal = {arXiv:1706.02413 [cs]},
	author = {Qi, Charles R. and Yi, Li and Su, Hao and Guibas, Leonidas J.},
	month = jun,
	year = {2017},
	note = {arXiv: 1706.02413},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{shi_pv-rcnn_2021,
	title = {{PV}-{RCNN}: {Point}-{Voxel} {Feature} {Set} {Abstraction} for {3D} {Object} {Detection}},
	shorttitle = {{PV}-{RCNN}},
	url = {http://arxiv.org/abs/1912.13192},
	abstract = {We present a novel and high-performance 3D object detection framework, named PointVoxel-RCNN (PV-RCNN), for accurate 3D object detection from point clouds. Our proposed method deeply integrates both 3D voxel Convolutional Neural Network (CNN) and PointNet-based set abstraction to learn more discriminative point cloud features. It takes advantages of efﬁcient learning and high-quality proposals of the 3D voxel CNN and the ﬂexible receptive ﬁelds of the PointNet-based networks. Speciﬁcally, the proposed framework summarizes the 3D scene with a 3D voxel CNN into a small set of keypoints via a novel voxel set abstraction module to save follow-up computations and also to encode representative scene features. Given the highquality 3D proposals generated by the voxel CNN, the RoIgrid pooling is proposed to abstract proposal-speciﬁc features from the keypoints to the RoI-grid points via keypoint set abstraction with multiple receptive ﬁelds. Compared with conventional pooling operations, the RoI-grid feature points encode much richer context information for accurately estimating object conﬁdences and locations. Extensive experiments on both the KITTI dataset and the Waymo Open dataset show that our proposed PV-RCNN surpasses state-of-the-art 3D detection methods with remarkable margins by using only point clouds. Code is available at https://github.com/open-mmlab/OpenPCDet.},
	language = {en},
	urldate = {2021-05-30},
	journal = {arXiv:1912.13192 [cs, eess]},
	author = {Shi, Shaoshuai and Guo, Chaoxu and Jiang, Li and Wang, Zhe and Shi, Jianping and Wang, Xiaogang and Li, Hongsheng},
	month = apr,
	year = {2021},
	note = {arXiv: 1912.13192},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing},
}

@article{yan_second_2018,
	title = {{SECOND}: {Sparsely} {Embedded} {Convolutional} {Detection}},
	volume = {18},
	issn = {1424-8220},
	shorttitle = {{SECOND}},
	url = {http://www.mdpi.com/1424-8220/18/10/3337},
	doi = {10.3390/s18103337},
	abstract = {LiDAR-based or RGB-D-based object detection is used in numerous applications, ranging from autonomous driving to robot vision. Voxel-based 3D convolutional networks have been used for some time to enhance the retention of information when processing point cloud LiDAR data. However, problems remain, including a slow inference speed and low orientation estimation performance. We therefore investigate an improved sparse convolution method for such networks, which signiﬁcantly increases the speed of both training and inference. We also introduce a new form of angle loss regression to improve the orientation estimation performance and a new data augmentation approach that can enhance the convergence speed and performance. The proposed network produces state-of-the-art results on the KITTI 3D object detection benchmarks while maintaining a fast inference speed.},
	language = {en},
	number = {10},
	urldate = {2021-05-30},
	journal = {Sensors},
	author = {Yan, Yan and Mao, Yuxing and Li, Bo},
	month = oct,
	year = {2018},
	pages = {3337},
}

@article{shi_points_2020,
	title = {From {Points} to {Parts}: {3D} {Object} {Detection} from {Point} {Cloud} with {Part}-aware and {Part}-aggregation {Network}},
	shorttitle = {From {Points} to {Parts}},
	url = {http://arxiv.org/abs/1907.03670},
	abstract = {3D object detection from LiDAR point cloud is a challenging problem in 3D scene understanding and has many practical applications. In this paper, we extend our preliminary work PointRCNN to a novel and strong point-cloud-based 3D object detection framework, the part-aware and aggregation neural network (Part-\$A{\textasciicircum}2\$ net). The whole framework consists of the part-aware stage and the part-aggregation stage. Firstly, the part-aware stage for the first time fully utilizes free-of-charge part supervisions derived from 3D ground-truth boxes to simultaneously predict high quality 3D proposals and accurate intra-object part locations. The predicted intra-object part locations within the same proposal are grouped by our new-designed RoI-aware point cloud pooling module, which results in an effective representation to encode the geometry-specific features of each 3D proposal. Then the part-aggregation stage learns to re-score the box and refine the box location by exploring the spatial relationship of the pooled intra-object part locations. Extensive experiments are conducted to demonstrate the performance improvements from each component of our proposed framework. Our Part-\$A{\textasciicircum}2\$ net outperforms all existing 3D detection methods and achieves new state-of-the-art on KITTI 3D object detection dataset by utilizing only the LiDAR point cloud data. Code is available at https://github.com/sshaoshuai/PointCloudDet3D.},
	language = {en},
	urldate = {2021-05-30},
	journal = {arXiv:1907.03670 [cs]},
	author = {Shi, Shaoshuai and Wang, Zhe and Shi, Jianping and Wang, Xiaogang and Li, Hongsheng},
	month = mar,
	year = {2020},
	note = {arXiv: 1907.03670},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{shi_pointrcnn_2019,
	title = {{PointRCNN}: {3D} {Object} {Proposal} {Generation} and {Detection} from {Point} {Cloud}},
	shorttitle = {{PointRCNN}},
	url = {http://arxiv.org/abs/1812.04244},
	abstract = {In this paper, we propose PointRCNN for 3D object detection from raw point cloud. The whole framework is composed of two stages: stage-1 for the bottom-up 3D proposal generation and stage-2 for reﬁning proposals in the canonical coordinates to obtain the ﬁnal detection results. Instead of generating proposals from RGB image or projecting point cloud to bird’s view or voxels as previous methods do, our stage-1 sub-network directly generates a small number of high-quality 3D proposals from point cloud in a bottom-up manner via segmenting the point cloud of the whole scene into foreground points and background. The stage-2 sub-network transforms the pooled points of each proposal to canonical coordinates to learn better local spatial features, which is combined with global semantic features of each point learned in stage-1 for accurate box reﬁnement and conﬁdence prediction. Extensive experiments on the 3D detection benchmark of KITTI dataset show that our proposed architecture outperforms state-of-the-art methods with remarkable margins by using only point cloud as input. The code is available at https://github.com/sshaoshuai/PointRCNN.},
	language = {en},
	urldate = {2021-05-30},
	journal = {arXiv:1812.04244 [cs]},
	author = {Shi, Shaoshuai and Wang, Xiaogang and Li, Hongsheng},
	month = may,
	year = {2019},
	note = {arXiv: 1812.04244},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{liu_ssd_2016,
	title = {{SSD}: {Single} {Shot} {MultiBox} {Detector}},
	volume = {9905},
	shorttitle = {{SSD}},
	url = {http://arxiv.org/abs/1512.02325},
	doi = {10.1007/978-3-319-46448-0_2},
	abstract = {We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. SSD is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stages and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, COCO, and ILSVRC datasets conﬁrm that SSD has competitive accuracy to methods that utilize an additional object proposal step and is much faster, while providing a uniﬁed framework for both training and inference. For 300 × 300 input, SSD achieves 74.3\% mAP1 on VOC2007 test at 59 FPS on a Nvidia Titan X and for 512 × 512 input, SSD achieves 76.9\% mAP, outperforming a comparable state-of-the-art Faster R-CNN model. Compared to other single stage methods, SSD has much better accuracy even with a smaller input image size. Code is available at: https://github.com/weiliu89/caffe/tree/ssd .},
	language = {en},
	urldate = {2021-05-28},
	journal = {arXiv:1512.02325 [cs]},
	author = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander C.},
	year = {2016},
	note = {arXiv: 1512.02325},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {21--37},
}

@article{redmon_you_2016,
	title = {You {Only} {Look} {Once}: {Unified}, {Real}-{Time} {Object} {Detection}},
	shorttitle = {You {Only} {Look} {Once}},
	url = {http://arxiv.org/abs/1506.02640},
	abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classiﬁers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance.},
	language = {en},
	urldate = {2021-05-27},
	journal = {arXiv:1506.02640 [cs]},
	author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
	month = may,
	year = {2016},
	note = {arXiv: 1506.02640},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{he_deep_2015,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	url = {http://arxiv.org/abs/1512.03385},
	abstract = {Deeper neural networks are more difﬁcult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers—8× deeper than VGG nets [41] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classiﬁcation task. We also present analysis on CIFAR-10 with 100 and 1000 layers.},
	language = {en},
	urldate = {2021-05-25},
	journal = {arXiv:1512.03385 [cs]},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = dec,
	year = {2015},
	note = {arXiv: 1512.03385},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{redmon_yolo9000_2016,
	title = {{YOLO9000}: {Better}, {Faster}, {Stronger}},
	shorttitle = {{YOLO9000}},
	url = {http://arxiv.org/abs/1612.08242},
	abstract = {We introduce YOLO9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories. First we propose various improvements to the YOLO detection method, both novel and drawn from prior work. The improved model, YOLOv2, is state-of-the-art on standard detection tasks like PASCAL VOC and COCO. Using a novel, multi-scale training method the same YOLOv2 model can run at varying sizes, offering an easy tradeoff between speed and accuracy. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. At 40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like Faster RCNN with ResNet and SSD while still running signiﬁcantly faster. Finally we propose a method to jointly train on object detection and classiﬁcation. Using this method we train YOLO9000 simultaneously on the COCO detection dataset and the ImageNet classiﬁcation dataset. Our joint training allows YOLO9000 to predict detections for object classes that don’t have labelled detection data. We validate our approach on the ImageNet detection task. YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despite only having detection data for 44 of the 200 classes. On the 156 classes not in COCO, YOLO9000 gets 16.0 mAP. But YOLO can detect more than just 200 classes; it predicts detections for more than 9000 different object categories. And it still runs in real-time.},
	language = {en},
	urldate = {2021-05-25},
	journal = {arXiv:1612.08242 [cs]},
	author = {Redmon, Joseph and Farhadi, Ali},
	month = dec,
	year = {2016},
	note = {arXiv: 1612.08242},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{chen_deeplab_2017,
	title = {{DeepLab}: {Semantic} {Image} {Segmentation} with {Deep} {Convolutional} {Nets}, {Atrous} {Convolution}, and {Fully} {Connected} {CRFs}},
	shorttitle = {{DeepLab}},
	url = {http://arxiv.org/abs/1606.00915},
	abstract = {In this work we address the task of semantic image segmentation with Deep Learning and make three main contributions that are experimentally shown to have substantial practical merit. First, we highlight convolution with upsampled ﬁlters, or ‘atrous convolution’, as a powerful tool in dense prediction tasks. Atrous convolution allows us to explicitly control the resolution at which feature responses are computed within Deep Convolutional Neural Networks. It also allows us to effectively enlarge the ﬁeld of view of ﬁlters to incorporate larger context without increasing the number of parameters or the amount of computation. Second, we propose atrous spatial pyramid pooling (ASPP) to robustly segment objects at multiple scales. ASPP probes an incoming convolutional feature layer with ﬁlters at multiple sampling rates and effective ﬁelds-of-views, thus capturing objects as well as image context at multiple scales. Third, we improve the localization of object boundaries by combining methods from DCNNs and probabilistic graphical models. The commonly deployed combination of max-pooling and downsampling in DCNNs achieves invariance but has a toll on localization accuracy. We overcome this by combining the responses at the ﬁnal DCNN layer with a fully connected Conditional Random Field (CRF), which is shown both qualitatively and quantitatively to improve localization performance. Our proposed “DeepLab” system sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 79.7\% mIOU in the test set, and advances the results on three other datasets: PASCAL-Context, PASCAL-Person-Part, and Cityscapes. All of our code is made publicly available online.},
	language = {en},
	urldate = {2021-05-25},
	journal = {arXiv:1606.00915 [cs]},
	author = {Chen, Liang-Chieh and Papandreou, George and Kokkinos, Iasonas and Murphy, Kevin and Yuille, Alan L.},
	month = may,
	year = {2017},
	note = {arXiv: 1606.00915},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{chen_semantic_2016,
	title = {Semantic {Image} {Segmentation} with {Deep} {Convolutional} {Nets} and {Fully} {Connected} {CRFs}},
	url = {http://arxiv.org/abs/1412.7062},
	abstract = {Deep Convolutional Neural Networks (DCNNs) have recently shown state of the art performance in high level vision tasks, such as image classiﬁcation and object detection. This work brings together methods from DCNNs and probabilistic graphical models for addressing the task of pixel-level classiﬁcation (also called ”semantic image segmentation”). We show that responses at the ﬁnal layer of DCNNs are not sufﬁciently localized for accurate object segmentation. This is due to the very invariance properties that make DCNNs good for high level tasks. We overcome this poor localization property of deep networks by combining the responses at the ﬁnal DCNN layer with a fully connected Conditional Random Field (CRF). Qualitatively, our “DeepLab” system is able to localize segment boundaries at a level of accuracy which is beyond previous methods. Quantitatively, our method sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 71.6\% IOU accuracy in the test set. We show how these results can be obtained efﬁciently: Careful network re-purposing and a novel application of the ’hole’ algorithm from the wavelet community allow dense computation of neural net responses at 8 frames per second on a modern GPU.},
	language = {en},
	urldate = {2021-05-25},
	journal = {arXiv:1412.7062 [cs]},
	author = {Chen, Liang-Chieh and Papandreou, George and Kokkinos, Iasonas and Murphy, Kevin and Yuille, Alan L.},
	month = jun,
	year = {2016},
	note = {arXiv: 1412.7062},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@article{noh_learning_2015,
	title = {Learning {Deconvolution} {Network} for {Semantic} {Segmentation}},
	url = {http://arxiv.org/abs/1505.04366},
	abstract = {We propose a novel semantic segmentation algorithm by learning a deconvolution network. We learn the network on top of the convolutional layers adopted from VGG 16-layer net. The deconvolution network is composed of deconvolution and unpooling layers, which identify pixel-wise class labels and predict segmentation masks. We apply the trained network to each proposal in an input image, and construct the final semantic segmentation map by combining the results from all proposals in a simple manner. The proposed algorithm mitigates the limitations of the existing methods based on fully convolutional networks by integrating deep deconvolution network and proposal-wise prediction; our segmentation method typically identifies detailed structures and handles objects in multiple scales naturally. Our network demonstrates outstanding performance in PASCAL VOC 2012 dataset, and we achieve the best accuracy (72.5\%) among the methods trained with no external data through ensemble with the fully convolutional network.},
	language = {en},
	urldate = {2021-05-25},
	journal = {arXiv:1505.04366 [cs]},
	author = {Noh, Hyeonwoo and Hong, Seunghoon and Han, Bohyung},
	month = may,
	year = {2015},
	note = {arXiv: 1505.04366},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{shelhamer_fully_2017,
	title = {Fully {Convolutional} {Networks} for {Semantic} {Segmentation}},
	volume = {39},
	issn = {0162-8828, 2160-9292},
	url = {http://ieeexplore.ieee.org/document/7478072/},
	doi = {10.1109/TPAMI.2016.2572683},
	abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, improve on the previous best result in semantic segmentation. Our key insight is to build “fully convolutional” networks that take input of arbitrary size and produce correspondingly-sized output with efﬁcient inference and learning. We deﬁne and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classiﬁcation networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by ﬁne-tuning to the segmentation task. We then deﬁne a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, ﬁne layer to produce accurate and detailed segmentations. Our fully convolutional networks achieve improved segmentation of PASCAL VOC (30\% relative improvement to 67.2\% mean IU on 2012), NYUDv2, SIFT Flow, and PASCAL-Context, while inference takes one tenth of a second for a typical image.},
	language = {en},
	number = {4},
	urldate = {2021-05-25},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Shelhamer, Evan and Long, Jonathan and Darrell, Trevor},
	month = apr,
	year = {2017},
	pages = {640--651},
}

@article{sermanet_overfeat_2014,
	title = {{OverFeat}: {Integrated} {Recognition}, {Localization} and {Detection} using {Convolutional} {Networks}},
	shorttitle = {{OverFeat}},
	url = {http://arxiv.org/abs/1312.6229},
	abstract = {We present an integrated framework for using Convolutional Networks for classiﬁcation, localization and detection. We show how a multiscale and sliding window approach can be efﬁciently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object boundaries. Bounding boxes are then accumulated rather than suppressed in order to increase detection conﬁdence. We show that different tasks can be learned simultaneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classiﬁcations tasks. In post-competition work, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat.},
	language = {en},
	urldate = {2021-05-22},
	journal = {arXiv:1312.6229 [cs]},
	author = {Sermanet, Pierre and Eigen, David and Zhang, Xiang and Mathieu, Michael and Fergus, Rob and LeCun, Yann},
	month = feb,
	year = {2014},
	note = {arXiv: 1312.6229},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{guo_deep_2020,
	title = {Deep {Learning} for {3D} {Point} {Clouds}: {A} {Survey}},
	issn = {0162-8828, 2160-9292, 1939-3539},
	shorttitle = {Deep {Learning} for {3D} {Point} {Clouds}},
	url = {https://ieeexplore.ieee.org/document/9127813/},
	doi = {10.1109/TPAMI.2020.3005434},
	abstract = {Point cloud learning has lately attracted increasing attention due to its wide applications in many areas, such as computer vision, autonomous driving, and robotics. As a dominating technique in AI, deep learning has been successfully used to solve various 2D vision problems. However, deep learning on point clouds is still in its infancy due to the unique challenges faced by the processing of point clouds with deep neural networks. Recently, deep learning on point clouds has become even thriving, with numerous methods being proposed to address different problems in this area. To stimulate future research, this paper presents a comprehensive review of recent progress in deep learning methods for point clouds. It covers three major tasks, including 3D shape classiﬁcation, 3D object detection and tracking, and 3D point cloud segmentation. It also presents comparative results on several publicly available datasets, together with insightful observations and inspiring future research directions.},
	language = {en},
	urldate = {2021-05-22},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Guo, Yulan and Wang, Hanyun and Hu, Qingyong and Liu, Hao and Liu, Li and Bennamoun, Mohammed},
	year = {2020},
	pages = {1--1},
}

@article{chen_3d_nodate,
	title = {{3D} {Object} {Proposals} for {Accurate} {Object} {Class} {Detection}},
	abstract = {The goal of this paper is to generate high-quality 3D object proposals in the context of autonomous driving. Our method exploits stereo imagery to place proposals in the form of 3D bounding boxes. We formulate the problem as minimizing an energy function encoding object size priors, ground plane as well as several depth informed features that reason about free space, point cloud densities and distance to the ground. Our experiments show signiﬁcant performance gains over existing RGB and RGB-D object proposal methods on the challenging KITTI benchmark. Combined with convolutional neural net (CNN) scoring, our approach outperforms all existing results on all three KITTI object classes.},
	language = {en},
	author = {Chen, Xiaozhi and Kundu, Kaustav and Zhu, Yukun and Berneshawi, Andrew and Ma, Huimin and Fidler, Sanja and Urtasun, Raquel},
	pages = {9},
}

@article{simonyan_very_2015,
	title = {Very {Deep} {Convolutional} {Networks} for {Large}-{Scale} {Image} {Recognition}},
	url = {http://arxiv.org/abs/1409.1556},
	abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3 × 3) convolution ﬁlters, which shows that a signiﬁcant improvement on the prior-art conﬁgurations can be achieved by pushing the depth to 16–19 weight layers. These ﬁndings were the basis of our ImageNet Challenge 2014 submission, where our team secured the ﬁrst and the second places in the localisation and classiﬁcation tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
	language = {en},
	urldate = {2021-05-22},
	journal = {arXiv:1409.1556 [cs]},
	author = {Simonyan, Karen and Zisserman, Andrew},
	month = apr,
	year = {2015},
	note = {arXiv: 1409.1556},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{he_spatial_2014,
	title = {Spatial {Pyramid} {Pooling} in {Deep} {Convolutional} {Networks} for {Visual} {Recognition}},
	volume = {8691},
	url = {http://arxiv.org/abs/1406.4729},
	doi = {10.1007/978-3-319-10578-9_23},
	abstract = {Existing deep convolutional neural networks (CNNs) require a ﬁxed-size (e.g., 224×224) input image. This requirement is “artiﬁcial” and may reduce the recognition accuracy for the images or sub-images of an arbitrary size/scale. In this work, we equip the networks with another pooling strategy, “spatial pyramid pooling”, to eliminate the above requirement. The new network structure, called SPP-net, can generate a ﬁxed-length representation regardless of image size/scale. Pyramid pooling is also robust to object deformations. With these advantages, SPP-net should in general improve all CNN-based image classiﬁcation methods. On the ImageNet 2012 dataset, we demonstrate that SPP-net boosts the accuracy of a variety of CNN architectures despite their different designs. On the Pascal VOC 2007 and Caltech101 datasets, SPP-net achieves state-of-theart classiﬁcation results using a single full-image representation and no ﬁne-tuning.},
	language = {en},
	urldate = {2021-05-22},
	journal = {arXiv:1406.4729 [cs]},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	year = {2014},
	note = {arXiv: 1406.4729},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {346--361},
}

@article{girshick_rich_2014,
	title = {Rich feature hierarchies for accurate object detection and semantic segmentation},
	url = {http://arxiv.org/abs/1311.2524},
	abstract = {Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30\% relative to the previous best result on VOC 2012—achieving a mAP of 53.3\%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-speciﬁc ﬁne-tuning, yields a signiﬁcant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also compare R-CNN to OverFeat, a recently proposed sliding-window detector based on a similar CNN architecture. We ﬁnd that R-CNN outperforms OverFeat by a large margin on the 200-class ILSVRC2013 detection dataset. Source code for the complete system is available at http://www.cs.berkeley.edu/˜rbg/rcnn.},
	language = {en},
	urldate = {2021-05-22},
	journal = {arXiv:1311.2524 [cs]},
	author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
	month = oct,
	year = {2014},
	note = {arXiv: 1311.2524},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@incollection{fleet_edge_2014,
	address = {Cham},
	title = {Edge {Boxes}: {Locating} {Object} {Proposals} from {Edges}},
	volume = {8693},
	isbn = {978-3-319-10601-4 978-3-319-10602-1},
	shorttitle = {Edge {Boxes}},
	url = {http://link.springer.com/10.1007/978-3-319-10602-1_26},
	abstract = {The use of object proposals is an eﬀective recent approach for increasing the computational eﬃciency of object detection. We propose a novel method for generating object bounding box proposals using edges. Edges provide a sparse yet informative representation of an image. Our main observation is that the number of contours that are wholly contained in a bounding box is indicative of the likelihood of the box containing an object. We propose a simple box objectness score that measures the number of edges that exist in the box minus those that are members of contours that overlap the box’s boundary. Using eﬃcient data structures, millions of candidate boxes can be evaluated in a fraction of a second, returning a ranked set of a few thousand top-scoring proposals. Using standard metrics, we show results that are signiﬁcantly more accurate than the current state-of-the-art while being faster to compute. In particular, given just 1000 proposals we achieve over 96\% object recall at overlap threshold of 0.5 and over 75\% recall at the more challenging overlap of 0.7. Our approach runs in 0.25 seconds and we additionally demonstrate a near real-time variant with only minor loss in accuracy.},
	language = {en},
	urldate = {2021-05-22},
	booktitle = {Computer {Vision} – {ECCV} 2014},
	publisher = {Springer International Publishing},
	author = {Zitnick, C. Lawrence and Dollár, Piotr},
	editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
	year = {2014},
	doi = {10.1007/978-3-319-10602-1_26},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {391--405},
}

@article{uijlings_selective_2013,
	title = {Selective {Search} for {Object} {Recognition}},
	volume = {104},
	issn = {0920-5691, 1573-1405},
	url = {http://link.springer.com/10.1007/s11263-013-0620-5},
	doi = {10.1007/s11263-013-0620-5},
	abstract = {This paper addresses the problem of generating possible object locations for use in object recognition. We introduce Selective Search which combines the strength of both an exhaustive search and segmentation. Like segmentation, we use the image structure to guide our sampling process. Like exhaustive search, we aim to capture all possible object locations. Instead of a single technique to generate possible object locations, we diversify our search and use a variety of complementary image partitionings to deal with as many image conditions as possible. Our Selective Search results in a small set of data-driven, class-independent, high quality locations, yielding 99\% recall and a Mean Average Best Overlap of 0.879 at 10,097 locations. The reduced number of locations compared to an exhaustive search enables the use of stronger machine learning techniques and stronger appearance models for object recognition. In this paper we show that our selective search enables the use of the powerful Bag-of-Words model for recognition. The Selective Search software is made publicly available 1.},
	language = {en},
	number = {2},
	urldate = {2021-05-22},
	journal = {International Journal of Computer Vision},
	author = {Uijlings, J. R. R. and van de Sande, K. E. A. and Gevers, T. and Smeulders, A. W. M.},
	month = sep,
	year = {2013},
	pages = {154--171},
}

@article{zhang_survey_2021,
	title = {A {Survey} {On} {Universal} {Adversarial} {Attack}},
	url = {http://arxiv.org/abs/2103.01498},
	abstract = {Deep neural networks (DNNs) have demonstrated remarkable performance for various applications, meanwhile, they are widely known to be vulnerable to the attack of adversarial perturbations. This intriguing phenomenon has attracted signiﬁcant attention in machine learning and what might be more surprising to the community is the existence of universal adversarial perturbations (UAPs), i.e. a single perturbation to fool the target DNN for most images. The advantage of UAP is that it can be generated beforehand and then be applied on-the-ﬂy during the attack. With the focus on UAP against deep classiﬁers, this survey summarizes the recent progress on universal adversarial attacks, discussing the challenges from both the attack and defense sides, as well as the reason for the existence of UAP. Additionally, universal attacks in a wide range of applications beyond deep classiﬁcation are also covered.},
	language = {en},
	urldate = {2021-05-22},
	journal = {arXiv:2103.01498 [cs]},
	author = {Zhang, Chaoning and Benz, Philipp and Lin, Chenguo and Karjauv, Adil and Wu, Jing and Kweon, In So},
	month = mar,
	year = {2021},
	note = {arXiv: 2103.01498},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{pouyanfar_survey_2019,
	title = {A {Survey} on {Deep} {Learning}: {Algorithms}, {Techniques}, and {Applications}},
	volume = {51},
	issn = {0360-0300, 1557-7341},
	shorttitle = {A {Survey} on {Deep} {Learning}},
	url = {https://dl.acm.org/doi/10.1145/3234150},
	doi = {10.1145/3234150},
	abstract = {The field of machine learning is witnessing its golden era as deep learning slowly becomes the leader in this domain. Deep learning uses multiple layers to represent the abstractions of data to build computational models. Some key enabler deep learning algorithms such as generative adversarial networks, convolutional neural networks, and model transfers have completely changed our perception of information processing. However, there exists an aperture of understanding behind this tremendously fast-paced domain, because it was never previously represented from a multiscope perspective. The lack of core understanding renders these powerful methods as black-box machines that inhibit development at a fundamental level. Moreover, deep learning has repeatedly been perceived as a silver bullet to all stumbling blocks in machine learning, which is far from the truth. This article presents a comprehensive review of historical and recent state-of-the-art approaches in visual, audio, and text processing; social network analysis; and natural language processing, followed by the in-depth analysis on pivoting and groundbreaking advances in deep learning applications. It was also undertaken to review the issues faced in deep learning such as unsupervised learning, black-box models, and online learning and to illustrate how these challenges can be transformed into prolific future research avenues.},
	language = {en},
	number = {5},
	urldate = {2021-05-22},
	journal = {ACM Computing Surveys},
	author = {Pouyanfar, Samira and Sadiq, Saad and Yan, Yilin and Tian, Haiman and Tao, Yudong and Reyes, Maria Presa and Shyu, Mei-Ling and Chen, Shu-Ching and Iyengar, S. S.},
	month = jan,
	year = {2019},
	pages = {1--36},
}

@article{chakraborty_adversarial_2018,
	title = {Adversarial {Attacks} and {Defences}: {A} {Survey}},
	shorttitle = {Adversarial {Attacks} and {Defences}},
	url = {http://arxiv.org/abs/1810.00069},
	abstract = {Deep learning has emerged as a strong and efficient framework that can be applied to a broad spectrum of complex learning problems which were difficult to solve using the traditional machine learning techniques in the past. In the last few years, deep learning has advanced radically in such a way that it can surpass human-level performance on a number of tasks. As a consequence, deep learning is being extensively used in most of the recent day-to-day applications. However, security of deep learning systems are vulnerable to crafted adversarial examples, which may be imperceptible to the human eye, but can lead the model to misclassify the output. In recent times, different types of adversaries based on their threat model leverage these vulnerabilities to compromise a deep learning system where adversaries have high incentives. Hence, it is extremely important to provide robustness to deep learning algorithms against these adversaries. However, there are only a few strong countermeasures which can be used in all types of attack scenarios to design a robust deep learning system. In this paper, we attempt to provide a detailed discussion on different types of adversarial attacks with various threat models and also elaborate the efficiency and challenges of recent countermeasures against them.},
	language = {en},
	urldate = {2021-05-22},
	journal = {arXiv:1810.00069 [cs, stat]},
	author = {Chakraborty, Anirban and Alam, Manaar and Dey, Vishal and Chattopadhyay, Anupam and Mukhopadhyay, Debdeep},
	month = sep,
	year = {2018},
	note = {arXiv: 1810.00069},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{chakraborty_survey_2021,
	title = {A survey on adversarial attacks and defences},
	volume = {6},
	issn = {2468-2322, 2468-2322},
	url = {https://onlinelibrary.wiley.com/doi/10.1049/cit2.12028},
	doi = {10.1049/cit2.12028},
	abstract = {Deep learning has evolved as a strong and efficient framework that can be applied to a broad spectrum of complex learning problems which were difficult to solve using the traditional machine learning techniques in the past. The advancement of deep learning has been so radical that today it can surpass human‐level performance. As a consequence, deep learning is being extensively used in most of the recent day‐to‐day applications. However, efficient deep learning systems can be jeopardised by using crafted adversarial samples, which may be imperceptible to the human eye, but can lead the model to misclassify the output. In recent times, different types of adversaries based on their threat model leverage these vulnerabilities to compromise a deep learning system where adversaries have high incentives. Hence, it is extremely important to provide robustness to deep learning algorithms against these adversaries. However, there are only a few strong countermeasures which can be used in all types of attack scenarios to design a robust deep learning system. Herein, the authors attempt to provide a detailed discussion on different types of adversarial attacks with various threat models and also elaborate on the efficiency and challenges of recent countermeasures against them.},
	language = {en},
	number = {1},
	urldate = {2021-05-22},
	journal = {CAAI Transactions on Intelligence Technology},
	author = {Chakraborty, Anirban and Alam, Manaar and Dey, Vishal and Chattopadhyay, Anupam and Mukhopadhyay, Debdeep},
	month = mar,
	year = {2021},
	pages = {25--45},
}

@article{kamilaris_deep_2018,
	title = {Deep learning in agriculture: {A} survey},
	volume = {147},
	issn = {01681699},
	shorttitle = {Deep learning in agriculture},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0168169917308803},
	doi = {10.1016/j.compag.2018.02.016},
	abstract = {Deep learning constitutes a recent, modern technique for image processing and data analysis, with promising results and large potential. As deep learning has been successfully applied in various domains, it has recently entered also the domain of agriculture. In this paper, we perform a survey of 40 research efforts that employ deep learning techniques, applied to various agricultural and food production challenges. We examine the particular agricultural problems under study, the specific models and frameworks employed, the sources, nature and pre-processing of data used, and the overall performance achieved according to the metrics used at each work under study. Moreover, we study comparisons of deep learning with other existing popular techniques, in respect to differences in classification or regression performance. Our findings indicate that deep learning provides high accuracy, outperforming existing commonly used image processing techniques.},
	language = {en},
	urldate = {2021-05-22},
	journal = {Computers and Electronics in Agriculture},
	author = {Kamilaris, Andreas and Prenafeta-Boldú, Francesc X.},
	month = apr,
	year = {2018},
	pages = {70--90},
}

@inproceedings{deng_imagenet_2009,
	address = {Miami, FL},
	title = {{ImageNet}: {A} large-scale hierarchical image database},
	isbn = {978-1-4244-3992-8},
	shorttitle = {{ImageNet}},
	url = {https://ieeexplore.ieee.org/document/5206848/},
	doi = {10.1109/CVPR.2009.5206848},
	abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a largescale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 5001000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classiﬁcation and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
	language = {en},
	urldate = {2021-05-22},
	booktitle = {2009 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and {Kai Li} and {Li Fei-Fei}},
	month = jun,
	year = {2009},
	pages = {248--255},
}

@inproceedings{sanchez_high-dimensional_2011,
	address = {Colorado Springs, CO, USA},
	title = {High-dimensional signature compression for large-scale image classification},
	isbn = {978-1-4577-0394-2},
	url = {http://ieeexplore.ieee.org/document/5995504/},
	doi = {10.1109/CVPR.2011.5995504},
	abstract = {We address image classiﬁcation on a large-scale, i.e. when a large number of images and classes are involved. First, we study classiﬁcation accuracy as a function of the image signature dimensionality and the training set size. We show experimentally that the larger the training set, the higher the impact of the dimensionality on the accuracy. In other words, high-dimensional signatures are important to obtain state-of-the-art results on large datasets. Second, we tackle the problem of data compression on very large signatures (on the order of 105 dimensions) using two lossy compression strategies: a dimensionality reduction technique known as the hash kernel and an encoding technique based on product quantizers. We explain how the gain in storage can be traded against a loss in accuracy and / or an increase in CPU cost. We report results on two large databases – ImageNet and a dataset of 1M Flickr images – showing that we can reduce the storage of our signatures by a factor 64 to 128 with little loss in accuracy. Integrating the decompression in the classiﬁer learning yields an efﬁcient and scalable training algorithm. On ILSVRC2010 we report a 74.3\% accuracy at top-5, which corresponds to a 2.5\% absolute improvement with respect to the state-of-the-art. On a subset of 10K classes of ImageNet we report a top-1 accuracy of 16.7\%, a relative improvement of 160\% with respect to the state-of-the-art.},
	language = {en},
	urldate = {2021-05-21},
	booktitle = {{CVPR} 2011},
	publisher = {IEEE},
	author = {Sanchez, Jorge and Perronnin, Florent},
	month = jun,
	year = {2011},
	pages = {1665--1672},
}

@article{krizhevsky_imagenet_2017,
	title = {{ImageNet} classification with deep convolutional neural networks},
	volume = {60},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/3065386},
	doi = {10.1145/3065386},
	abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of ﬁve convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a ﬁnal 1000-way softmax. To make training faster, we used non-saturating neurons and a very efﬁcient GPU implementation of the convolution operation. To reduce overﬁtting in the fully-connected layers we employed a recently-developed regularization method called “dropout” that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
	language = {en},
	number = {6},
	urldate = {2021-05-21},
	journal = {Communications of the ACM},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
	month = may,
	year = {2017},
	pages = {84--90},
}

@article{everingham_pascal_2010,
	title = {The {Pascal} {Visual} {Object} {Classes} ({VOC}) {Challenge}},
	volume = {88},
	issn = {0920-5691, 1573-1405},
	url = {http://link.springer.com/10.1007/s11263-009-0275-4},
	doi = {10.1007/s11263-009-0275-4},
	abstract = {The PASCAL Visual Object Classes (VOC) challenge is a benchmark in visual object category recognition and detection, providing the vision and machine learning communities with a standard dataset of images and annotation, and standard evaluation procedures. Organised annually from 2005 to present, the challenge and its associated dataset has become accepted as the benchmark for object detection.},
	language = {en},
	number = {2},
	urldate = {2021-05-21},
	journal = {International Journal of Computer Vision},
	author = {Everingham, Mark and Van Gool, Luc and Williams, Christopher K. I. and Winn, John and Zisserman, Andrew},
	month = jun,
	year = {2010},
	pages = {303--338},
}

@article{madry_towards_2019,
	title = {Towards {Deep} {Learning} {Models} {Resistant} to {Adversarial} {Attacks}},
	url = {http://arxiv.org/abs/1706.06083},
	abstract = {Recent work has demonstrated that deep neural networks are vulnerable to adversarial examples---inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. Code and pre-trained models are available at https://github.com/MadryLab/mnist\_challenge and https://github.com/MadryLab/cifar10\_challenge.},
	language = {en},
	urldate = {2021-05-21},
	journal = {arXiv:1706.06083 [cs, stat]},
	author = {Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
	month = sep,
	year = {2019},
	note = {arXiv: 1706.06083},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{akhtar_threat_2018,
	title = {Threat of {Adversarial} {Attacks} on {Deep} {Learning} in {Computer} {Vision}: {A} {Survey}},
	shorttitle = {Threat of {Adversarial} {Attacks} on {Deep} {Learning} in {Computer} {Vision}},
	url = {http://arxiv.org/abs/1801.00553},
	abstract = {Deep learning is at the heart of the current rise of artiﬁcial intelligence. In the ﬁeld of Computer Vision, it has become the workhorse for applications ranging from self-driving cars to surveillance and security. Whereas deep neural networks have demonstrated phenomenal success (often beyond human capabilities) in solving complex problems, recent studies show that they are vulnerable to adversarial attacks in the form of subtle perturbations to inputs that lead a model to predict incorrect outputs. For images, such perturbations are often too small to be perceptible, yet they completely fool the deep learning models. Adversarial attacks pose a serious threat to the success of deep learning in practice. This fact has recently lead to a large inﬂux of contributions in this direction. This article presents the ﬁrst comprehensive survey on adversarial attacks on deep learning in Computer Vision. We review the works that design adversarial attacks, analyze the existence of such attacks and propose defenses against them. To emphasize that adversarial attacks are possible in practical conditions, we separately review the contributions that evaluate adversarial attacks in the real-world scenarios. Finally, drawing on the reviewed literature, we provide a broader outlook of this research direction.},
	language = {en},
	urldate = {2021-05-21},
	journal = {arXiv:1801.00553 [cs]},
	author = {Akhtar, Naveed and Mian, Ajmal},
	month = feb,
	year = {2018},
	note = {arXiv: 1801.00553},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{pitropov_canadian_2021,
	title = {Canadian {Adverse} {Driving} {Conditions} dataset},
	volume = {40},
	issn = {0278-3649, 1741-3176},
	url = {http://journals.sagepub.com/doi/10.1177/0278364920979368},
	doi = {10.1177/0278364920979368},
	abstract = {The Canadian Adverse Driving Conditions (CADC) dataset was collected with the Autonomoose autonomous vehicle platform, based on a modified Lincoln MKZ. The dataset, collected during winter within the Region of Waterloo, Canada, is the first autonomous driving dataset that focuses on adverse driving conditions specifically. It contains 7,000 frames of annotated data from 8 cameras (Ximea MQ013CG-E2), lidar (VLP-32C), and a GNSS + INS system (Novatel OEM638), collected through a variety of winter weather conditions. The sensors are time synchronized and calibrated with the intrinsic and extrinsic calibrations included in the dataset. Lidar frame annotations that represent ground truth for 3D object detection and tracking have been provided by Scale AI.},
	language = {en},
	number = {4-5},
	urldate = {2021-05-12},
	journal = {The International Journal of Robotics Research},
	author = {Pitropov, Matthew and Garcia, Danson Evan and Rebello, Jason and Smart, Michael and Wang, Carlos and Czarnecki, Krzysztof and Waslander, Steven},
	month = apr,
	year = {2021},
	pages = {681--690},
}

@article{zhu_cylindrical_2020,
	title = {Cylindrical and {Asymmetrical} {3D} {Convolution} {Networks} for {LiDAR} {Segmentation}},
	url = {http://arxiv.org/abs/2011.10033},
	abstract = {State-of-the-art methods for large-scale driving-scene LiDAR segmentation often project the point clouds to 2D space and then process them via 2D convolution. Although this corporation shows the competitiveness in the point cloud, it inevitably alters and abandons the 3D topology and geometric relations. A natural remedy is to utilize the 3D voxelization and 3D convolution network. However, we found that in the outdoor point cloud, the improvement obtained in this way is quite limited. An important reason is the property of the outdoor point cloud, namely sparsity and varying density. Motivated by this investigation, we propose a new framework for the outdoor LiDAR segmentation, where cylindrical partition and asymmetrical 3D convolution networks are designed to explore the 3D geometric pattern while maintaining these inherent properties. Moreover, a point-wise reﬁnement module is introduced to alleviate the interference of lossy voxel-based label encoding. We evaluate the proposed model on two large-scale datasets, i.e., SemanticKITTI and nuScenes. Our method achieves the 1st place in the leaderboard of SemanticKITTI 1 and outperforms existing methods on nuScenes with a noticeable margin, about 4\%. Furthermore, the proposed 3D framework also generalizes well to LiDAR panoptic segmentation and LiDAR 3D detection.},
	language = {en},
	urldate = {2021-05-03},
	journal = {arXiv:2011.10033 [cs]},
	author = {Zhu, Xinge and Zhou, Hui and Wang, Tai and Hong, Fangzhou and Ma, Yuexin and Li, Wei and Li, Hongsheng and Lin, Dahua},
	month = nov,
	year = {2020},
	note = {arXiv: 2011.10033},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{dong_boosting_2018,
	title = {Boosting {Adversarial} {Attacks} with {Momentum}},
	url = {http://arxiv.org/abs/1710.06081},
	abstract = {Deep neural networks are vulnerable to adversarial examples, which poses security concerns on these algorithms due to the potentially severe consequences. Adversarial attacks serve as an important surrogate to evaluate the robustness of deep learning models before they are deployed. However, most of existing adversarial attacks can only fool a black-box model with a low success rate. To address this issue, we propose a broad class of momentum-based iterative algorithms to boost adversarial attacks. By integrating the momentum term into the iterative process for attacks, our methods can stabilize update directions and escape from poor local maxima during the iterations, resulting in more transferable adversarial examples. To further improve the success rates for black-box attacks, we apply momentum iterative algorithms to an ensemble of models, and show that the adversarially trained models with a strong defense ability are also vulnerable to our black-box attacks. We hope that the proposed methods will serve as a benchmark for evaluating the robustness of various deep models and defense methods. With this method, we won the ﬁrst places in NIPS 2017 Non-targeted Adversarial Attack and Targeted Adversarial Attack competitions.},
	language = {en},
	urldate = {2021-05-03},
	journal = {arXiv:1710.06081 [cs, stat]},
	author = {Dong, Yinpeng and Liao, Fangzhou and Pang, Tianyu and Su, Hang and Zhu, Jun and Hu, Xiaolin and Li, Jianguo},
	month = mar,
	year = {2018},
	note = {arXiv: 1710.06081},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{goodfellow_explaining_2015,
	title = {Explaining and {Harnessing} {Adversarial} {Examples}},
	url = {http://arxiv.org/abs/1412.6572},
	abstract = {Several machine learning models, including neural networks, consistently misclassify adversarial examples—inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high conﬁdence. Early attempts at explaining this phenomenon focused on nonlinearity and overﬁtting. We argue instead that the primary cause of neural networks’ vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the ﬁrst explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.},
	language = {en},
	urldate = {2021-05-03},
	journal = {arXiv:1412.6572 [cs, stat]},
	author = {Goodfellow, Ian J. and Shlens, Jonathon and Szegedy, Christian},
	month = mar,
	year = {2015},
	note = {arXiv: 1412.6572},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{moosavi-dezfooli_deepfool_2016,
	title = {{DeepFool}: a simple and accurate method to fool deep neural networks},
	shorttitle = {{DeepFool}},
	url = {http://arxiv.org/abs/1511.04599},
	abstract = {State-of-the-art deep neural networks have achieved impressive results on many image classification tasks. However, these same architectures have been shown to be unstable to small, well sought, perturbations of the images. Despite the importance of this phenomenon, no effective methods have been proposed to accurately compute the robustness of state-of-the-art deep classifiers to such perturbations on large-scale datasets. In this paper, we fill this gap and propose the DeepFool algorithm to efficiently compute perturbations that fool deep networks, and thus reliably quantify the robustness of these classifiers. Extensive experimental results show that our approach outperforms recent methods in the task of computing adversarial perturbations and making classifiers more robust.},
	language = {en},
	urldate = {2021-05-03},
	journal = {arXiv:1511.04599 [cs]},
	author = {Moosavi-Dezfooli, Seyed-Mohsen and Fawzi, Alhussein and Frossard, Pascal},
	month = jul,
	year = {2016},
	note = {arXiv: 1511.04599},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{kurakin_adversarial_2017,
	title = {Adversarial examples in the physical world},
	url = {http://arxiv.org/abs/1607.02533},
	abstract = {Most existing machine learning classiﬁers are highly vulnerable to adversarial examples. An adversarial example is a sample of input data which has been modiﬁed very slightly in a way that is intended to cause a machine learning classiﬁer to misclassify it. In many cases, these modiﬁcations can be so subtle that a human observer does not even notice the modiﬁcation at all, yet the classiﬁer still makes a mistake. Adversarial examples pose security concerns because they could be used to perform an attack on machine learning systems, even if the adversary has no access to the underlying model. Up to now, all previous work has assumed a threat model in which the adversary can feed data directly into the machine learning classiﬁer. This is not always the case for systems operating in the physical world, for example those which are using signals from cameras and other sensors as input. This paper shows that even in such physical world scenarios, machine learning systems are vulnerable to adversarial examples. We demonstrate this by feeding adversarial images obtained from a cell-phone camera to an ImageNet Inception classiﬁer and measuring the classiﬁcation accuracy of the system. We ﬁnd that a large fraction of adversarial examples are classiﬁed incorrectly even when perceived through the camera.},
	language = {en},
	urldate = {2021-05-03},
	journal = {arXiv:1607.02533 [cs, stat]},
	author = {Kurakin, Alexey and Goodfellow, Ian and Bengio, Samy},
	month = feb,
	year = {2017},
	note = {arXiv: 1607.02533},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{carballo_libre_2020,
	title = {{LIBRE}: {The} {Multiple} {3D} {LiDAR} {Dataset}},
	shorttitle = {{LIBRE}},
	url = {http://arxiv.org/abs/2003.06129},
	abstract = {In this work, we present LIBRE: LiDAR Benchmarking and Reference, a ﬁrst-of-its-kind dataset featuring 10 different LiDAR sensors, covering a range of manufacturers, models, and laser conﬁgurations. Data captured independently from each sensor includes three different environments and conﬁgurations: static targets, where objects were placed at known distances and measured from a ﬁxed position within a controlled environment; adverse weather, where static obstacles were measured from a moving vehicle, captured in a weather chamber where LiDARs were exposed to different conditions (fog, rain, strong light); and ﬁnally, dynamic trafﬁc, where dynamic objects were captured from a vehicle driven on public urban roads, multiple times at different times of the day, and including supporting sensors such as cameras, infrared imaging, and odometry devices. LIBRE will contribute to the research community to (1) provide a means for a fair comparison of currently available LiDARs, and (2) facilitate the improvement of existing self-driving vehicles and robotics-related software, in terms of development and tuning of LiDAR-based perception algorithms.},
	language = {en},
	urldate = {2021-05-03},
	journal = {arXiv:2003.06129 [cs]},
	author = {Carballo, Alexander and Lambert, Jacob and Monrroy-Cano, Abraham and Wong, David Robert and Narksri, Patiphon and Kitsukawa, Yuki and Takeuchi, Eijiro and Kato, Shinpei and Takeda, Kazuya},
	month = jun,
	year = {2020},
	note = {arXiv: 2003.06129},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@article{geiger_vision_2013,
	title = {Vision meets robotics: {The} {KITTI} dataset},
	volume = {32},
	issn = {0278-3649, 1741-3176},
	shorttitle = {Vision meets robotics},
	url = {http://journals.sagepub.com/doi/10.1177/0278364913491297},
	doi = {10.1177/0278364913491297},
	abstract = {We present a novel dataset captured from a VW station wagon for use in mobile robotics and autonomous driving research. In total, we recorded 6 hours of trafﬁc scenarios at 10–100 Hz using a variety of sensor modalities such as high-resolution color and grayscale stereo cameras, a Velodyne 3D laser scanner and a high-precision GPS/IMU inertial navigation system. The scenarios are diverse, capturing real-world trafﬁc situations, and range from freeways over rural areas to inner-city scenes with many static and dynamic objects. Our data is calibrated, synchronized and timestamped, and we provide the rectiﬁed and raw image sequences. Our dataset also contains object labels in the form of 3D tracklets, and we provide online benchmarks for stereo, optical ﬂow, object detection and other tasks. This paper describes our recording platform, the data format and the utilities that we provide.},
	language = {en},
	number = {11},
	urldate = {2021-04-18},
	journal = {The International Journal of Robotics Research},
	author = {Geiger, A and Lenz, P and Stiller, C and Urtasun, R},
	month = sep,
	year = {2013},
	pages = {1231--1237},
}

@inproceedings{zhang_towards_2019,
	address = {Seoul, Korea (South)},
	title = {Towards {Adversarially} {Robust} {Object} {Detection}},
	isbn = {978-1-72814-803-8},
	url = {https://ieeexplore.ieee.org/document/9009990/},
	doi = {10.1109/ICCV.2019.00051},
	abstract = {Object detection is an important vision task and has emerged as an indispensable component in many vision system, rendering its robustness as an increasingly important performance factor for practical applications. While object detection models have been demonstrated to be vulnerable against adversarial attacks by many recent works, very few efforts have been devoted to improving their robustness. In this work, we take an initial attempt towards this direction. We ﬁrst revisit and systematically analyze object detectors and many recently developed attacks from the perspective of model robustness. We then present a multi-task learning perspective of object detection and identify an asymmetric role of task losses. We further develop an adversarial training approach which can leverage the multiple sources of attacks for improving the robustness of detection models. Extensive experiments on PASCAL-VOC and MS-COCO veriﬁed the effectiveness of the proposed approach.},
	language = {en},
	urldate = {2021-03-17},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Zhang, Haichao and Wang, Jianyu},
	month = oct,
	year = {2019},
	pages = {421--430},
}

@article{hendrycks_benchmarking_2019,
	title = {Benchmarking {Neural} {Network} {Robustness} to {Common} {Corruptions} and {Perturbations}},
	url = {http://arxiv.org/abs/1903.12261},
	abstract = {In this paper we establish rigorous benchmarks for image classiﬁer robustness. Our ﬁrst benchmark, IMAGENET-C, standardizes and expands the corruption robustness topic, while showing which classiﬁers are preferable in safety-critical applications. Then we propose a new dataset called IMAGENET-P which enables researchers to benchmark a classiﬁer’s robustness to common perturbations. Unlike recent robustness research, this benchmark evaluates performance on common corruptions and perturbations not worst-case adversarial perturbations. We ﬁnd that there are negligible changes in relative corruption robustness from AlexNet classiﬁers to ResNet classiﬁers. Afterward we discover ways to enhance corruption and perturbation robustness. We even ﬁnd that a bypassed adversarial defense provides substantial common perturbation robustness. Together our benchmarks may aid future work toward networks that robustly generalize.},
	language = {en},
	urldate = {2021-03-09},
	journal = {arXiv:1903.12261 [cs, stat]},
	author = {Hendrycks, Dan and Dietterich, Thomas},
	month = mar,
	year = {2019},
	note = {arXiv: 1903.12261},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{michaelis_benchmarking_2020,
	title = {Benchmarking {Robustness} in {Object} {Detection}: {Autonomous} {Driving} when {Winter} is {Coming}},
	shorttitle = {Benchmarking {Robustness} in {Object} {Detection}},
	url = {http://arxiv.org/abs/1907.07484},
	abstract = {The ability to detect objects regardless of image distortions or weather conditions is crucial for real-world applications of deep learning like autonomous driving. We here provide an easy-to-use benchmark to assess how object detection models perform when image quality degrades. The three resulting benchmark datasets, termed Pascal-C, Coco-C and Cityscapes-C, contain a large variety of image corruptions. We show that a range of standard object detection models suffer a severe performance loss on corrupted images (down to 30–60\% of the original performance). However, a simple data augmentation trick—stylizing the training images—leads to a substantial increase in robustness across corruption type, severity and dataset. We envision our comprehensive benchmark to track future progress towards building robust object detection models. Benchmark, code and data are publicly available.},
	language = {en},
	urldate = {2021-03-09},
	journal = {arXiv:1907.07484 [cs, stat]},
	author = {Michaelis, Claudio and Mitzkus, Benjamin and Geirhos, Robert and Rusak, Evgenia and Bringmann, Oliver and Ecker, Alexander S. and Bethge, Matthias and Brendel, Wieland},
	month = mar,
	year = {2020},
	note = {arXiv: 1907.07484},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{ramasinghe_spectral-gans_2020,
	title = {Spectral-{GANs} for {High}-{Resolution} {3D} {Point}-cloud {Generation}},
	url = {http://arxiv.org/abs/1912.01800},
	abstract = {Point-clouds are a popular choice for vision and graphics tasks due to their accurate shape description and direct acquisition from range-scanners. This demands the ability to synthesize and reconstruct high-quality point-clouds. Current deep generative models for 3D data generally work on simpliﬁed representations (e.g., voxelized objects) and cannot deal with the inherent redundancy and irregularity in point-clouds. A few recent efforts on 3D point-cloud generation offer limited resolution and their complexity grows with the increase in output resolution. In this paper, we develop a principled approach to synthesize 3D point-clouds using a spectral-domain Generative Adversarial Network (GAN). Our spectral representation is highly structured and allows us to disentangle various frequency bands such that the learning task is simpliﬁed for a GAN model. As compared to spatial-domain generative approaches, our formulation allows us to generate high-resolution point-clouds with minimal computational overhead. Furthermore, we propose a fully differentiable block to transform from the spectral to the spatial domain and back, thereby allowing us to integrate knowledge from well-established spatial models. We demonstrate that Spectral-GAN performs well for pointcloud generation task. Additionally, it can learn a highly discriminative representation in an unsupervised fashion and can be used to accurately reconstruct 3D objects.},
	language = {en},
	urldate = {2021-02-26},
	journal = {arXiv:1912.01800 [cs]},
	author = {Ramasinghe, Sameera and Khan, Salman and Barnes, Nick and Gould, Stephen},
	month = jul,
	year = {2020},
	note = {arXiv: 1912.01800},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{arshad_progressive_2020,
	title = {A {Progressive} {Conditional} {Generative} {Adversarial} {Network} for {Generating} {Dense} and {Colored} {3D} {Point} {Clouds}},
	url = {http://arxiv.org/abs/2010.05391},
	abstract = {In this paper, we introduce a novel conditional generative adversarial network that creates dense 3D point clouds, with color, for assorted classes of objects in an unsupervised manner. To overcome the difﬁculty of capturing intricate details at high resolutions, we propose a point transformer that progressively grows the network through the use of graph convolutions. The network is composed of a leaf output layer and an initial set of branches. Every training iteration evolves a point vector into a point cloud of increasing resolution. After a ﬁxed number of iterations, the number of branches is increased by replicating the last branch. Experimental results show that our network is capable of learning and mimicking a 3D data distribution, and produces colored point clouds with ﬁne details at multiple resolutions.},
	language = {en},
	urldate = {2021-02-26},
	journal = {arXiv:2010.05391 [cs]},
	author = {Arshad, Mohammad Samiul and Beksi, William J.},
	month = oct,
	year = {2020},
	note = {arXiv: 2010.05391},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{yang_dsm-net_2020,
	title = {{DSM}-{Net}: {Disentangled} {Structured} {Mesh} {Net} for {Controllable} {Generation} of {Fine} {Geometry}},
	shorttitle = {{DSM}-{Net}},
	url = {http://arxiv.org/abs/2008.05440},
	abstract = {3D shape generation is a fundamental operation in computer graphics. While significant progress has been made, especially with recent deep generative models, it remains a challenge to synthesize high-quality geometric shapes with rich detail and complex structure, in a controllable manner. To tackle this, we introduce DSM-Net, a deep neural network that learns a disentangled structured mesh representation for 3D shapes, where two key aspects of shapes, geometry and structure, are encoded in a synergistic manner to ensure plausibility of the generated shapes, while also being disentangled as much as possible. This supports a range of novel shape generation applications with intuitive control, such as interpolation of structure (geometry) while keeping geometry (structure) unchanged. To achieve this, we simultaneously learn structure and geometry through variational autoencoders (VAEs) in a hierarchical manner for both, with bijective mappings at each level. In this manner we effectively encode geometry and structure in separate latent spaces, while ensuring their compatibility: the structure is used to guide the geometry and vice versa. At the leaf level, the part geometry is represented using a conditional part VAE, to encode high-quality geometric details, guided by the structure context as the condition. Our method not only supports controllable generation applications, but also produces high-quality synthesized shapes, outperforming state-of-the-art methods.},
	language = {en},
	urldate = {2021-02-26},
	journal = {arXiv:2008.05440 [cs]},
	author = {Yang, Jie and Mo, Kaichun and Lai, Yu-Kun and Guibas, Leonidas J. and Gao, Lin},
	month = aug,
	year = {2020},
	note = {arXiv: 2008.05440},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
}

@inproceedings{yang_foldingnet_2018,
	address = {Salt Lake City, UT},
	title = {{FoldingNet}: {Point} {Cloud} {Auto}-{Encoder} via {Deep} {Grid} {Deformation}},
	isbn = {978-1-5386-6420-9},
	shorttitle = {{FoldingNet}},
	url = {https://ieeexplore.ieee.org/document/8578127/},
	doi = {10.1109/CVPR.2018.00029},
	language = {en},
	urldate = {2021-02-26},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Yang, Yaoqing and Feng, Chen and Shen, Yiru and Tian, Dong},
	month = jun,
	year = {2018},
	pages = {206--215},
}

@article{qin_pointdan_2019,
	title = {{PointDAN}: {A} {Multi}-{Scale} {3D} {Domain} {Adaption} {Network} for {Point} {Cloud} {Representation}},
	shorttitle = {{PointDAN}},
	url = {http://arxiv.org/abs/1911.02744},
	abstract = {Domain Adaptation (DA) approaches achieved significant improvements in a wide range of machine learning and computer vision tasks (i.e., classification, detection, and segmentation). However, as far as we are aware, there are few methods yet to achieve domain adaptation directly on 3D point cloud data. The unique challenge of point cloud data lies in its abundant spatial geometric information, and the semantics of the whole object is contributed by including regional geometric structures. Specifically, most general-purpose DA methods that struggle for global feature alignment and ignore local geometric information are not suitable for 3D domain alignment. In this paper, we propose a novel 3D Domain Adaptation Network for point cloud data (PointDAN). PointDAN jointly aligns the global and local features in multi-level. For local alignment, we propose Self-Adaptive (SA) node module with an adjusted receptive field to model the discriminative local structures for aligning domains. To represent hierarchically scaled features, node-attention module is further introduced to weight the relationship of SA nodes across objects and domains. For global alignment, an adversarial-training strategy is employed to learn and align global features across domains. Since there is no common evaluation benchmark for 3D point cloud DA scenario, we build a general benchmark (i.e., PointDA-10) extracted from three popular 3D object/scene datasets (i.e., ModelNet, ShapeNet and ScanNet) for cross-domain 3D objects classification fashion. Extensive experiments on PointDA-10 illustrate the superiority of our model over the state-of-the-art general-purpose DA methods.},
	language = {en},
	urldate = {2021-02-26},
	journal = {arXiv:1911.02744 [cs]},
	author = {Qin, Can and You, Haoxuan and Wang, Lichen and Kuo, C.-C. Jay and Fu, Yun},
	month = nov,
	year = {2019},
	note = {arXiv: 1911.02744},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@incollection{rodrigues_potential_2019,
	address = {Cham},
	title = {On the {Potential} and {Challenges} of {Neural} {Style} {Transfer} for {Three}-{Dimensional} {Shape} {Data}},
	isbn = {978-3-319-97772-0 978-3-319-97773-7},
	url = {http://link.springer.com/10.1007/978-3-319-97773-7_52},
	abstract = {In the ﬁeld of two-dimensional image and video processing, convolutional neural networks have been successfully applied to generate novel images by composing content and style of two different sources, a process called artistic or neural style transfer. However a usage of these methods for threedimensional objects is not straightforward due to the unstructured mesh representations of typical shape data. Hence efﬁcient geometry representations are required to use neural network based style transfer concepts for threedimensional shapes and to enable the fast creation of style options for instance in a product ideation process. In this paper an overview of current stateof-the-art shape representations is presented with respect to their applicability of neural style transfer on three-dimensional shape data. Combinations of threedimensional geometric representations with deep neural network architectures are evaluated towards their capability to store and reproduce content and style information based on previously proposed reconstruction tests.},
	language = {en},
	urldate = {2021-02-26},
	booktitle = {{EngOpt} 2018 {Proceedings} of the 6th {International} {Conference} on {Engineering} {Optimization}},
	publisher = {Springer International Publishing},
	author = {Friedrich, Timo and Aulig, Nikola and Menzel, Stefan},
	editor = {Rodrigues, H.C. and Herskovits, J. and Mota Soares, C.M. and Araújo, A.L. and Guedes, J.M. and Folgado, J.O. and Moleiro, F. and Madeira, J. F. A.},
	year = {2019},
	doi = {10.1007/978-3-319-97773-7_52},
	pages = {581--592},
}

@article{cao_neural_2019,
	title = {Neural {Style} {Transfer} for {Point} {Clouds}},
	url = {http://arxiv.org/abs/1903.05807},
	abstract = {How can we edit or transform the geometric or color property of a point cloud? In this study, we propose a neural style transfer method for point clouds which allows us to transfer the style of geometry or color from one point cloud either independently or simultaneously to another. This transfer is achieved by manipulating the content representations and Gram-based style representations extracted from a pre-trained PointNet-based classiﬁcation network for colored point clouds. As Gram-based style representation is invariant to the number or the order of points, the same method can be extended to transfer the style extracted from an image to the color expression of a point cloud by merely treating the image as a set of pixels. Experimental results demonstrate the capability of the proposed method for transferring style from either an image or a point cloud to another point cloud of a single object or even an indoor scene.},
	language = {en},
	urldate = {2021-02-26},
	journal = {arXiv:1903.05807 [cs]},
	author = {Cao, Xu and Wang, Weimin and Nagao, Katashi},
	month = mar,
	year = {2019},
	note = {arXiv: 1903.05807},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{cao_psnet_2020,
	address = {Snowmass Village, CO, USA},
	title = {{PSNet}: {A} {Style} {Transfer} {Network} for {Point} {Cloud} {Stylization} on {Geometry} and {Color}},
	isbn = {978-1-72816-553-0},
	shorttitle = {{PSNet}},
	url = {https://ieeexplore.ieee.org/document/9093513/},
	doi = {10.1109/WACV45572.2020.9093513},
	abstract = {We propose a neural style transfer method for colored point clouds which allows stylizing the geometry and/or color property of a point cloud from another. The stylization is achieved by manipulating the content representations and Gram-based style representations extracted from a pretrained PointNet-based classiﬁcation network for colored point clouds. As Gram-based style representation is invariant to the number or the order of points, the style can also be an image in the case of stylizing the color property of a point cloud by merely treating the image as a set of pixels. Experimental results and analysis demonstrate the capability of the proposed method for stylizing a point cloud either from another point cloud or an image.},
	language = {en},
	urldate = {2021-02-26},
	booktitle = {2020 {IEEE} {Winter} {Conference} on {Applications} of {Computer} {Vision} ({WACV})},
	publisher = {IEEE},
	author = {Cao, Xu and Wang, Weimin and Nagao, Katashi and Nakamura, Ryosuke},
	month = mar,
	year = {2020},
	pages = {3326--3334},
}

@article{fang_augmented_2019,
	title = {Augmented {LiDAR} {Simulator} for {Autonomous} {Driving}},
	url = {http://arxiv.org/abs/1811.07112},
	abstract = {In Autonomous Driving (AD), detection and tracking of obstacles on the roads is a critical task. Deep-learning based methods using annotated LiDAR data have been the most widely adopted approach for this. Unfortunately, annotating 3D point cloud is a very challenging, time- and money-consuming task. In this paper, we propose a novel LiDAR simulator that augments real point cloud with synthetic obstacles (e.g., cars, pedestrians, and other movable objects). Unlike previous simulators that entirely rely on CG models and game engines, our augmented simulator bypasses the requirement to create high-ﬁdelity background CAD models. Instead, we can simply deploy a vehicle with a LiDAR scanner to sweep the street of interests to obtain the background point cloud, based on which annotated point cloud can be automatically generated. This unique ”scanand-simulate” capability makes our approach scalable and practical, ready for large-scale industrial applications. In this paper, we describe our simulator in detail, in particular the placement of obstacles that is critical for performance enhancement. We show that detectors with our simulated LiDAR point cloud alone can perform comparably (within two percentage points) with these trained with real data. Mixing real and simulated data can achieve over 95\% accuracy.},
	language = {en},
	urldate = {2021-02-26},
	journal = {arXiv:1811.07112 [cs]},
	author = {Fang, Jin and Zhou, Dingfu and Yan, Feilong and Zhao, Tongtong and Zhang, Feihu and Ma, Yu and Wang, Liang and Yang, Ruigang},
	month = apr,
	year = {2019},
	note = {arXiv: 1811.07112},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{bhattacharjee_dunit_2020,
	address = {Seattle, WA, USA},
	title = {{DUNIT}: {Detection}-{Based} {Unsupervised} {Image}-to-{Image} {Translation}},
	isbn = {978-1-72817-168-5},
	shorttitle = {{DUNIT}},
	url = {https://ieeexplore.ieee.org/document/9157516/},
	doi = {10.1109/CVPR42600.2020.00484},
	abstract = {Image-to-image translation has made great strides in recent years, with current techniques being able to handle unpaired training images and to account for the multimodality of the translation problem. Despite this, most methods treat the image as a whole, which makes the results they produce for content-rich scenes less realistic. In this paper, we introduce a Detection-based Unsupervised Image-to-image Translation (DUNIT) approach that explicitly accounts for the object instances in the translation process. To this end, we extract separate representations for the global image and for the instances, which we then fuse into a common representation from which we generate the translated image. This allows us to preserve the detailed content of object instances, while still modeling the fact that we aim to produce an image of a single consistent scene. We introduce an instance consistency loss to maintain the coherence between the detections. Furthermore, by incorporating a detector into our architecture, we can still exploit object instances at test time. As evidenced by our experiments, this allows us to outperform the state-of-the-art unsupervised image-to-image translation methods. Furthermore, our approach can also be used as an unsupervised domain adaptation strategy for object detection, and it also achieves state-of-the-art performance on this task.},
	language = {en},
	urldate = {2021-02-26},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Bhattacharjee, Deblina and Kim, Seungryong and Vizier, Guillaume and Salzmann, Mathieu},
	month = jun,
	year = {2020},
	pages = {4786--4795},
}

@inproceedings{saleh_domain_2019,
	address = {Seoul, Korea (South)},
	title = {Domain {Adaptation} for {Vehicle} {Detection} from {Bird}'s {Eye} {View} {LiDAR} {Point} {Cloud} {Data}},
	isbn = {978-1-72815-023-9},
	url = {https://ieeexplore.ieee.org/document/9022327/},
	doi = {10.1109/ICCVW.2019.00404},
	abstract = {Point cloud data from 3D LiDAR sensors are one of the most crucial sensor modalities for versatile safety-critical applications such as self-driving vehicles. Since the annotations of point cloud data is an expensive and timeconsuming process, therefore recently the utilisation of simulated environments and 3D LiDAR sensors for this task started to get some popularity. However, the generated synthetic point cloud data are still missing the artefacts usually exist in point cloud data from real 3D LiDAR sensors. Thus, in this work, we are proposing a domain adaptation framework for bridging this gap between synthetic and real point cloud data. Our proposed framework is based on the deep cycle-consistent generative adversarial networks (CycleGAN) architecture. We have evaluated the performance of our proposed framework on the task of vehicle detection from a bird’s eye view (BEV) point cloud images coming from real 3D LiDAR sensors. The framework has shown competitive results with an improvement of more than 7\% in average precision score over other baseline approaches when tested on real BEV point cloud images.},
	language = {en},
	urldate = {2021-02-26},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} {Workshop} ({ICCVW})},
	publisher = {IEEE},
	author = {Saleh, Khaled and Abobakr, Ahmed and Attia, Mohammed and Iskander, Julie and Nahavandi, Darius and Hossny, Mohammed and Nahvandi, Saeid},
	month = oct,
	year = {2019},
	pages = {3235--3242},
}

@article{zhou_sparse-gan_2020,
	title = {Sparse-{GAN}: {Sparsity}-constrained {Generative} {Adversarial} {Network} for {Anomaly} {Detection} in {Retinal} {OCT} {Image}},
	shorttitle = {Sparse-{GAN}},
	url = {http://arxiv.org/abs/1911.12527},
	abstract = {With the development of convolutional neural network, deep learning has shown its success for retinal disease detection from optical coherence tomography (OCT) images. However, deep learning often relies on large scale labelled data for training, which is oftentimes challenging especially for disease with low occurrence. Moreover, a deep learning system trained from data-set with one or a few diseases is unable to detect other unseen diseases, which limits the practical usage of the system in disease screening. To address the limitation, we propose a novel anomaly detection framework termed Sparsity-constrained Generative Adversarial Network (Sparse-GAN) for disease screening where only healthy data are available in the training set. The contributions of SparseGAN are two-folds: 1) The proposed Sparse-GAN predicts the anomalies in latent space rather than image-level; 2) Sparse-GAN is constrained by a novel Sparsity Regularization Net. Furthermore, in light of the role of lesions for disease screening, we present to leverage on an anomaly activation map to show the heatmap of lesions. We evaluate our proposed Sparse-GAN on a publicly available dataset, and the results show that the proposed method outperforms the state-of-the-art methods.},
	language = {en},
	urldate = {2021-02-26},
	journal = {arXiv:1911.12527 [physics]},
	author = {Zhou, Kang and Gao, Shenghua and Cheng, Jun and Gu, Zaiwang and Fu, Huazhu and Tu, Zhi and Yang, Jianlong and Zhao, Yitian and Liu, Jiang},
	month = feb,
	year = {2020},
	note = {arXiv: 1911.12527},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing, Physics - Optics},
}

@inproceedings{mahdizadehaghdam_sparse_2019,
	address = {Seoul, Korea (South)},
	title = {Sparse {Generative} {Adversarial} {Network}},
	isbn = {978-1-72815-023-9},
	url = {https://ieeexplore.ieee.org/document/9022395/},
	doi = {10.1109/ICCVW.2019.00369},
	abstract = {We propose a new approach to Generative Adversarial Networks (GANs) to achieve an improved performance with additional robustness to its so-called and well recognized mode collapse. We ﬁrst proceed by mapping the desired data onto a frame-based space for a sparse representation to lift any limitation of small support features prior to learning the structure. To that end we start by dividing an image into multiple patches and modifying the role of the generative network from producing an entire image, at once, to creating a sparse representation vector for each image patch. We synthesize an entire image by multiplying generated sparse representations to a pre-trained dictionary and assembling the resulting patches. This approach restricts the output of the generator to a particular structure, obtained by imposing a Union of Subspaces (UoS) model to the original training data, leading to more realistic images, while maintaining a desired diversity. To further regularize GANs in generating high-quality images and to avoid the notorious mode-collapse problem, we introduce a third player in GANs, called reconstructor. This player utilizes an auto-encoding scheme to ensure that ﬁrst, the input-output relation in the generator is injective and second each real image corresponds to some input noise. We present a number of experiments, where the proposed algorithm shows a remarkably higher inception score compared to the equivalent conventional GANs.},
	language = {en},
	urldate = {2021-02-26},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} {Workshop} ({ICCVW})},
	publisher = {IEEE},
	author = {Mahdizadehaghdam, Shahin and Panahi, Ashkan and Krim, Hamid},
	month = oct,
	year = {2019},
	pages = {3063--3071},
}

@article{zhang_sparsely_2020,
	title = {Sparsely {Grouped} {Multi}-task {Generative} {Adversarial} {Networks} for {Facial} {Attribute} {Manipulation}},
	url = {http://arxiv.org/abs/1805.07509},
	abstract = {Recent Image-to-Image Translation algorithms have achieved signiﬁcant progress in neural style transfer and image attribute manipulation tasks. However, existing approaches require exhaustively labelling training data, which is labor demanding, difﬁcult to scale up, and hard to migrate into new domains. To overcome such a key limitation, we propose Sparsely Grouped Generative Adversarial Networks (SG-GAN) as a novel approach that can translate images on sparsely grouped datasets where only a few samples for training are labelled. Using a novel oneinput multi-output architecture, SG-GAN is well-suited for tackling sparsely grouped learning and multi-task learning. The proposed model can translate images among multiple groups using only a single commonly trained model. To experimentally validate advantages of the new model, we apply the proposed method to tackle a series of attribute manipulation tasks for facial images. Experimental results demonstrate that SG-GAN can generate image translation results of comparable quality with baselines methods on adequately labelled datasets and results of superior quality on sparsely grouped datasets. The ofﬁcial implementation is publicly available 1.},
	language = {en},
	urldate = {2021-02-26},
	journal = {arXiv:1805.07509 [cs]},
	author = {Zhang, Jichao and Shu, Yezhi and Xu, Songhua and Cao, Gongze and Zhong, Fan and Liu, Meng and Qin, Xueying},
	month = may,
	year = {2020},
	note = {arXiv: 1805.07509},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{wang_rethinking_2020,
	title = {Rethinking {Sampling} in {3D} {Point} {Cloud} {Generative} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/2006.07029},
	abstract = {In this paper, we examine the long-neglected yet important effects of point sampling patterns in point cloud GANs. Through extensive experiments, we show that sampling-insensitive discriminators (e.g. PointNet-Max) produce shape point clouds with point clustering artifacts while sampling-oversensitive discriminators (e.g. PointNet++, DGCNN) fail to guide valid shape generation. We propose the concept of sampling spectrum to depict the different sampling sensitivities of discriminators. We further study how different evaluation metrics weigh the sampling pattern against the geometry and propose several perceptual metrics forming a sampling spectrum of metrics. Guided by the proposed sampling spectrum, we discover a middle-point sampling-aware baseline discriminator, PointNet-Mix, which improves all existing point cloud generators by a large margin on samplingrelated metrics. We point out that, though recent research has been focused on the generator design, the main bottleneck of point cloud GAN actually lies in the discriminator design. Our work provides both suggestions and tools for building future discriminators. We will release the code to facilitate future research.},
	language = {en},
	urldate = {2021-02-26},
	journal = {arXiv:2006.07029 [cs, eess]},
	author = {Wang, He and Jiang, Zetian and Yi, Li and Mo, Kaichun and Su, Hao and Guibas, Leonidas J.},
	month = jun,
	year = {2020},
	note = {arXiv: 2006.07029},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing},
}

@article{wu_learning_nodate,
	title = {Learning a {Probabilistic} {Latent} {Space} of {Object} {Shapes} via {3D} {Generative}-{Adversarial} {Modeling}},
	language = {en},
	author = {Wu, Jiajun and Zhang, Chengkai and Xue, Tianfan and Freeman, William T and Tenenbaum, Joshua B},
	pages = {1},
}

@inproceedings{shu_3d_2019,
	address = {Seoul, Korea (South)},
	title = {{3D} {Point} {Cloud} {Generative} {Adversarial} {Network} {Based} on {Tree} {Structured} {Graph} {Convolutions}},
	isbn = {978-1-72814-803-8},
	url = {https://ieeexplore.ieee.org/document/9009495/},
	doi = {10.1109/ICCV.2019.00396},
	language = {en},
	urldate = {2021-02-26},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Shu, Dongwook and Park, Sung Woo and Kwon, Junseok},
	month = oct,
	year = {2019},
	pages = {3858--3867},
}

@article{yi_large-scale_2017,
	title = {Large-{Scale} {3D} {Shape} {Reconstruction} and {Segmentation} from {ShapeNet} {Core55}},
	url = {http://arxiv.org/abs/1710.06104},
	abstract = {We introduce a large-scale 3D shape understanding benchmark using data and annotation from ShapeNet 3D object database. The benchmark consists of two tasks: part-level segmentation of 3D shapes and 3D reconstruction from single view images. Ten teams have participated in the challenge and the best performing teams have outperformed state-of-the-art approaches on both tasks. A few novel deep learning architectures have been proposed on various 3D representations on both tasks. We report the techniques used by each team and the corresponding performances. In addition, we summarize the major discoveries from the reported results and possible trends for the future work in the ﬁeld.},
	language = {en},
	urldate = {2021-02-26},
	journal = {arXiv:1710.06104 [cs]},
	author = {Yi, Li and Shao, Lin and Savva, Manolis and Huang, Haibin and Zhou, Yang and Wang, Qirui and Graham, Benjamin and Engelcke, Martin and Klokov, Roman and Lempitsky, Victor and Gan, Yuan and Wang, Pengyu and Liu, Kun and Yu, Fenggen and Shui, Panpan and Hu, Bingyang and Zhang, Yan and Li, Yangyan and Bu, Rui and Sun, Mingchao and Wu, Wei and Jeong, Minki and Choi, Jaehoon and Kim, Changick and Geetchandra, Angom and Murthy, Narasimha and Ramu, Bhargava and Manda, Bharadwaj and Ramanathan, M. and Kumar, Gautam and Preetham, P. and Srivastava, Siddharth and Bhugra, Swati and Lall, Brejesh and Haene, Christian and Tulsiani, Shubham and Malik, Jitendra and Lafer, Jared and Jones, Ramsey and Li, Siyuan and Lu, Jie and Jin, Shi and Yu, Jingyi and Huang, Qixing and Kalogerakis, Evangelos and Savarese, Silvio and Hanrahan, Pat and Funkhouser, Thomas and Su, Hao and Guibas, Leonidas},
	month = oct,
	year = {2017},
	note = {arXiv: 1710.06104},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{you_survey_2020,
	address = {Kota Kinabalu, Malaysia},
	title = {A {Survey} on {Surface} {Reconstruction} {Techniques} for {Structured} and {Unstructured} {Data}},
	isbn = {978-1-72819-020-4},
	url = {https://ieeexplore.ieee.org/document/9293685/},
	doi = {10.1109/ICOS50156.2020.9293685},
	abstract = {Surface reconstruction of real-world objects is a commonly discussed topic in reverse engineering. Generally, 3D scanning technologies are used to scan the objects through multiple angles and represent them using point cloud. The point cloud can be either in structured or unstructured form which may contain problems such as noise, outliers and incomplete points. The point cloud is considered as unstructured form when it does not contain any connectivity information between adjacent points and structure information. Various types of surface reconstruction techniques are proposed to overcome the problems of point cloud and the limitations of existing techniques. Besides, soft computing techniques are also employed to enhance the performance and overcome the downsides of existing techniques. Therefore, the objective of this paper is to conduct a survey towards the existing techniques in the surface reconstruction on structured or unstructured data. Generally, this paper will only focus on the interpolation and approximation techniques, learning-based techniques, and soft computing techniques. Based on the analysis, it shows that learning-based techniques performed better compared to other techniques as they are able to handle the problem of unstructured point clouds. It can also form as hybrid techniques by integrating with other techniques which can improve its accuracy. The outcome of this paper can be used to assist the researchers in understanding and finding suitable surface reconstruction techniques in representing the objects and solving their case studies.},
	language = {en},
	urldate = {2021-02-26},
	booktitle = {2020 {IEEE} {Conference} on {Open} {Systems} ({ICOS})},
	publisher = {IEEE},
	author = {You, Cheng Chun and Lim, Seng Poh and Lim, Seng Chee and Tan, Joi San and Lee, Chen Kang and Khaw, Yen Min Jasmina},
	month = nov,
	year = {2020},
	pages = {37--42},
}

@article{yi_complete_2020,
	title = {Complete \& {Label}: {A} {Domain} {Adaptation} {Approach} to {Semantic} {Segmentation} of {LiDAR} {Point} {Clouds}},
	shorttitle = {Complete \& {Label}},
	url = {http://arxiv.org/abs/2007.08488},
	abstract = {We study an unsupervised domain adaptation problem for the semantic labeling of 3D point clouds, with a particular focus on domain discrepancies induced by different LiDAR sensors. Based on the observation that sparse 3D point clouds are sampled from 3D surfaces, we take a Complete and Label approach to recover the underlying surfaces before passing them to a segmentation network. Speciﬁcally, we design a Sparse Voxel Completion Network (SVCN) to complete the 3D surfaces of a sparse point cloud. Unlike semantic labels, to obtain training pairs for SVCN requires no manual labeling. We also introduce local adversarial learning to model the surface prior. The recovered 3D surfaces serve as a canonical domain, from which semantic labels can transfer across different LiDAR sensors. Experiments and ablation studies with our new benchmark for cross-domain semantic labeling of LiDAR data show that the proposed approach provides 8.2-36.6\% better performance than previous domain adaptation methods.},
	language = {en},
	urldate = {2021-02-26},
	journal = {arXiv:2007.08488 [cs]},
	author = {Yi, Li and Gong, Boqing and Funkhouser, Thomas},
	month = jul,
	year = {2020},
	note = {arXiv: 2007.08488},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{wen_geometry-aware_2020,
	title = {Geometry-{Aware} {Generation} of {Adversarial} {Point} {Clouds}},
	issn = {0162-8828, 2160-9292, 1939-3539},
	url = {https://ieeexplore.ieee.org/document/9294112/},
	doi = {10.1109/TPAMI.2020.3044712},
	abstract = {Machine learning models have been shown to be vulnerable to adversarial examples. While most of the existing methods for adversarial attack and defense work on the 2D image domain, a few recent attempts have been made to extend them to 3D point cloud data. However, adversarial results obtained by these methods typically contain point outliers, which are both noticeable and easy to defend against using the simple techniques of outlier removal. Motivated by the different mechanisms by which humans perceive 2D images and 3D shapes, in this paper we propose the new design of geometry-aware objectives, whose solutions favor (the discrete versions of) the desired surface properties of smoothness and fairness. To generate adversarial point clouds, we use a targeted attack misclassiﬁcation loss that supports continuous pursuit of increasingly malicious signals. Regularizing the targeted attack loss with our proposed geometry-aware objectives results in our proposed method, Geometry-Aware Adversarial Attack (GeoA3). The results of GeoA3 tend to be more harmful, arguably harder to defend against, and of the key adversarial characterization of being imperceptible to humans. While the main focus of this paper is to learn to generate adversarial point clouds, we also present a simple but effective algorithm termed Geo+A3-IterNormPro, with Iterative Normal Projection (IterNorPro) that solves a new objective function Geo+A3, towards surface-level adversarial attacks via generation of adversarial point clouds. We quantitatively evaluate our methods on both synthetic and physical objects in terms of attack success rate and geometric regularity. For a qualitative evaluation, we conduct subjective studies by collecting human preferences from Amazon Mechanical Turk. Comparative results in comprehensive experiments conﬁrm the advantages of our proposed methods. Our source codes are publicly available at https://github.com/Yuxin-Wen/GeoA3.},
	language = {en},
	urldate = {2021-02-26},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Wen, Yuxin and Lin, Jiehong and Chen, Ke and Chen, C. L. Philip and Jia, Kui},
	year = {2020},
	pages = {1--1},
}

@article{suo_lpd-ae_2020,
	title = {{LPD}-{AE}: {Latent} {Space} {Representation} of {Large}-{Scale} {3D} {Point} {Cloud}},
	volume = {8},
	issn = {2169-3536},
	shorttitle = {{LPD}-{AE}},
	url = {https://ieeexplore.ieee.org/document/9107146/},
	doi = {10.1109/ACCESS.2020.2999727},
	abstract = {The effective latent space representation of point cloud provides a foremost and fundamental manner that can be used for challenging tasks, including point cloud based place recognition and reconstruction, especially in large-scale dynamic environments. In this paper, we present a novel deep neural network, LPD-AE(Large-scale Place Description AutoEncoder Network), to obtain meaningful local and contextual features for the generation of latent space from 3D point cloud directly. The encoder network constructs the discriminative global descriptors to realize high accuracy and robust place recognition, which contributed by extracting the local neighbor geometric features and aggregating neighborhood relationships both in feature space and physical space. The decoder network performs hierarchical reconstruction on coarse key points and ultimately produce dense point clouds, which shows that it is capable of reconstructing a full point cloud frame from a single compact but high dimensional descriptor. Our proposed network demonstrates performance that is comparable to the state-of-the-art approaches. With the beneﬁt of the LPD-AE, many computationally complex tasks that rely directly on point clouds can be effortlessly conducted on latent space with lower memory costs, such as relocalization, loop closure detection, and map compression reconstruction. Comprehensive validations on Oxford RobotCar dataset, KITTI dataset, and our freshly collected dataset, which contains multiple trials of repeated routes in different weather and at different times, manifest its potency for real robotic and self-driving implementation. The source code is available at https://github.com/Suoivy/LPD-AE.},
	language = {en},
	urldate = {2021-02-26},
	journal = {IEEE Access},
	author = {Suo, Chuanzhe and Liu, Zhe and Mo, Lingfei and Liu, Yunhui},
	year = {2020},
	pages = {108402--108417},
}

@article{raina_fine_nodate,
	title = {Fine {Feature} {Reconstruction} in {Point} {Clouds} by {Adversarial} {Domain} {Translation}},
	abstract = {Point cloud neighborhoods are unstructured and often lacking in ﬁne details, particularly when the original surface is sparsely sampled. This has motivated the development of methods for reconstructing these ﬁne geometric features before the point cloud is converted into a mesh, usually by some form of upsampling of the point cloud. We present a novel data-driven approach to reconstructing ﬁne details of the underlying surfaces of point clouds at the local neighborhood level, along with normals and locations of edges. This is achieved by an innovative application of recent advances in domain translation using GANs. We “translate” local neighborhoods between two domains: point cloud neighborhoods and triangular mesh neighborhoods. This allows us to obtain some of the beneﬁts of meshes at training time, while still dealing with point clouds at the time of evaluation. By resampling the translated neighborhood, we can obtain a denser point cloud equipped with normals that allows the underlying surface to be easily reconstructed as a mesh. Our reconstructed meshes preserve ﬁne details of the original surface better than the state of the art in point cloud upsampling techniques, even at different input resolutions. In addition, the trained GAN can generalize to operate on low resolution point clouds even without being explicitly trained on low-resolution data. We also give an example demonstrating that the same domain translation approach we use for reconstructing local neighborhood geometry can also be used to estimate a scalar ﬁeld at the newly generated points, thus reducing the need for expensive recomputation of the scalar ﬁeld on the dense point cloud.},
	language = {en},
	author = {Raina, Prashant and Popa, Tiberiu and Mudur, Sudhir},
	pages = {13},
}

@article{lang_geometric_2020,
	title = {Geometric {Adversarial} {Attacks} and {Defenses} on {3D} {Point} {Clouds}},
	url = {http://arxiv.org/abs/2012.05657},
	abstract = {Deep neural networks are prone to adversarial examples that maliciously alter the network’s outcome. Due to the increasing popularity of 3D sensors in safety-critical systems and the vast deployment of deep learning models for 3D point sets, there is a growing interest in adversarial attacks and defenses for such models. So far, the research has focused on the semantic level, namely, deep point cloud classiﬁers. However, point clouds are also widely used in a geometric-related form that includes encoding and reconstructing the geometry.},
	language = {en},
	urldate = {2021-02-26},
	journal = {arXiv:2012.05657 [cs]},
	author = {Lang, Itai and Kotlicki, Uriel and Avidan, Shai},
	month = dec,
	year = {2020},
	note = {arXiv: 2012.05657},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{behley_semantickitti_2019,
	title = {{SemanticKITTI}: {A} {Dataset} for {Semantic} {Scene} {Understanding} of {LiDAR} {Sequences}},
	shorttitle = {{SemanticKITTI}},
	url = {http://arxiv.org/abs/1904.01416},
	abstract = {Semantic scene understanding is important for various applications. In particular, self-driving cars need a fine-grained understanding of the surfaces and objects in their vicinity. Light detection and ranging (LiDAR) provides precise geometric information about the environment and is thus a part of the sensor suites of almost all self-driving cars. Despite the relevance of semantic scene understanding for this application, there is a lack of a large dataset for this task which is based on an automotive LiDAR. In this paper, we introduce a large dataset to propel research on laser-based semantic segmentation. We annotated all sequences of the KITTI Vision Odometry Benchmark and provide dense point-wise annotations for the complete \$360{\textasciicircum}\{o\}\$ field-of-view of the employed automotive LiDAR. We propose three benchmark tasks based on this dataset: (i) semantic segmentation of point clouds using a single scan, (ii) semantic segmentation using multiple past scans, and (iii) semantic scene completion, which requires to anticipate the semantic scene in the future. We provide baseline experiments and show that there is a need for more sophisticated models to efficiently tackle these tasks. Our dataset opens the door for the development of more advanced methods, but also provides plentiful data to investigate new research directions.},
	language = {en},
	urldate = {2021-02-03},
	journal = {arXiv:1904.01416 [cs]},
	author = {Behley, Jens and Garbade, Martin and Milioto, Andres and Quenzel, Jan and Behnke, Sven and Stachniss, Cyrill and Gall, Juergen},
	month = aug,
	year = {2019},
	note = {arXiv: 1904.01416},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@article{biasutti_lu-net_2019,
	title = {{LU}-{Net}: {An} {Efficient} {Network} for {3D} {LiDAR} {Point} {Cloud} {Semantic} {Segmentation} {Based} on {End}-to-{End}-{Learned} {3D} {Features} and {U}-{Net}},
	shorttitle = {{LU}-{Net}},
	url = {http://arxiv.org/abs/1908.11656},
	abstract = {We propose LU-Net—for LiDAR U-Net, a new method for the semantic segmentation of a 3D LiDAR point cloud. Instead of applying some global 3D segmentation method such as PointNet, we propose an end-to-end architecture for LiDAR point cloud semantic segmentation that efﬁciently solves the problem as an image processing problem. We ﬁrst extract high-level 3D features for each point given its 3D neighbors. Then, these features are projected into a 2D multichannel range-image by considering the topology of the sensor. Thanks to these learned features and this projection, we can ﬁnally perform the segmentation using a simple U-Net segmentation network, which performs very well while being very efﬁcient. In this way, we can exploit both the 3D nature of the data and the speciﬁcity of the LiDAR sensor. This approach outperforms the state-of-the-art by a large margin on the KITTI dataset, as our experiments show. Moreover, this approach operates at 24fps on a single GPU. This is above the acquisition rate of common LiDAR sensors which makes it suitable for real-time applications.},
	language = {en},
	urldate = {2021-01-08},
	journal = {arXiv:1908.11656 [cs]},
	author = {Biasutti, Pierre and Lepetit, Vincent and Aujol, Jean-François and Brédif, Mathieu and Bugeau, Aurélie},
	month = aug,
	year = {2019},
	note = {arXiv: 1908.11656},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{nobis_deep_2020,
	title = {A {Deep} {Learning}-based {Radar} and {Camera} {Sensor} {Fusion} {Architecture} for {Object} {Detection}},
	url = {http://arxiv.org/abs/2005.07431},
	abstract = {Object detection in camera images, using deep learning has been proven successfully in recent years. Rising detection rates and computationally efficient network structures are pushing this technique towards application in production vehicles. Nevertheless, the sensor quality of the camera is limited in severe weather conditions and through increased sensor noise in sparsely lit areas and at night. Our approach enhances current 2D object detection networks by fusing camera data and projected sparse radar data in the network layers. The proposed CameraRadarFusionNet (CRF-Net) automatically learns at which level the fusion of the sensor data is most beneficial for the detection result. Additionally, we introduce BlackIn, a training strategy inspired by Dropout, which focuses the learning on a specific sensor type. We show that the fusion network is able to outperform a state-of-the-art image-only network for two different datasets. The code for this research will be made available to the public at: https://github.com/TUMFTM/CameraRadarFusionNet.},
	language = {en},
	urldate = {2020-12-31},
	journal = {arXiv:2005.07431 [cs]},
	author = {Nobis, Felix and Geisslinger, Maximilian and Weber, Markus and Betz, Johannes and Lienkamp, Markus},
	month = may,
	year = {2020},
	note = {arXiv: 2005.07431},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{tian_deep_2020,
	title = {Deep {Learning} on {Image} {Denoising}: {An} overview},
	shorttitle = {Deep {Learning} on {Image} {Denoising}},
	url = {http://arxiv.org/abs/1912.13171},
	abstract = {Deep learning techniques have received much attention in the area of image denoising. However, there are substantial differences in the various types of deep learning methods dealing with image denoising. Speciﬁcally, discriminative learning based on deep learning can ably address the issue of Gaussian noise. Optimization models based on deep learning are effective in estimating the real noise. However, there has thus far been little related research to summarize the different deep learning techniques for image denoising. In this paper, we offer a comparative study of deep techniques in image denoising. We ﬁrst classify the deep convolutional neural networks (CNNs) for additive white noisy images; the deep CNNs for real noisy images; the deep CNNs for blind denoising and the deep CNNs for hybrid noisy images, which represents the combination of noisy, blurred and low-resolution images. Then, we analyze the motivations and principles of the different types of deep learning methods. Next, we compare the state-of-the-art methods on public denoising datasets in terms of quantitative and qualitative analysis. Finally, we point out some potential challenges and directions of future research.},
	language = {en},
	urldate = {2020-12-31},
	journal = {arXiv:1912.13171 [cs, eess]},
	author = {Tian, Chunwei and Fei, Lunke and Zheng, Wenxian and Xu, Yong and Zuo, Wangmeng and Lin, Chia-Wen},
	month = aug,
	year = {2020},
	note = {arXiv: 1912.13171},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
}

@article{mao_multitask_2020,
	title = {Multitask {Learning} {Strengthens} {Adversarial} {Robustness}},
	url = {http://arxiv.org/abs/2007.07236},
	abstract = {Although deep networks achieve strong accuracy on a range of computer vision benchmarks, they remain vulnerable to adversarial attacks, where imperceptible input perturbations fool the network. We present both theoretical and empirical analyses that connect the adversarial robustness of a model to the number of tasks that it is trained on. Experiments on two datasets show that attack diﬃculty increases as the number of target tasks increase. Moreover, our results suggest that when models are trained on multiple tasks at once, they become more robust to adversarial attacks on individual tasks. While adversarial defense remains an open challenge, our results suggest that deep networks are vulnerable partly because they are trained on too few tasks.},
	language = {en},
	urldate = {2020-12-31},
	journal = {arXiv:2007.07236 [cs]},
	author = {Mao, Chengzhi and Gupta, Amogh and Nitin, Vikram and Ray, Baishakhi and Song, Shuran and Yang, Junfeng and Vondrick, Carl},
	month = sep,
	year = {2020},
	note = {arXiv: 2007.07236},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@article{zhang_polarnet_2020,
	title = {{PolarNet}: {An} {Improved} {Grid} {Representation} for {Online} {LiDAR} {Point} {Clouds} {Semantic} {Segmentation}},
	shorttitle = {{PolarNet}},
	url = {http://arxiv.org/abs/2003.14032},
	abstract = {The need for ﬁne-grained perception in autonomous driving systems has resulted in recently increased research on online semantic segmentation of single-scan LiDAR. Despite the emerging datasets and technological advancements, it remains challenging due to three reasons: (1) the need for near-real-time latency with limited hardware; (2) uneven or even long-tailed distribution of LiDAR points across space; and (3) an increasing number of extremely ﬁne-grained semantic classes. In an attempt to jointly tackle all the aforementioned challenges, we propose a new LiDAR-speciﬁc, nearest-neighbor-free segmentation algorithm — PolarNet. Instead of using common spherical or bird’s-eye-view projection, our polar bird’s-eye-view representation balances the points across grid cells in a polar coordinate system, indirectly aligning a segmentation network’s attention with the long-tailed distribution of the points along the radial axis. We ﬁnd that our encoding scheme greatly increases the mIoU in three drastically different segmentation datasets of real urban LiDAR single scans while retaining near real-time throughput.},
	language = {en},
	urldate = {2020-12-31},
	journal = {arXiv:2003.14032 [cs]},
	author = {Zhang, Yang and Zhou, Zixiang and David, Philip and Yue, Xiangyu and Xi, Zerong and Gong, Boqing and Foroosh, Hassan},
	month = apr,
	year = {2020},
	note = {arXiv: 2003.14032},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{huang_multimodal_2018,
	title = {Multimodal {Unsupervised} {Image}-to-{Image} {Translation}},
	url = {http://arxiv.org/abs/1804.04732},
	abstract = {Unsupervised image-to-image translation is an important and challenging problem in computer vision. Given an image in the source domain, the goal is to learn the conditional distribution of corresponding images in the target domain, without seeing any examples of corresponding image pairs. While this conditional distribution is inherently multimodal, existing approaches make an overly simpliﬁed assumption, modeling it as a deterministic one-to-one mapping. As a result, they fail to generate diverse outputs from a given source domain image. To address this limitation, we propose a Multimodal Unsupervised Image-to-image Translation (MUNIT) framework. We assume that the image representation can be decomposed into a content code that is domain-invariant, and a style code that captures domain-speciﬁc properties. To translate an image to another domain, we recombine its content code with a random style code sampled from the style space of the target domain. We analyze the proposed framework and establish several theoretical results. Extensive experiments with comparisons to state-of-the-art approaches further demonstrate the advantage of the proposed framework. Moreover, our framework allows users to control the style of translation outputs by providing an example style image. Code and pretrained models are available at https://github.com/nvlabs/MUNIT.},
	language = {en},
	urldate = {2020-12-31},
	journal = {arXiv:1804.04732 [cs, stat]},
	author = {Huang, Xun and Liu, Ming-Yu and Belongie, Serge and Kautz, Jan},
	month = aug,
	year = {2018},
	note = {arXiv: 1804.04732},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{zhu_unpaired_2020,
	title = {Unpaired {Image}-to-{Image} {Translation} using {Cycle}-{Consistent} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1703.10593},
	abstract = {Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain X to a target domain Y in the absence of paired examples. Our goal is to learn a mapping G : X → Y such that the distribution of images from G(X) is indistinguishable from the distribution Y using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping F : Y → X and introduce a cycle consistency loss to enforce F (G(X)) ≈ X (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transﬁguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.},
	language = {en},
	urldate = {2020-12-31},
	journal = {arXiv:1703.10593 [cs]},
	author = {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A.},
	month = aug,
	year = {2020},
	note = {arXiv: 1703.10593},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{alsaiari_image_2019,
	address = {Kahului, HI, USA},
	title = {Image {Denoising} {Using} {A} {Generative} {Adversarial} {Network}},
	isbn = {978-1-72813-323-2},
	url = {https://ieeexplore.ieee.org/document/8710893/},
	doi = {10.1109/INFOCT.2019.8710893},
	abstract = {Animation studios render 3D scenes using a technique called path tracing which enables them to create high quality photorealistic frames. Path tracing involves shooting 1000’s of rays into a pixel randomly (Monte Carlo) which will then hit the objects in the scene and, based on the reflective property of the object, these rays reflect or refract or get absorbed. The colors returned by these rays are averaged to determine the color of the pixel. This process is repeated for all the pixels. Due to the computational complexity it might take 8-16 hours to render a single frame. We implemented a neural network-based solution to reduce the time it takes to render a frame to less than a second using a generative adversarial network (GAN), once the network is trained. The main idea behind this proposed method is to render the image using a much smaller number of samples per pixel than is normal for path tracing (e.g., 1, 4, or 8 samples instead of, say, 32,000 samples) and then pass the noisy, incompletely rendered image to our network, which is capable of generating a highquality photorealistic image.},
	language = {en},
	urldate = {2020-12-10},
	booktitle = {2019 {IEEE} 2nd {International} {Conference} on {Information} and {Computer} {Technologies} ({ICICT})},
	publisher = {IEEE},
	author = {Alsaiari, Abeer and Rustagi, Ridhi and Alhakamy, A'aeshah and Thomas, Manu Mathew and Forbes, Angus G.},
	month = mar,
	year = {2019},
	pages = {126--132},
}

@inproceedings{heinzler_weather_2019,
	address = {Paris, France},
	title = {Weather {Influence} and {Classification} with {Automotive} {Lidar} {Sensors}},
	isbn = {978-1-72810-560-4},
	url = {https://ieeexplore.ieee.org/document/8814205/},
	doi = {10.1109/IVS.2019.8814205},
	abstract = {Lidar sensors are often used in mobile robots and autonomous vehicles to complement camera, radar and ultrasonic sensors for environment perception. Typically, perception algorithms are trained to only detect moving and static objects as well as ground estimation, but intentionally ignore weather effects to reduce false detections. In this work, we present an in-depth analysis of automotive lidar performance under harsh weather conditions, i.e. heavy rain and dense fog. An extensive data set has been recorded for various fog and rain conditions, which is the basis for the conducted indepth analysis of the point cloud under changing environmental conditions. In addition, we introduce a novel approach to detect and classify rain or fog with lidar sensors only and achieve an mean union over intersection of 97.14 \% for a data set in controlled environments. The analysis of weather inﬂuences on the performance of lidar sensors and the weather detection are important steps towards improving safety levels for autonomous driving in adverse weather conditions by providing reliable information to adapt vehicle behavior.},
	language = {en},
	urldate = {2020-11-30},
	booktitle = {2019 {IEEE} {Intelligent} {Vehicles} {Symposium} ({IV})},
	publisher = {IEEE},
	author = {Heinzler, Robin and Schindler, Philipp and Seekircher, Jurgen and Ritter, Werner and Stork, Wilhelm},
	month = jun,
	year = {2019},
	pages = {1527--1534},
}

@article{piewak_boosting_2018,
	title = {Boosting {LiDAR}-based {Semantic} {Labeling} by {Cross}-{Modal} {Training} {Data} {Generation}},
	url = {http://arxiv.org/abs/1804.09915},
	abstract = {Mobile robots and autonomous vehicles rely on multi-modal sensor setups to perceive and understand their surroundings. Aside from cameras, LiDAR sensors represent a central component of state-of-theart perception systems. In addition to accurate spatial perception, a comprehensive semantic understanding of the environment is essential for eﬃcient and safe operation. In this paper we present a novel deep neural network architecture called LiLaNet for point-wise, multi-class semantic labeling of semi-dense LiDAR data. The network utilizes virtual image projections of the 3D point clouds for eﬃcient inference. Further, we propose an automated process for large-scale cross-modal training data generation called Autolabeling, in order to boost semantic labeling performance while keeping the manual annotation eﬀort low. The eﬀectiveness of the proposed network architecture as well as the automated data generation process is demonstrated on a manually annotated ground truth dataset. LiLaNet is shown to signiﬁcantly outperform current state-ofthe-art CNN architectures for LiDAR data. Applying our automatically generated large-scale training data yields a boost of up to 14 percentage points compared to networks trained on manually annotated data only.},
	language = {en},
	urldate = {2020-11-26},
	journal = {arXiv:1804.09915 [cs]},
	author = {Piewak, Florian and Pinggera, Peter and Schäfer, Manuel and Peter, David and Schwarz, Beate and Schneider, Nick and Pfeiffer, David and Enzweiler, Markus and Zöllner, Marius},
	month = apr,
	year = {2018},
	note = {arXiv: 1804.09915},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{charron_-noising_nodate,
	title = {De-noising of {Lidar} {Point} {Clouds} {Corrupted} by {Snowfall}},
	abstract = {A common problem in autonomous driving is designing a system that can operate in adverse weather conditions. Falling rain and snow tends to corrupt sensor measurements, particularly for lidar sensors. Surprisingly, very little research has been published on methods to de-noise point clouds which are collected by lidar in rainy or snowy weather conditions. In this paper, we present a method for removing snow noise by processing point clouds using a 3D outlier detection algorithm. Our method, the dynamic radius outlier removal ﬁlter, accounts for the variation in point cloud density with increasing distance from the sensor, with the goal of removing the noise caused by snow while retaining detail in environmental features (which is necessary for autonomous localization and navigation).},
	language = {en},
	author = {Charron, Nicholas and Phillips, Stephen and Waslander, Steven L},
	pages = {8},
}

@article{zhao_ntgan_nodate,
	title = {{NTGAN}: {Learning} {Blind} {Image} {Denoising} without {Clean} {Reference}},
	abstract = {Recent studies on learning-based image denoising have achieved promising performance on various noise reduction tasks. Most of these deep denoisers are trained either under the supervision of clean references, or unsupervised on synthetic noise. The assumption with the synthetic noise leads to poor generalization when facing real photographs. To address this issue, we propose a novel deep unsupervised image-denoising method by regarding the noise reduction task as a special case of the noise transference task. Learning noise transference enables the network to acquire the denoising ability by only observing the corrupted samples. The results on real-world denoising benchmarks demonstrate that our proposed method achieves state-of-the-art performance on removing realistic noises, making it a potential solution to practical noise reduction problems.},
	language = {en},
	author = {Zhao, Rui},
	pages = {13},
}

@article{li_pu-gan_nodate,
	title = {{PU}-{GAN}: {A} {Point} {Cloud} {Upsampling} {Adversarial} {Network}},
	abstract = {Point clouds acquired from range scans are often sparse, noisy, and non-uniform. This paper presents a new point cloud upsampling network called PU-GAN 1, which is formulated based on a generative adversarial network (GAN), to learn a rich variety of point distributions from the latent space and upsample points over patches on object surfaces. To realize a working GAN network, we construct an up-down-up expansion unit in the generator for upsampling point features with error feedback and self-correction, and formulate a self-attention unit to enhance the feature integration. Further, we design a compound loss with adversarial, uniform and reconstruction terms, to encourage the discriminator to learn more latent patterns and enhance the output point distribution uniformity. Qualitative and quantitative evaluations demonstrate the quality of our results over the state-of-the-arts in terms of distribution uniformity, proximity-to-surface, and 3D reconstruction quality.},
	language = {en},
	author = {Li, Ruihui and Li, Xianzhi and Fu, Chi-Wing and Cohen-Or, Daniel and Heng, Pheng-Ann},
	pages = {10},
}

@article{vargas_rivero_weather_2020,
	title = {Weather {Classification} {Using} an {Automotive} {LIDAR} {Sensor} {Based} on {Detections} on {Asphalt} and {Atmosphere}},
	volume = {20},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/20/15/4306},
	doi = {10.3390/s20154306},
	abstract = {A semi-/autonomous driving car requires local weather information to identify if it is working inside its operational design domain and adapt itself accordingly. This information can be extracted from changes in the detections of a light detection and ranging (LIDAR) sensor. These changes are caused by modiﬁcations in the volumetric scattering of the atmosphere or surface reﬂection of objects in the ﬁeld of view of the LIDAR. In order to evaluate the use of an automotive LIDAR as a weather sensor, a LIDAR is placed outdoor in a ﬁxed position for a period of 9 months covering all seasons. As target, an asphalt region from a parking lot is chosen. The collected sensor raw data is labeled depending on the occurring weather conditions as: clear, rain, fog and snow, and the presence of sunlight: with or without background radiation. The inﬂuence of diﬀerent weather types and background radiations on the measurement results is analyzed and diﬀerent parameters are chosen in order to maximize the classiﬁcation accuracy. The classiﬁcation is done per frame in order to provide fast update rates while still keeping an F1 score higher than 80\%. Additionally, the ﬁeld of view is divided into two regions: atmosphere and street, where the inﬂuences of diﬀerent weather types are most notable. The resulting classiﬁers can be used separately or together increasing the versatility of the system. A possible way of extending the method for a moving platform and alternatives to virtually simulate the scene are also discussed.},
	language = {en},
	number = {15},
	urldate = {2020-11-14},
	journal = {Sensors},
	author = {Vargas Rivero, Jose Roberto and Gerbich, Thiemo and Teiluf, Valentina and Buschardt, Boris and Chen, Jia},
	month = aug,
	year = {2020},
	pages = {4306},
}

@article{park_fast_2020,
	title = {Fast and {Accurate} {Desnowing} {Algorithm} for {LiDAR} {Point} {Clouds}},
	volume = {8},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9180326/},
	doi = {10.1109/ACCESS.2020.3020266},
	abstract = {LiDAR sensors have the advantage of being able to generate high-resolution imaging quickly during both day and night; however, their performance is severely limited in adverse weather conditions such as snow, rain, and dense fog. Consequently, many researchers are actively working to overcome these limitations by applying sensor fusion with radar and optical cameras to LiDAR. While studies on the denoising of point clouds acquired by LiDAR in adverse weather have been conducted recently, the results are still insufﬁcient for application to autonomous vehicles because of speed and accuracy performance limitations. Therefore, we propose a new intensity-based ﬁlter that differs from the existing distance-based ﬁlter, which limits the speed. The proposed method showed overwhelming performance advantages in terms of both speed and accuracy by removing only snow particles while leaving important environmental features. The intensity criteria for snow removal were derived based on an analysis of the properties of laser light and snow particles.},
	language = {en},
	urldate = {2020-11-14},
	journal = {IEEE Access},
	author = {Park, Ji-Il and Park, Jihyuk and Kim, Kyung-Soo},
	year = {2020},
	pages = {160202--160212},
}

@article{yang_lanoising_nodate,
	title = {{LaNoising}: {A} {Data}-{Driven} {Approach} for 903nm {ToF} {LiDAR} {Performance} {Modeling} under {Fog}},
	abstract = {As a critical sensor for high-level autonomous vehicles, LiDAR’s limitations in adverse weather (e.g. rain, fog, snow, etc.) impede the deployment of self-driving cars in all weather conditions. In this paper, we model the performance of a popular 903nm ToF LiDAR under various fog conditions based on a LiDAR dataset collected in a well-controlled artiﬁcial fog chamber. Speciﬁcally, a two-stage data-driven method, called LaNoising (la for laser), is proposed for generating LiDAR measurements under fog conditions. In the ﬁrst stage, the Gaussian Process Regression (GPR) model is established to predict whether a laser can successfully output a true detection range or not, given certain fog visibility values. If not, then in the second stage, the Mixture Density Network (MDN) is used to provide a probability prediction of the noisy measurement range. The performance of the proposed method has been quantitatively and qualitatively evaluated. Experimental results show that our approach can provide a promising description of 903nm ToF LiDAR performance under fog.},
	language = {en},
	author = {Yang, Tao and Li, You and Ruichek, Yassine and Yan, Zhi},
	pages = {8},
}

@article{heinzler_cnn-based_2020,
	title = {{CNN}-based {Lidar} {Point} {Cloud} {De}-{Noising} in {Adverse} {Weather}},
	volume = {5},
	issn = {2377-3766, 2377-3774},
	url = {http://arxiv.org/abs/1912.03874},
	doi = {10.1109/LRA.2020.2972865},
	abstract = {Lidar sensors are frequently used in environment perception for autonomous vehicles and mobile robotics to complement camera, radar, and ultrasonic sensors. Adverse weather conditions are signiﬁcantly impacting the performance of lidarbased scene understanding by causing undesired measurement points that in turn effect missing detections and false positives. In heavy rain or dense fog, water drops could be misinterpreted as objects in front of the vehicle which brings a mobile robot to a full stop.},
	language = {en},
	number = {2},
	urldate = {2020-11-05},
	journal = {IEEE Robotics and Automation Letters},
	author = {Heinzler, Robin and Piewak, Florian and Schindler, Philipp and Stork, Wilhelm},
	month = apr,
	year = {2020},
	note = {arXiv: 1912.03874},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
	pages = {2514--2521},
}

@inproceedings{chen_image_2018,
	address = {Salt Lake City, UT},
	title = {Image {Blind} {Denoising} with {Generative} {Adversarial} {Network} {Based} {Noise} {Modeling}},
	isbn = {978-1-5386-6420-9},
	url = {https://ieeexplore.ieee.org/document/8578431/},
	doi = {10.1109/CVPR.2018.00333},
	abstract = {In this paper, we consider a typical image blind denoising problem, which is to remove unknown noise from noisy images. As we all know, discriminative learning based methods, such as DnCNN, can achieve state-of-the-art denoising results, but they are not applicable to this problem due to the lack of paired training data. To tackle the barrier, we propose a novel two-step framework. First, a Generative Adversarial Network (GAN) is trained to estimate the noise distribution over the input noisy images and to generate noise samples. Second, the noise patches sampled from the ﬁrst step are utilized to construct a paired training dataset, which is used, in turn, to train a deep Convolutional Neural Network (CNN) for denoising. Extensive experiments have been done to demonstrate the superiority of our approach in image blind denoising.},
	language = {en},
	urldate = {2020-11-05},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Chen, Jingwen and Chen, Jiawei and Chao, Hongyang and Yang, Ming},
	month = jun,
	year = {2018},
	pages = {3155--3164},
}

@article{xiao_generating_2019,
	title = {Generating {Adversarial} {Examples} with {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1801.02610},
	abstract = {Deep neural networks (DNNs) have been found to be vulnerable to adversarial examples resulting from adding small-magnitude perturbations to inputs. Such adversarial examples can mislead DNNs to produce adversary-selected results. Different attack strategies have been proposed to generate adversarial examples, but how to produce them with high perceptual quality and more efficiently requires more research efforts. In this paper, we propose AdvGAN to generate adversarial examples with generative adversarial networks (GANs), which can learn and approximate the distribution of original instances. For AdvGAN, once the generator is trained, it can generate adversarial perturbations efficiently for any instance, so as to potentially accelerate adversarial training as defenses. We apply AdvGAN in both semi-whitebox and black-box attack settings. In semi-whitebox attacks, there is no need to access the original target model after the generator is trained, in contrast to traditional white-box attacks. In black-box attacks, we dynamically train a distilled model for the black-box model and optimize the generator accordingly. Adversarial examples generated by AdvGAN on different target models have high attack success rate under state-of-the-art defenses compared to other attacks. Our attack has placed the first with 92.76\% accuracy on a public MNIST black-box attack challenge.},
	language = {en},
	urldate = {2020-11-05},
	journal = {arXiv:1801.02610 [cs, stat]},
	author = {Xiao, Chaowei and Li, Bo and Zhu, Jun-Yan and He, Warren and Liu, Mingyan and Song, Dawn},
	month = feb,
	year = {2019},
	note = {arXiv: 1801.02610},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Statistics - Machine Learning},
}

@incollection{vedaldi_advpc_2020,
	address = {Cham},
	title = {{AdvPC}: {Transferable} {Adversarial} {Perturbations} on {3D} {Point} {Clouds}},
	volume = {12357},
	isbn = {978-3-030-58609-6 978-3-030-58610-2},
	shorttitle = {{AdvPC}},
	url = {http://link.springer.com/10.1007/978-3-030-58610-2_15},
	abstract = {Deep neural networks are vulnerable to adversarial attacks, in which imperceptible perturbations to their input lead to erroneous network predictions. This phenomenon has been extensively studied in the image domain, and has only recently been extended to 3D point clouds. In this work, we present novel data-driven adversarial attacks against 3D point cloud networks. We aim to address the following problems in current 3D point cloud adversarial attacks: they do not transfer well between diﬀerent networks, and they are easy to defend against via simple statistical methods. To this extent, we develop a new point cloud attack (dubbed AdvPC) that exploits the input data distribution by adding an adversarial loss, after Auto-Encoder reconstruction, to the objective it optimizes. AdvPC leads to perturbations that are resilient against current defenses, while remaining highly transferable compared to state-of-theart attacks. We test AdvPC using four popular point cloud networks: PointNet, PointNet++ (MSG and SSG), and DGCNN. Our proposed attack increases the attack success rate by up to 40\% for those transferred to unseen networks (transferability), while maintaining a high success rate on the attacked network. AdvPC also increases the ability to break defenses by up to 38\% as compared to other baselines on the ModelNet40 dataset. The code is available at https://github.com/ajhamdi/AdvPC.},
	language = {en},
	urldate = {2020-11-05},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Hamdi, Abdullah and Rojas, Sara and Thabet, Ali and Ghanem, Bernard},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	doi = {10.1007/978-3-030-58610-2_15},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {241--257},
}

@article{rovid_towards_2020,
	title = {Towards {Reliable} {Multisensory} {Perception} and {Its} {Automotive} {Applications}},
	volume = {48},
	issn = {1587-3811, 0303-7800},
	url = {https://pp.bme.hu/tr/article/view/15921},
	doi = {10.3311/PPtr.15921},
	abstract = {Autonomous driving poses numerous challenging problems, one of which is perceiving and understanding the environment. Since selfdriving is safety critical and many actions taken during driving rely on the outcome of various perception algorithms (for instance all traffic participants and infrastructural objects in the vehicle's surroundings must reliably be recognized and localized), thus the perception might be considered as one of the most critical subsystems in an autonomous vehicle. Although the perception itself might further be decomposed into various sub-problems, such as object detection, lane detection, traffic sign detection, environment modeling, etc. In this paper the focus is on fusion models in general (giving support for multisensory data processing) and some related automotive applications such as object detection, traffic sign recognition, end-to-end driving models and an example of taking decisions in multi-criterial traffic situations that are complex for both human drivers and for the self-driving vehicles as well.},
	language = {en},
	number = {4},
	urldate = {2020-11-01},
	journal = {Periodica Polytechnica Transportation Engineering},
	author = {Rövid, András and Remeli, Viktor and Paufler, Norbert and Lengyel, Henrietta and Zöldy, Máté and Szalay, Zsolt},
	month = jul,
	year = {2020},
	pages = {334--340},
}

@article{wang_towards_2020,
	title = {Towards {Robust} {Sensor} {Fusion} in {Visual} {Perception}},
	url = {http://arxiv.org/abs/2006.13192},
	abstract = {We study the problem of robust sensor fusion in visual perception, especially under the autonomous driving settings. We evaluate the robustness of RGB camera and LiDAR sensor fusion for binary classiﬁcation and object detection. In this work, we are interested in the behavior of different fusion methods under adversarial attacks on different sensors. We ﬁrst train both classiﬁcation and detection models with early fusion and late fusion, then apply different combinations of adversarial attacks on both sensor inputs for evaluation. We also study the effectiveness of adversarial attacks with varying budgets. Experiment results show that while sensor fusion models are generally vulnerable to adversarial attacks, late fusion method is more robust than early fusion. The results also provide insights on further obtaining robust sensor fusion models.},
	language = {en},
	urldate = {2020-11-01},
	journal = {arXiv:2006.13192 [cs]},
	author = {Wang, Shaojie and Wu, Tong and Vorobeychik, Yevgeniy},
	month = jun,
	year = {2020},
	note = {arXiv: 2006.13192},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{tu_physically_2020,
	address = {Seattle, WA, USA},
	title = {Physically {Realizable} {Adversarial} {Examples} for {LiDAR} {Object} {Detection}},
	isbn = {978-1-72817-168-5},
	url = {https://ieeexplore.ieee.org/document/9156447/},
	doi = {10.1109/CVPR42600.2020.01373},
	abstract = {Modern autonomous driving systems rely heavily on deep learning models to process point cloud sensory data; meanwhile, deep models have been shown to be susceptible to adversarial attacks with visually imperceptible perturbations. Despite the fact that this poses a security concern for the self-driving industry, there has been very little exploration in terms of 3D perception, as most adversarial attacks have only been applied to 2D ﬂat images. In this paper, we address this issue and present a method to generate universal 3D adversarial objects to fool LiDAR detectors. In particular, we demonstrate that placing an adversarial object on the rooftop of any target vehicle to hide the vehicle entirely from LiDAR detectors with a success rate of 80\%. We report attack results on a suite of detectors using various input representation of point clouds. We also conduct a pilot study on adversarial defense using data augmentation. This is one step closer towards safer self-driving under unseen conditions from limited training data.},
	language = {en},
	urldate = {2020-11-01},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Tu, James and Ren, Mengye and Manivasagam, Sivabalan and Liang, Ming and Yang, Bin and Du, Richard and Cheng, Frank and Urtasun, Raquel},
	month = jun,
	year = {2020},
	pages = {13713--13722},
}

@article{ren_faster_2016,
	title = {Faster {R}-{CNN}: {Towards} {Real}-{Time} {Object} {Detection} with {Region} {Proposal} {Networks}},
	shorttitle = {Faster {R}-{CNN}},
	url = {http://arxiv.org/abs/1506.01497},
	abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
	urldate = {2020-10-28},
	journal = {arXiv:1506.01497 [cs]},
	author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
	month = jan,
	year = {2016},
	note = {arXiv: 1506.01497},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{li_robust_2019,
	title = {Robust {Adversarial} {Perturbation} on {Deep} {Proposal}-based {Models}},
	url = {http://arxiv.org/abs/1809.05962},
	abstract = {Adversarial noises are useful tools to probe the weakness of deep learning based computer vision algorithms. In this paper, we describe a robust adversarial perturbation (RAP) method to attack deep proposal-based object detectors and instance segmentation algorithms. Our method focuses on attacking the common component in these algorithms, namely Region Proposal Network (RPN), to universally degrade their performance in a black-box fashion. To do so, we design a loss function that combines a label loss and a novel shape loss, and optimize it with respect to image using a gradient based iterative algorithm. Evaluations are performed on the MS COCO 2014 dataset for the adversarial attacking of 6 state-of-the-art object detectors and 2 instance segmentation algorithms. Experimental results demonstrate the efﬁcacy of the proposed method.},
	language = {en},
	urldate = {2020-10-27},
	journal = {arXiv:1809.05962 [cs]},
	author = {Li, Yuezun and Tian, Daniel and Chang, Ming-Ching and Bian, Xiao and Lyu, Siwei},
	month = nov,
	year = {2019},
	note = {arXiv: 1809.05962},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{chen_shapeshifter_2019,
	title = {{ShapeShifter}: {Robust} {Physical} {Adversarial} {Attack} on {Faster} {R}-{CNN} {Object} {Detector}},
	volume = {11051},
	shorttitle = {{ShapeShifter}},
	url = {http://arxiv.org/abs/1804.05810},
	doi = {10.1007/978-3-030-10925-7_4},
	abstract = {Given the ability to directly manipulate image pixels in the digital input space, an adversary can easily generate imperceptible perturbations to fool a Deep Neural Network (DNN) image classiﬁer, as demonstrated in prior work. In this work, we propose ShapeShifter, an attack that tackles the more challenging problem of crafting physical adversarial perturbations to fool image-based object detectors like Faster R-CNN. Attacking an object detector is more difﬁcult than attacking an image classiﬁer, as it needs to mislead the classiﬁcation results in multiple bounding boxes with different scales. Extending the digital attack to the physical world adds another layer of difﬁculty, because it requires the perturbation to be robust enough to survive real-world distortions due to different viewing distances and angles, lighting conditions, and camera limitations. We show that the Expectation over Transformation technique, which was originally proposed to enhance the robustness of adversarial perturbations in image classiﬁcation, can be successfully adapted to the object detection setting. ShapeShifter can generate adversarially perturbed stop signs that are consistently mis-detected by Faster RCNN as other objects, posing a potential threat to autonomous vehicles and other safety-critical computer vision systems.},
	language = {en},
	urldate = {2020-10-27},
	journal = {arXiv:1804.05810 [cs, stat]},
	author = {Chen, Shang-Tse and Cornelius, Cory and Martin, Jason and Chau, Duen Horng},
	year = {2019},
	note = {arXiv: 1804.05810},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {52--68},
}

@article{wei_transferable_2019,
	title = {Transferable {Adversarial} {Attacks} for {Image} and {Video} {Object} {Detection}},
	url = {http://arxiv.org/abs/1811.12641},
	abstract = {Identifying adversarial examples is beneﬁcial for understanding deep networks and developing robust models. However, existing attacking methods for image object detection have two limitations: weak transferability—the generated adversarial examples often have a low success rate to attack other kinds of detection methods, and high computation cost—they need much time to deal with video data, where many frames need polluting. To address these issues, we present a generative method to obtain adversarial images and videos, thereby significantly reducing the processing time. To enhance transferability, we manipulate the feature maps extracted by a feature network, which usually constitutes the basis of object detectors. Our method is based on the Generative Adversarial Network (GAN) framework, where we combine a high-level class loss and a low-level feature loss to jointly train the adversarial example generator. Experimental results on PASCAL VOC and ImageNet VID datasets show that our method efﬁciently generates image and video adversarial examples, and more importantly, these adversarial examples have better transferability, therefore being able to simultaneously attack two kinds of representative object detection models: proposal based models like FasterRCNN and regression based models like SSD.},
	language = {en},
	urldate = {2020-10-24},
	journal = {arXiv:1811.12641 [cs]},
	author = {Wei, Xingxing and Liang, Siyuan and Chen, Ning and Cao, Xiaochun},
	month = may,
	year = {2019},
	note = {arXiv: 1811.12641},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{xie_adversarial_2017,
	title = {Adversarial {Examples} for {Semantic} {Segmentation} and {Object} {Detection}},
	url = {http://arxiv.org/abs/1703.08603},
	abstract = {It has been well demonstrated that adversarial examples, i.e., natural images with visually imperceptible perturbations added, cause deep networks to fail on image classiﬁcation. In this paper, we extend adversarial examples to semantic segmentation and object detection which are much more difﬁcult. Our observation is that both segmentation and detection are based on classifying multiple targets on an image (e.g., the target is a pixel or a receptive ﬁeld in segmentation, and an object proposal in detection). This inspires us to optimize a loss function over a set of pixels/proposals for generating adversarial perturbations. Based on this idea, we propose a novel algorithm named Dense Adversary Generation (DAG), which generates a large family of adversarial examples, and applies to a wide range of state-of-the-art deep networks for segmentation and detection. We also ﬁnd that the adversarial perturbations can be transferred across networks with different training data, based on different architectures, and even for different recognition tasks. In particular, the transferability across networks with the same architecture is more signiﬁcant than in other cases. Besides, summing up heterogeneous perturbations often leads to better transfer performance, which provides an effective method of blackbox adversarial attack.},
	language = {en},
	urldate = {2020-10-24},
	journal = {arXiv:1703.08603 [cs]},
	author = {Xie, Cihang and Wang, Jianyu and Zhang, Zhishuai and Zhou, Yuyin and Xie, Lingxi and Yuille, Alan},
	month = jul,
	year = {2017},
	note = {arXiv: 1703.08603},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{vora_pointpainting_2020,
	title = {{PointPainting}: {Sequential} {Fusion} for {3D} {Object} {Detection}},
	shorttitle = {{PointPainting}},
	url = {http://arxiv.org/abs/1911.10150},
	abstract = {Camera and lidar are important sensor modalities for robotics in general and self-driving cars in particular. The sensors provide complementary information offering an opportunity for tight sensor-fusion. Surprisingly, lidar-only methods outperform fusion methods on the main benchmark datasets, suggesting a gap in the literature. In this work, we propose PointPainting: a sequential fusion method to ﬁll this gap. PointPainting works by projecting lidar points into the output of an image-only semantic segmentation network and appending the class scores to each point. The appended (painted) point cloud can then be fed to any lidaronly method. Experiments show large improvements on three different state-of-the art methods, Point-RCNN, VoxelNet and PointPillars on the KITTI and nuScenes datasets. The painted version of PointRCNN represents a new state of the art on the KITTI leaderboard for the bird’s-eye view detection task. In ablation, we study how the effects of Painting depends on the quality and format of the semantic segmentation output, and demonstrate how latency can be minimized through pipelining.},
	language = {en},
	urldate = {2020-07-31},
	journal = {arXiv:1911.10150 [cs, eess, stat]},
	author = {Vora, Sourabh and Lang, Alex H. and Helou, Bassam and Beijbom, Oscar},
	month = may,
	year = {2020},
	note = {arXiv: 1911.10150},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing, Statistics - Machine Learning},
}

@article{wang_pillar-based_2020,
	title = {Pillar-based {Object} {Detection} for {Autonomous} {Driving}},
	url = {http://arxiv.org/abs/2007.10323},
	abstract = {We present a simple and ﬂexible object detection framework optimized for autonomous driving. Building on the observation that point clouds in this application are extremely sparse, we propose a practical pillar-based approach to ﬁx the imbalance issue caused by anchors. In particular, our algorithm incorporates a cylindrical projection into multiview feature learning, predicts bounding box parameters per pillar rather than per point or per anchor, and includes an aligned pillar-to-point projection module to improve the ﬁnal prediction. Our anchor-free approach avoids hyperparameter search associated with past methods, simplifying 3D object detection while signiﬁcantly improving upon state-of-the-art.},
	language = {en},
	urldate = {2020-07-31},
	journal = {arXiv:2007.10323 [cs]},
	author = {Wang, Yue and Fathi, Alireza and Kundu, Abhijit and Ross, David and Pantofaru, Caroline and Funkhouser, Thomas and Solomon, Justin},
	month = jul,
	year = {2020},
	note = {arXiv: 2007.10323},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@article{hahner_quantifying_2020,
	title = {Quantifying {Data} {Augmentation} for {LiDAR} based {3D} {Object} {Detection}},
	url = {http://arxiv.org/abs/2004.01643},
	abstract = {In this work, we shed light on different data augmentation techniques commonly used in Light Detection and Ranging (LiDAR) based 3D Object Detection. We, therefore, utilize a state of the art voxel based 3D Object Detection pipeline called PointPillars [1] and carry out our experiments on the well established KITTI [2] dataset. We investigate a variety of global and local augmentation techniques, where global augmentation techniques are applied to the entire point cloud of a scene and local augmentation techniques are only applied to points belonging to individual objects in the scene. Our ﬁndings show that both types of data augmentation can lead to performance increases, but it also turns out, that some augmentation techniques, such as individual object translation, for example, can be counterproductive and can hurt overall performance. We show that when we apply our ﬁndings to the data augmentation policy of PointPillars [1] we can easily increase its performance by up to 2\%. In order to provide reproducibility, our code will be publicly available at www.trace.ethz.ch/3D Object Detection.},
	language = {en},
	urldate = {2020-06-18},
	journal = {arXiv:2004.01643 [cs, eess]},
	author = {Hahner, Martin and Dai, Dengxin and Liniger, Alexander and Van Gool, Luc},
	month = apr,
	year = {2020},
	note = {arXiv: 2004.01643},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing},
}

@inproceedings{zhou_dup-net_2019,
	address = {Seoul, Korea (South)},
	title = {{DUP}-{Net}: {Denoiser} and {Upsampler} {Network} for {3D} {Adversarial} {Point} {Clouds} {Defense}},
	isbn = {978-1-72814-803-8},
	shorttitle = {{DUP}-{Net}},
	url = {https://ieeexplore.ieee.org/document/9010939/},
	doi = {10.1109/ICCV.2019.00205},
	abstract = {Neural networks are vulnerable to adversarial examples, which poses a threat to their application in security sensitive systems. We propose a Denoiser and UPsampler Network (DUP-Net) structure as defenses for 3D adversarial point cloud classiﬁcation, where the two modules reconstruct surface smoothness by dropping or adding points. In this paper, statistical outlier removal (SOR) and a datadriven upsampling network are considered as denoiser and upsampler respectively. Compared with baseline defenses, DUP-Net has three advantages. First, with DUP-Net as a defense, the target model is more robust to white-box adversarial attacks. Second, the statistical outlier removal provides added robustness since it is a non-differentiable denoising operation. Third, the upsampler network can be trained on a small dataset and defends well against adversarial attacks generated from other point cloud datasets. We conduct various experiments to validate that DUP-Net is very effective as defense in practice. Our best defense eliminates 83.8\% of C\&W and l2 loss based attack (point shifting), 50.0\% of C\&W and Hausdorff distance loss based attack (point adding) and 9.0\% of saliency map based attack (point dropping) under 200 dropped points on PointNet.},
	language = {en},
	urldate = {2020-05-07},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Zhou, Hang and Chen, Kejiang and Zhang, Weiming and Fang, Han and Zhou, Wenbo and Yu, Nenghai},
	month = oct,
	year = {2019},
	pages = {1961--1970},
}

@article{zhou_lg-gan_nodate,
	title = {{LG}-{GAN}: {Label} {Guided} {Adversarial} {Network} for {Flexible} {Targeted} {Attack} of {Point} {Cloud}-based {Deep} {Networks}},
	abstract = {Deep neural networks have made tremendous progress in 3D point-cloud recognition. Recent works have shown that these 3D recognition networks are also vulnerable to adversarial samples produced from various attack methods, including optimization-based 3D Carlini-Wagner attack, gradient-based iterative fast gradient method, and skeleton-detach based point-dropping. However, after a careful analysis, these methods are either extremely slow because of the optimization/iterative scheme, or not ﬂexible to support targeted attack of a speciﬁc category. To overcome these shortcomings, this paper proposes a novel label guided adversarial network (LG-GAN) for real-time ﬂexible targeted point cloud attack. To the best of our knowledge, this is the ﬁrst generation based 3D point cloud attack method. By feeding the original point clouds and target attack label into LG-GAN, it can learn how to deform the point clouds to mislead the recognition network into the speciﬁc label only with a single forward pass. In detail, LGGAN ﬁrst leverages one multi-branch adversarial network to extract hierarchical features of the input point clouds, then incorporates the speciﬁed label information into multiple intermediate features using the label encoder. Finally, the encoded features will be fed into the coordinate reconstruction decoder to generate the target adversarial sample. By evaluating different point-cloud recognition models (e.g., PointNet, PointNet++ and DGCNN), we demonstrate that the proposed LG-GAN can support ﬂexible targeted attack on the ﬂy while guaranteeing good attack performance and higher efﬁciency simultaneously.},
	language = {en},
	author = {Zhou, Hang and Chen, Dongdong and Liao, Jing and Chen, Kejiang and Dong, Xiaoyi and Liu, Kunlin and Zhang, Weiming and Hua, Gang and Yu, Nenghai},
	pages = {10},
}

@article{liu_adversarial_2019,
	title = {Adversarial point perturbations on {3D} objects},
	url = {http://arxiv.org/abs/1908.06062},
	abstract = {The importance of training robust neural network grows as 3D data is increasingly utilized in deep learning for vision tasks, like autonomous driving. We examine this problem from the perspective of the attacker, which is necessary in understanding how neural networks can be exploited, and thus defended. More specifically, we propose adversarial attacks based on solving different optimization problems, like minimizing the perceptibility of our generated adversarial examples, or maintaining a uniform density distribution of points across the adversarial object surfaces. Our four proposed algorithms for attacking 3D point cloud classification are all highly successful on existing neural networks, and we find that some of them are even effective against previously proposed point removal defenses.},
	language = {en},
	urldate = {2020-04-24},
	journal = {arXiv:1908.06062 [cs, eess, stat]},
	author = {Liu, Daniel and Yu, Ronald and Su, Hao},
	month = aug,
	year = {2019},
	note = {arXiv: 1908.06062},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing, Statistics - Machine Learning},
}

@article{liu_extending_2019,
	title = {Extending {Adversarial} {Attacks} and {Defenses} to {Deep} {3D} {Point} {Cloud} {Classifiers}},
	url = {http://arxiv.org/abs/1901.03006},
	abstract = {3D object classification and segmentation using deep neural networks has been extremely successful. As the problem of identifying 3D objects has many safety-critical applications, the neural networks have to be robust against adversarial changes to the input data set. There is a growing body of research on generating human-imperceptible adversarial attacks and defenses against them in the 2D image classification domain. However, 3D objects have various differences with 2D images, and this specific domain has not been rigorously studied so far. We present a preliminary evaluation of adversarial attacks on deep 3D point cloud classifiers, namely PointNet and PointNet++, by evaluating both white-box and black-box adversarial attacks that were proposed for 2D images and extending those attacks to reduce the perceptibility of the perturbations in 3D space. We also show the high effectiveness of simple defenses against those attacks by proposing new defenses that exploit the unique structure of 3D point clouds. Finally, we attempt to explain the effectiveness of the defenses through the intrinsic structures of both the point clouds and the neural network architectures. Overall, we find that networks that process 3D point cloud data are weak to adversarial attacks, but they are also more easily defensible compared to 2D image classifiers. Our investigation will provide the groundwork for future studies on improving the robustness of deep neural networks that handle 3D data.},
	language = {en},
	urldate = {2020-04-21},
	journal = {arXiv:1901.03006 [cs, stat]},
	author = {Liu, Daniel and Yu, Ronald and Su, Hao},
	month = jun,
	year = {2019},
	note = {arXiv: 1901.03006},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{zhao_isometry_2020,
	title = {On {Isometry} {Robustness} of {Deep} {3D} {Point} {Cloud} {Models} under {Adversarial} {Attacks}},
	url = {http://arxiv.org/abs/2002.12222},
	abstract = {While deep learning in 3D domain has achieved revolutionary performance in many tasks, the robustness of these models has not been sufﬁciently studied or explored. Regarding the 3D adversarial samples, most existing works focus on manipulation of local points, which may fail to invoke the global geometry properties, like robustness under linear projection that preserves the Euclidean distance, i.e., isometry. In this work, we show that existing state-ofthe-art deep 3D models are extremely vulnerable to isometry transformations. Armed with the Thompson Sampling, we develop a black-box attack with success rate over 95\% on ModelNet40 data set. Incorporating with the Restricted Isometry Property, we propose a novel framework of whitebox attack on top of spectral norm based perturbation. In contrast to previous works, our adversarial samples are experimentally shown to be strongly transferable. Evaluated on a sequence of prevailing 3D models, our white-box attack achieves success rates from 98.88\% to 100\%. It maintains a successful attack rate over 95\% even within an imperceptible rotation range [±2.81◦].},
	language = {en},
	urldate = {2020-04-08},
	journal = {arXiv:2002.12222 [cs, stat]},
	author = {Zhao, Yue and Wu, Yuwei and Chen, Caihua and Lim, Andrew},
	month = mar,
	year = {2020},
	note = {arXiv: 2002.12222},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{yang_adversarial_2019,
	title = {Adversarial {Attack} and {Defense} on {Point} {Sets}},
	url = {http://arxiv.org/abs/1902.10899},
	abstract = {Emergence of the utility of 3D point cloud data in critical vision tasks (e.g., ADAS) urges researchers to pay more attention to the robustness of 3D representations and deep networks. To this end, we develop an attack and defense scheme, dedicated to 3D point cloud data, for preventing 3D point clouds from manipulated as well as pursuing noise-tolerable 3D representation. A set of novel 3D point cloud attack operations are proposed via pointwise gradient perturbation and adversarial point attachment / detachment. We then develop a flexible perturbation-measurement scheme for 3D point cloud data to detect potential attack data or noisy sensing data. Extensive experimental results on common point cloud benchmarks demonstrate the validity of the proposed 3D attack and defense framework.},
	language = {en},
	urldate = {2020-03-11},
	journal = {arXiv:1902.10899 [cs]},
	author = {Yang, Jiancheng and Zhang, Qiang and Fang, Rongyao and Ni, Bingbing and Liu, Jinxian and Tian, Qi},
	month = feb,
	year = {2019},
	note = {arXiv: 1902.10899},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@article{xiang_generating_2019,
	title = {Generating {3D} {Adversarial} {Point} {Clouds}},
	url = {http://arxiv.org/abs/1809.07016},
	abstract = {Deep neural networks are known to be vulnerable to adversarial examples which are carefully crafted instances to cause the models to make wrong predictions. While adversarial examples for 2D images and CNNs have been extensively studied, less attention has been paid to 3D data such as point clouds. Given many safety-critical 3D applications such as autonomous driving, it is important to study how adversarial point clouds could affect current deep 3D models. In this work, we propose several novel algorithms to craft adversarial point clouds against PointNet, a widely used deep neural network for point cloud processing. Our algorithms work in two ways: adversarial point perturbation and adversarial point generation. For point perturbation, we shift existing points negligibly. For point generation, we generate either a set of independent and scattered points or a small number (1-3) of point clusters with meaningful shapes such as balls and airplanes which could be hidden in the human psyche. In addition, we formulate six perturbation measurement metrics tailored to the attacks in point clouds and conduct extensive experiments to evaluate the proposed algorithms on the ModelNet40 3D shape classiﬁcation dataset. Overall, our attack algorithms achieve a success rate higher than 99\% for all targeted attacks 1.},
	language = {en},
	urldate = {2020-03-11},
	journal = {arXiv:1809.07016 [cs]},
	author = {Xiang, Chong and Qi, Charles R. and Li, Bo},
	month = jul,
	year = {2019},
	note = {arXiv: 1809.07016},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@article{qi_pointnet_2017,
	title = {{PointNet}: {Deep} {Learning} on {Point} {Sets} for {3D} {Classification} and {Segmentation}},
	shorttitle = {{PointNet}},
	url = {http://arxiv.org/abs/1612.00593},
	abstract = {Point cloud is an important type of geometric data structure. Due to its irregular format, most researchers transform such data to regular 3D voxel grids or collections of images. This, however, renders data unnecessarily voluminous and causes issues. In this paper, we design a novel type of neural network that directly consumes point clouds, which well respects the permutation invariance of points in the input. Our network, named PointNet, provides a uniﬁed architecture for applications ranging from object classiﬁcation, part segmentation, to scene semantic parsing. Though simple, PointNet is highly efﬁcient and effective. Empirically, it shows strong performance on par or even better than state of the art. Theoretically, we provide analysis towards understanding of what the network has learnt and why the network is robust with respect to input perturbation and corruption.},
	language = {en},
	urldate = {2020-03-03},
	journal = {arXiv:1612.00593 [cs]},
	author = {Qi, Charles R. and Su, Hao and Mo, Kaichun and Guibas, Leonidas J.},
	month = apr,
	year = {2017},
	note = {arXiv: 1612.00593},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{zheng_pointcloud_2019,
	title = {{PointCloud} {Saliency} {Maps}},
	url = {http://arxiv.org/abs/1812.01687},
	abstract = {3D point-cloud recognition with PointNet and its variants has received remarkable progress. A missing ingredient, however, is the ability to automatically evaluate point-wise importance w.r.t.{\textbackslash}! classification performance, which is usually reflected by a saliency map. A saliency map is an important tool as it allows one to perform further processes on point-cloud data. In this paper, we propose a novel way of characterizing critical points and segments to build point-cloud saliency maps. Our method assigns each point a score reflecting its contribution to the model-recognition loss. The saliency map explicitly explains which points are the key for model recognition. Furthermore, aggregations of highly-scored points indicate important segments/subsets in a point-cloud. Our motivation for constructing a saliency map is by point dropping, which is a non-differentiable operator. To overcome this issue, we approximate point-dropping with a differentiable procedure of shifting points towards the cloud centroid. Consequently, each saliency score can be efficiently measured by the corresponding gradient of the loss w.r.t the point under the spherical coordinates. Extensive evaluations on several state-of-the-art point-cloud recognition models, including PointNet, PointNet++ and DGCNN, demonstrate the veracity and generality of our proposed saliency map. Code for experiments is released on {\textbackslash}url\{https://github.com/tianzheng4/PointCloud-Saliency-Maps\}.},
	language = {en},
	urldate = {2020-03-02},
	journal = {arXiv:1812.01687 [cs]},
	author = {Zheng, Tianhang and Chen, Changyou and Yuan, Junsong and Li, Bo and Ren, Kui},
	month = sep,
	year = {2019},
	note = {arXiv: 1812.01687},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
}

@article{lang_pointpillars_2019,
	title = {{PointPillars}: {Fast} {Encoders} for {Object} {Detection} from {Point} {Clouds}},
	shorttitle = {{PointPillars}},
	url = {http://arxiv.org/abs/1812.05784},
	abstract = {Object detection in point clouds is an important aspect of many robotics applications such as autonomous driving. In this paper we consider the problem of encoding a point cloud into a format appropriate for a downstream detection pipeline. Recent literature suggests two types of encoders; ﬁxed encoders tend to be fast but sacriﬁce accuracy, while encoders that are learned from data are more accurate, but slower. In this work we propose PointPillars, a novel encoder which utilizes PointNets to learn a representation of point clouds organized in vertical columns (pillars). While the encoded features can be used with any standard 2D convolutional detection architecture, we further propose a lean downstream network. Extensive experimentation shows that PointPillars outperforms previous encoders with respect to both speed and accuracy by a large margin. Despite only using lidar, our full detection pipeline signiﬁcantly outperforms the state of the art, even among fusion methods, with respect to both the 3D and bird’s eye view KITTI benchmarks. This detection performance is achieved while running at 62 Hz: a 2 - 4 fold runtime improvement. A faster version of our method matches the state of the art at 105 Hz. These benchmarks suggest that PointPillars is an appropriate encoding for object detection in point clouds.},
	language = {en},
	urldate = {2020-03-02},
	journal = {arXiv:1812.05784 [cs, stat]},
	author = {Lang, Alex H. and Vora, Sourabh and Caesar, Holger and Zhou, Lubing and Yang, Jiong and Beijbom, Oscar},
	month = may,
	year = {2019},
	note = {arXiv: 1812.05784},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}
