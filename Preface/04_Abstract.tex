% !TeX root = ../thesis.tex

\chapter*{Abstract}
\addcontentsline{toc}{chapter}{Abstract}

Over the past few years, remarkable progress in machine learning, especially neural networks, provides revolutionized performance in various fields, such as computer vision, nature language processing, speech recognition. However, deep learning models are found to be vulnerable to adversarial attacks, specifically slight perturbations in the input will lead to dramatic mistakes in output predictions, provoking a growing concern about the credibility of neural network. Hereafter, robustness of detection approaches against adversarial attacks in 2D image domain has been largely investigated and numerous approaches to craft adversarial samples are proposed continually in the recent years, but model vulnerability with 3D adversarial samples is relatively poorly understood.

Along with the rapid development of autonomous driving, a safe-critical area and on which high-precision depth sensors such as LiDAR are widely used,  3D detection model to process point cloud data and corresponding vulnerability exploration are eagerly demanded. Simple transfer from 2D image detection approach to 3D is not easily feasible, and noise in the point cloud data is unpredictable, making the 3D point cloud object detection work highly complexed in the physical world.  



\chapter*{Kurzfassung}
\addcontentsline{toc}{chapter}{Kurzfassung}

\blindtext
